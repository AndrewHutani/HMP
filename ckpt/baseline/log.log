[2025-03-18 13:32:30,280] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_18_13_32_12.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_18_13_32_12.log",
    "weight_decay": 0.0001
}
[2025-03-18 13:35:15,908] INFO: Iter 100 Summary: 
[2025-03-18 13:35:15,909] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-18 13:38:01,145] INFO: Iter 200 Summary: 
[2025-03-18 13:38:01,145] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-18 13:40:46,560] INFO: Iter 300 Summary: 
[2025-03-18 13:40:46,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-18 13:43:32,075] INFO: Iter 400 Summary: 
[2025-03-18 13:43:32,075] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-18 13:46:17,640] INFO: Iter 500 Summary: 
[2025-03-18 13:46:17,640] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-18 13:49:03,157] INFO: Iter 600 Summary: 
[2025-03-18 13:49:03,158] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-18 13:51:48,784] INFO: Iter 700 Summary: 
[2025-03-18 13:51:48,784] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-18 13:54:34,200] INFO: Iter 800 Summary: 
[2025-03-18 13:54:34,200] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-18 13:57:29,209] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_18_13_57_09.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_18_13_57_09.log",
    "weight_decay": 0.0001
}
[2025-03-18 13:59:19,902] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_18_13_59_01.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_18_13_59_01.log",
    "weight_decay": 0.0001
}
[2025-03-18 14:02:05,713] INFO: Iter 100 Summary: 
[2025-03-18 14:02:05,713] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-18 14:04:51,069] INFO: Iter 200 Summary: 
[2025-03-18 14:04:51,069] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-18 14:07:36,600] INFO: Iter 300 Summary: 
[2025-03-18 14:07:36,601] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-18 14:10:21,970] INFO: Iter 400 Summary: 
[2025-03-18 14:10:21,970] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-18 14:13:07,530] INFO: Iter 500 Summary: 
[2025-03-18 14:13:07,530] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-18 14:15:52,957] INFO: Iter 600 Summary: 
[2025-03-18 14:15:52,957] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-18 14:18:38,306] INFO: Iter 700 Summary: 
[2025-03-18 14:18:38,306] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-18 14:21:23,710] INFO: Iter 800 Summary: 
[2025-03-18 14:21:23,710] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-18 14:24:09,079] INFO: Iter 900 Summary: 
[2025-03-18 14:24:09,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07361114241182803
[2025-03-18 14:26:54,586] INFO: Iter 1000 Summary: 
[2025-03-18 14:26:54,586] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0712160461395979
[2025-03-18 14:29:39,965] INFO: Iter 1100 Summary: 
[2025-03-18 14:29:39,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0662378877773881
[2025-03-18 14:32:25,243] INFO: Iter 1200 Summary: 
[2025-03-18 14:32:25,244] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06260723497718573
[2025-03-18 14:35:10,602] INFO: Iter 1300 Summary: 
[2025-03-18 14:35:10,602] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06077460564672947
[2025-03-18 14:37:55,960] INFO: Iter 1400 Summary: 
[2025-03-18 14:37:55,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059101610518991946
[2025-03-18 14:40:41,449] INFO: Iter 1500 Summary: 
[2025-03-18 14:40:41,449] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05815540604293346
[2025-03-18 14:43:26,929] INFO: Iter 1600 Summary: 
[2025-03-18 14:43:26,929] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768500231206417
[2025-03-18 14:46:12,268] INFO: Iter 1700 Summary: 
[2025-03-18 14:46:12,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05612747546285391
[2025-03-18 14:48:57,690] INFO: Iter 1800 Summary: 
[2025-03-18 14:48:57,691] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0559469723328948
[2025-03-18 14:51:43,117] INFO: Iter 1900 Summary: 
[2025-03-18 14:51:43,117] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055011885352432725
[2025-03-18 14:54:28,376] INFO: Iter 2000 Summary: 
[2025-03-18 14:54:28,376] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054927452243864534
[2025-03-18 14:57:13,764] INFO: Iter 2100 Summary: 
[2025-03-18 14:57:13,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05467277605086565
[2025-03-18 14:59:59,006] INFO: Iter 2200 Summary: 
[2025-03-18 14:59:59,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05439220994710922
[2025-03-18 15:02:44,206] INFO: Iter 2300 Summary: 
[2025-03-18 15:02:44,207] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05411341149359941
[2025-03-18 15:05:29,414] INFO: Iter 2400 Summary: 
[2025-03-18 15:05:29,414] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05394661214202642
[2025-03-18 15:08:14,705] INFO: Iter 2500 Summary: 
[2025-03-18 15:08:14,705] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05381324980407953
[2025-03-18 15:11:00,130] INFO: Iter 2600 Summary: 
[2025-03-18 15:11:00,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053067084886133674
[2025-03-18 15:13:45,563] INFO: Iter 2700 Summary: 
[2025-03-18 15:13:45,563] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05291106801480055
[2025-03-18 15:16:31,068] INFO: Iter 2800 Summary: 
[2025-03-18 15:16:31,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268501613289118
[2025-03-18 15:19:16,484] INFO: Iter 2900 Summary: 
[2025-03-18 15:19:16,484] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05244477484375239
[2025-03-18 15:22:01,892] INFO: Iter 3000 Summary: 
[2025-03-18 15:22:01,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052546527571976184
[2025-03-18 15:24:47,308] INFO: Iter 3100 Summary: 
[2025-03-18 15:24:47,308] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051631263718008996
[2025-03-18 15:27:32,777] INFO: Iter 3200 Summary: 
[2025-03-18 15:27:32,777] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05216690514236689
[2025-03-18 15:30:18,156] INFO: Iter 3300 Summary: 
[2025-03-18 15:30:18,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05180593386292458
[2025-03-18 15:33:03,575] INFO: Iter 3400 Summary: 
[2025-03-18 15:33:03,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128339599817991
[2025-03-18 15:35:48,944] INFO: Iter 3500 Summary: 
[2025-03-18 15:35:48,944] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05161994945257902
[2025-03-18 15:38:34,409] INFO: Iter 3600 Summary: 
[2025-03-18 15:38:34,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051079065576195716
[2025-03-18 15:41:19,864] INFO: Iter 3700 Summary: 
[2025-03-18 15:41:19,864] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05093852359801531
[2025-03-18 15:44:05,313] INFO: Iter 3800 Summary: 
[2025-03-18 15:44:05,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050728333853185176
[2025-03-18 15:46:50,554] INFO: Iter 3900 Summary: 
[2025-03-18 15:46:50,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051007907390594485
[2025-03-18 15:49:35,818] INFO: Iter 4000 Summary: 
[2025-03-18 15:49:35,818] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128511141985655
[2025-03-18 15:52:21,043] INFO: Iter 4100 Summary: 
[2025-03-18 15:52:21,044] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050616980902850627
[2025-03-18 15:55:06,329] INFO: Iter 4200 Summary: 
[2025-03-18 15:55:06,329] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05099097285419703
[2025-03-18 15:57:51,767] INFO: Iter 4300 Summary: 
[2025-03-18 15:57:51,767] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05006971627473831
[2025-03-18 16:00:37,018] INFO: Iter 4400 Summary: 
[2025-03-18 16:00:37,019] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05037182774394751
[2025-03-18 16:03:22,300] INFO: Iter 4500 Summary: 
[2025-03-18 16:03:22,300] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050234943144023415
[2025-03-18 16:06:07,541] INFO: Iter 4600 Summary: 
[2025-03-18 16:06:07,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05055206254124642
[2025-03-18 16:08:52,866] INFO: Iter 4700 Summary: 
[2025-03-18 16:08:52,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04994991168379784
[2025-03-18 16:11:38,189] INFO: Iter 4800 Summary: 
[2025-03-18 16:11:38,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05021801430732012
[2025-03-18 16:14:23,500] INFO: Iter 4900 Summary: 
[2025-03-18 16:14:23,500] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049785437248647216
[2025-03-18 16:17:08,924] INFO: Iter 5000 Summary: 
[2025-03-18 16:17:08,924] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04928127493709326
[2025-03-18 16:20:18,210] INFO: Iter 5100 Summary: 
[2025-03-18 16:20:18,211] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04932282216846943
[2025-03-18 16:23:03,606] INFO: Iter 5200 Summary: 
[2025-03-18 16:23:03,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0498723316937685
[2025-03-18 16:25:48,967] INFO: Iter 5300 Summary: 
[2025-03-18 16:25:48,967] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049440183341503144
[2025-03-18 16:28:34,227] INFO: Iter 5400 Summary: 
[2025-03-18 16:28:34,227] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049165061116218566
[2025-03-18 16:31:19,535] INFO: Iter 5500 Summary: 
[2025-03-18 16:31:19,535] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049324527978897095
[2025-03-18 16:34:04,814] INFO: Iter 5600 Summary: 
[2025-03-18 16:34:04,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487074363976717
[2025-03-18 16:36:50,168] INFO: Iter 5700 Summary: 
[2025-03-18 16:36:50,168] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04891889002174139
[2025-03-18 16:39:35,424] INFO: Iter 5800 Summary: 
[2025-03-18 16:39:35,424] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487964428216219
[2025-03-18 16:42:20,685] INFO: Iter 5900 Summary: 
[2025-03-18 16:42:20,685] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048565466962754725
[2025-03-18 16:45:05,994] INFO: Iter 6000 Summary: 
[2025-03-18 16:45:05,994] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04910611681640148
[2025-03-18 16:47:51,320] INFO: Iter 6100 Summary: 
[2025-03-18 16:47:51,321] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04909805666655302
[2025-03-18 16:50:36,729] INFO: Iter 6200 Summary: 
[2025-03-18 16:50:36,729] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048542876280844215
[2025-03-18 16:53:22,044] INFO: Iter 6300 Summary: 
[2025-03-18 16:53:22,044] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048175370916724206
[2025-03-19 08:49:37,964] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_19_08_49_11.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_19_08_49_11.log",
    "weight_decay": 0.0001
}
[2025-03-19 08:50:22,555] INFO: Iter 100 Summary: 
[2025-03-19 08:50:22,555] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-19 08:50:55,311] INFO: Iter 200 Summary: 
[2025-03-19 08:50:55,311] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-19 08:51:27,562] INFO: Iter 300 Summary: 
[2025-03-19 08:51:27,562] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-19 08:52:00,459] INFO: Iter 400 Summary: 
[2025-03-19 08:52:00,459] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-19 08:52:35,322] INFO: Iter 500 Summary: 
[2025-03-19 08:52:35,322] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-19 08:53:08,375] INFO: Iter 600 Summary: 
[2025-03-19 08:53:08,375] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-19 08:53:41,470] INFO: Iter 700 Summary: 
[2025-03-19 08:53:41,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-19 08:54:14,405] INFO: Iter 800 Summary: 
[2025-03-19 08:54:14,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-19 08:54:48,082] INFO: Iter 900 Summary: 
[2025-03-19 08:54:48,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07361114241182803
[2025-03-19 08:55:21,152] INFO: Iter 1000 Summary: 
[2025-03-19 08:55:21,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0712160461395979
[2025-03-19 08:55:54,542] INFO: Iter 1100 Summary: 
[2025-03-19 08:55:54,542] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0662378877773881
[2025-03-19 08:56:27,421] INFO: Iter 1200 Summary: 
[2025-03-19 08:56:27,421] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06260723497718573
[2025-03-19 08:57:00,642] INFO: Iter 1300 Summary: 
[2025-03-19 08:57:00,642] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06077460564672947
[2025-03-19 08:57:33,897] INFO: Iter 1400 Summary: 
[2025-03-19 08:57:33,897] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059101610518991946
[2025-03-19 08:58:08,100] INFO: Iter 1500 Summary: 
[2025-03-19 08:58:08,101] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05815540604293346
[2025-03-19 08:58:41,932] INFO: Iter 1600 Summary: 
[2025-03-19 08:58:41,932] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768500231206417
[2025-03-19 08:59:15,855] INFO: Iter 1700 Summary: 
[2025-03-19 08:59:15,855] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05612747546285391
[2025-03-19 08:59:50,235] INFO: Iter 1800 Summary: 
[2025-03-19 08:59:50,236] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0559469723328948
[2025-03-19 09:00:24,199] INFO: Iter 1900 Summary: 
[2025-03-19 09:00:24,199] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055011885352432725
[2025-03-19 09:00:58,280] INFO: Iter 2000 Summary: 
[2025-03-19 09:00:58,280] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054927452243864534
[2025-03-19 09:01:32,337] INFO: Iter 2100 Summary: 
[2025-03-19 09:01:32,338] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05467277605086565
[2025-03-19 09:02:06,421] INFO: Iter 2200 Summary: 
[2025-03-19 09:02:06,421] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05439220994710922
[2025-03-19 09:02:40,494] INFO: Iter 2300 Summary: 
[2025-03-19 09:02:40,494] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05411341149359941
[2025-03-19 09:03:14,927] INFO: Iter 2400 Summary: 
[2025-03-19 09:03:14,928] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05394661214202642
[2025-03-19 09:03:49,690] INFO: Iter 2500 Summary: 
[2025-03-19 09:03:49,690] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05381324980407953
[2025-03-19 09:04:23,902] INFO: Iter 2600 Summary: 
[2025-03-19 09:04:23,902] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053067084886133674
[2025-03-19 09:04:58,297] INFO: Iter 2700 Summary: 
[2025-03-19 09:04:58,297] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05291106801480055
[2025-03-19 09:05:32,727] INFO: Iter 2800 Summary: 
[2025-03-19 09:05:32,727] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268501613289118
[2025-03-19 09:06:05,661] INFO: Iter 2900 Summary: 
[2025-03-19 09:06:05,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05244477484375239
[2025-03-19 09:06:38,079] INFO: Iter 3000 Summary: 
[2025-03-19 09:06:38,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052546527571976184
[2025-03-19 09:07:10,531] INFO: Iter 3100 Summary: 
[2025-03-19 09:07:10,531] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051631263718008996
[2025-03-19 09:07:42,733] INFO: Iter 3200 Summary: 
[2025-03-19 09:07:42,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05216690514236689
[2025-03-19 09:08:14,924] INFO: Iter 3300 Summary: 
[2025-03-19 09:08:14,948] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05180593386292458
[2025-03-19 09:08:46,763] INFO: Iter 3400 Summary: 
[2025-03-19 09:08:46,763] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128339599817991
[2025-03-19 09:09:18,733] INFO: Iter 3500 Summary: 
[2025-03-19 09:09:18,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05161994945257902
[2025-03-19 09:09:50,632] INFO: Iter 3600 Summary: 
[2025-03-19 09:09:50,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051079065576195716
[2025-03-19 09:10:22,412] INFO: Iter 3700 Summary: 
[2025-03-19 09:10:22,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05093852359801531
[2025-03-19 09:10:54,244] INFO: Iter 3800 Summary: 
[2025-03-19 09:10:54,244] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050728333853185176
[2025-03-19 09:11:25,992] INFO: Iter 3900 Summary: 
[2025-03-19 09:11:25,992] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051007907390594485
[2025-03-19 09:11:57,999] INFO: Iter 4000 Summary: 
[2025-03-19 09:11:57,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128511141985655
[2025-03-19 09:12:29,811] INFO: Iter 4100 Summary: 
[2025-03-19 09:12:29,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050616980902850627
[2025-03-19 09:13:01,657] INFO: Iter 4200 Summary: 
[2025-03-19 09:13:01,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05099097285419703
[2025-03-19 09:13:33,419] INFO: Iter 4300 Summary: 
[2025-03-19 09:13:33,419] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05006971627473831
[2025-03-19 09:14:05,230] INFO: Iter 4400 Summary: 
[2025-03-19 09:14:05,230] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05037182774394751
[2025-03-19 09:14:37,348] INFO: Iter 4500 Summary: 
[2025-03-19 09:14:37,348] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050234943144023415
[2025-03-19 09:15:09,109] INFO: Iter 4600 Summary: 
[2025-03-19 09:15:09,109] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05055206254124642
[2025-03-19 09:15:40,792] INFO: Iter 4700 Summary: 
[2025-03-19 09:15:40,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04994991168379784
[2025-03-19 09:16:12,860] INFO: Iter 4800 Summary: 
[2025-03-19 09:16:12,861] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05021801430732012
[2025-03-19 09:16:44,603] INFO: Iter 4900 Summary: 
[2025-03-19 09:16:44,603] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049785437248647216
[2025-03-19 09:17:16,396] INFO: Iter 5000 Summary: 
[2025-03-19 09:17:16,396] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04928127493709326
[2025-03-19 09:17:54,829] INFO: Iter 5100 Summary: 
[2025-03-19 09:17:54,829] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04932282216846943
[2025-03-19 09:18:26,670] INFO: Iter 5200 Summary: 
[2025-03-19 09:18:26,670] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0498723316937685
[2025-03-19 09:18:58,442] INFO: Iter 5300 Summary: 
[2025-03-19 09:18:58,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049440183341503144
[2025-03-19 09:19:30,206] INFO: Iter 5400 Summary: 
[2025-03-19 09:19:30,206] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049165061116218566
[2025-03-19 09:20:01,983] INFO: Iter 5500 Summary: 
[2025-03-19 09:20:01,983] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049324527978897095
[2025-03-19 09:20:33,996] INFO: Iter 5600 Summary: 
[2025-03-19 09:20:33,996] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487074363976717
[2025-03-19 09:21:05,959] INFO: Iter 5700 Summary: 
[2025-03-19 09:21:05,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04891889002174139
[2025-03-19 09:21:37,855] INFO: Iter 5800 Summary: 
[2025-03-19 09:21:37,855] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487964428216219
[2025-03-19 09:22:09,682] INFO: Iter 5900 Summary: 
[2025-03-19 09:22:09,682] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048565466962754725
[2025-03-19 09:22:41,509] INFO: Iter 6000 Summary: 
[2025-03-19 09:22:41,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04910611681640148
[2025-03-19 09:23:13,306] INFO: Iter 6100 Summary: 
[2025-03-19 09:23:13,307] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04909805666655302
[2025-03-19 09:23:45,127] INFO: Iter 6200 Summary: 
[2025-03-19 09:23:45,127] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048542876280844215
[2025-03-19 09:24:16,990] INFO: Iter 6300 Summary: 
[2025-03-19 09:24:16,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048175370916724206
[2025-03-19 09:24:48,783] INFO: Iter 6400 Summary: 
[2025-03-19 09:24:48,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048257682099938394
[2025-03-19 09:25:20,774] INFO: Iter 6500 Summary: 
[2025-03-19 09:25:20,774] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04780059069395065
[2025-03-19 09:25:52,701] INFO: Iter 6600 Summary: 
[2025-03-19 09:25:52,701] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04812747694551945
[2025-03-19 09:26:24,619] INFO: Iter 6700 Summary: 
[2025-03-19 09:26:24,619] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04789849977940321
[2025-03-19 09:26:56,348] INFO: Iter 6800 Summary: 
[2025-03-19 09:26:56,348] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04803676519542933
[2025-03-19 09:27:28,489] INFO: Iter 6900 Summary: 
[2025-03-19 09:27:28,490] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047816569320857524
[2025-03-19 09:27:56,120] INFO: Iter 7000 Summary: 
[2025-03-19 09:27:56,120] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04756761498749256
[2025-03-19 09:28:23,715] INFO: Iter 7100 Summary: 
[2025-03-19 09:28:23,715] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047559353597462176
[2025-03-19 09:28:51,359] INFO: Iter 7200 Summary: 
[2025-03-19 09:28:51,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047472536452114585
[2025-03-19 09:29:19,021] INFO: Iter 7300 Summary: 
[2025-03-19 09:29:19,021] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04747529577463865
[2025-03-19 09:29:46,626] INFO: Iter 7400 Summary: 
[2025-03-19 09:29:46,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047281279265880584
[2025-03-19 09:30:14,590] INFO: Iter 7500 Summary: 
[2025-03-19 09:30:14,590] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04714436192065477
[2025-03-19 09:30:42,469] INFO: Iter 7600 Summary: 
[2025-03-19 09:30:42,469] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04699894677847624
[2025-03-19 09:31:10,371] INFO: Iter 7700 Summary: 
[2025-03-19 09:31:10,371] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04676682282239199
[2025-03-19 09:31:38,210] INFO: Iter 7800 Summary: 
[2025-03-19 09:31:38,210] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04656113795936108
[2025-03-19 09:32:06,119] INFO: Iter 7900 Summary: 
[2025-03-19 09:32:06,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04667622290551662
[2025-03-19 09:32:33,954] INFO: Iter 8000 Summary: 
[2025-03-19 09:32:33,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04621357962489128
[2025-03-19 09:33:01,990] INFO: Iter 8100 Summary: 
[2025-03-19 09:33:01,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04603332590311766
[2025-03-19 09:33:29,886] INFO: Iter 8200 Summary: 
[2025-03-19 09:33:29,887] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04624975711107254
[2025-03-19 09:33:57,803] INFO: Iter 8300 Summary: 
[2025-03-19 09:33:57,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04620081443339586
[2025-03-19 09:34:25,687] INFO: Iter 8400 Summary: 
[2025-03-19 09:34:25,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04588279686868191
[2025-03-19 09:34:53,583] INFO: Iter 8500 Summary: 
[2025-03-19 09:34:53,583] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045783929899334906
[2025-03-19 09:35:21,440] INFO: Iter 8600 Summary: 
[2025-03-19 09:35:21,440] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04578123599290848
[2025-03-19 09:35:49,325] INFO: Iter 8700 Summary: 
[2025-03-19 09:35:49,325] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0458084799349308
[2025-03-19 09:36:17,185] INFO: Iter 8800 Summary: 
[2025-03-19 09:36:17,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550562381744385
[2025-03-19 09:36:45,180] INFO: Iter 8900 Summary: 
[2025-03-19 09:36:45,181] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539607536047697
[2025-03-19 09:37:13,568] INFO: Iter 9000 Summary: 
[2025-03-19 09:37:13,568] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045433243066072465
[2025-03-19 09:37:41,473] INFO: Iter 9100 Summary: 
[2025-03-19 09:37:41,473] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04509299598634243
[2025-03-19 09:38:09,433] INFO: Iter 9200 Summary: 
[2025-03-19 09:38:09,433] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045294758789241316
[2025-03-19 09:38:37,304] INFO: Iter 9300 Summary: 
[2025-03-19 09:38:37,305] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044912171848118305
[2025-03-19 09:39:05,351] INFO: Iter 9400 Summary: 
[2025-03-19 09:39:05,351] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04520781476050615
[2025-03-19 09:39:33,388] INFO: Iter 9500 Summary: 
[2025-03-19 09:39:33,388] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04517476037144661
[2025-03-19 09:40:01,398] INFO: Iter 9600 Summary: 
[2025-03-19 09:40:01,398] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04456259276717901
[2025-03-19 09:40:29,370] INFO: Iter 9700 Summary: 
[2025-03-19 09:40:29,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044481374770402905
[2025-03-19 09:40:57,316] INFO: Iter 9800 Summary: 
[2025-03-19 09:40:57,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423113480210304
[2025-03-19 09:41:25,359] INFO: Iter 9900 Summary: 
[2025-03-19 09:41:25,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459886740893126
[2025-03-19 09:41:53,252] INFO: Iter 10000 Summary: 
[2025-03-19 09:41:53,252] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044652320109307767
[2025-03-19 09:42:28,259] INFO: Iter 10100 Summary: 
[2025-03-19 09:42:28,259] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449293956160545
[2025-03-19 09:42:56,547] INFO: Iter 10200 Summary: 
[2025-03-19 09:42:56,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436818648129701
[2025-03-19 09:43:24,419] INFO: Iter 10300 Summary: 
[2025-03-19 09:43:24,419] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044031220078468325
[2025-03-19 09:43:52,356] INFO: Iter 10400 Summary: 
[2025-03-19 09:43:52,356] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444341254606843
[2025-03-19 09:44:20,243] INFO: Iter 10500 Summary: 
[2025-03-19 09:44:20,243] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434599511325359
[2025-03-19 09:44:48,074] INFO: Iter 10600 Summary: 
[2025-03-19 09:44:48,074] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044023889005184176
[2025-03-19 09:45:15,910] INFO: Iter 10700 Summary: 
[2025-03-19 09:45:15,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0438985838368535
[2025-03-19 09:45:43,799] INFO: Iter 10800 Summary: 
[2025-03-19 09:45:43,799] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044074077159166336
[2025-03-19 09:46:11,684] INFO: Iter 10900 Summary: 
[2025-03-19 09:46:11,684] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043824025094509125
[2025-03-19 09:46:39,721] INFO: Iter 11000 Summary: 
[2025-03-19 09:46:39,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04427321132272482
[2025-03-19 09:47:07,659] INFO: Iter 11100 Summary: 
[2025-03-19 09:47:07,659] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043777071200311185
[2025-03-19 09:47:35,512] INFO: Iter 11200 Summary: 
[2025-03-19 09:47:35,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043537504561245444
[2025-03-19 09:48:03,406] INFO: Iter 11300 Summary: 
[2025-03-19 09:48:03,406] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043300241380929944
[2025-03-19 09:48:31,313] INFO: Iter 11400 Summary: 
[2025-03-19 09:48:31,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043706214837729934
[2025-03-19 09:48:59,175] INFO: Iter 11500 Summary: 
[2025-03-19 09:48:59,175] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043423142954707145
[2025-03-19 09:49:27,018] INFO: Iter 11600 Summary: 
[2025-03-19 09:49:27,018] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371713496744633
[2025-03-19 09:49:55,544] INFO: Iter 11700 Summary: 
[2025-03-19 09:49:55,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366573859006166
[2025-03-19 09:50:23,680] INFO: Iter 11800 Summary: 
[2025-03-19 09:50:23,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327931553125382
[2025-03-19 09:50:51,574] INFO: Iter 11900 Summary: 
[2025-03-19 09:50:51,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043304727971553804
[2025-03-19 09:51:19,574] INFO: Iter 12000 Summary: 
[2025-03-19 09:51:19,574] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351072043180466
[2025-03-19 09:51:47,369] INFO: Iter 12100 Summary: 
[2025-03-19 09:51:47,369] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324725206941366
[2025-03-19 09:52:15,221] INFO: Iter 12200 Summary: 
[2025-03-19 09:52:15,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326152633875609
[2025-03-19 09:52:43,107] INFO: Iter 12300 Summary: 
[2025-03-19 09:52:43,107] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043234428763389586
[2025-03-19 09:53:11,011] INFO: Iter 12400 Summary: 
[2025-03-19 09:53:11,011] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042961729168891905
[2025-03-19 09:53:38,906] INFO: Iter 12500 Summary: 
[2025-03-19 09:53:38,906] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043149159252643586
[2025-03-19 09:54:06,739] INFO: Iter 12600 Summary: 
[2025-03-19 09:54:06,739] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346694178879261
[2025-03-19 09:54:34,635] INFO: Iter 12700 Summary: 
[2025-03-19 09:54:34,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04286337073892355
[2025-03-19 09:55:02,493] INFO: Iter 12800 Summary: 
[2025-03-19 09:55:02,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042945913523435596
[2025-03-19 09:55:30,456] INFO: Iter 12900 Summary: 
[2025-03-19 09:55:30,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296220101416111
[2025-03-19 09:55:58,318] INFO: Iter 13000 Summary: 
[2025-03-19 09:55:58,318] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316669885069132
[2025-03-19 09:56:26,211] INFO: Iter 13100 Summary: 
[2025-03-19 09:56:26,211] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04283607821911573
[2025-03-19 09:56:54,245] INFO: Iter 13200 Summary: 
[2025-03-19 09:56:54,245] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04284658413380384
[2025-03-19 09:57:22,221] INFO: Iter 13300 Summary: 
[2025-03-19 09:57:22,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04274474676698446
[2025-03-19 09:57:50,237] INFO: Iter 13400 Summary: 
[2025-03-19 09:57:50,237] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042780365869402884
[2025-03-19 09:58:18,866] INFO: Iter 13500 Summary: 
[2025-03-19 09:58:18,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042822097018361094
[2025-03-19 09:58:47,036] INFO: Iter 13600 Summary: 
[2025-03-19 09:58:47,037] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04263179611414671
[2025-03-19 09:59:14,931] INFO: Iter 13700 Summary: 
[2025-03-19 09:59:14,931] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0425363589450717
[2025-03-19 09:59:42,795] INFO: Iter 13800 Summary: 
[2025-03-19 09:59:42,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042644629068672654
[2025-03-19 10:00:10,781] INFO: Iter 13900 Summary: 
[2025-03-19 10:00:10,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042574195228517055
[2025-03-19 10:00:38,635] INFO: Iter 14000 Summary: 
[2025-03-19 10:00:38,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042417608462274076
[2025-03-19 10:01:06,498] INFO: Iter 14100 Summary: 
[2025-03-19 10:01:06,498] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04244217522442341
[2025-03-19 10:01:34,457] INFO: Iter 14200 Summary: 
[2025-03-19 10:01:34,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04276251219213009
[2025-03-19 10:02:02,402] INFO: Iter 14300 Summary: 
[2025-03-19 10:02:02,402] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04227613303810358
[2025-03-19 10:02:30,214] INFO: Iter 14400 Summary: 
[2025-03-19 10:02:30,214] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04257926631718874
[2025-03-19 10:02:58,202] INFO: Iter 14500 Summary: 
[2025-03-19 10:02:58,202] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0423652433976531
[2025-03-19 10:03:26,007] INFO: Iter 14600 Summary: 
[2025-03-19 10:03:26,007] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04202359989285469
[2025-03-19 10:03:53,816] INFO: Iter 14700 Summary: 
[2025-03-19 10:03:53,816] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04252763915807009
[2025-03-19 10:04:21,582] INFO: Iter 14800 Summary: 
[2025-03-19 10:04:21,582] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0424383956566453
[2025-03-19 10:04:49,369] INFO: Iter 14900 Summary: 
[2025-03-19 10:04:49,369] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04254950750619173
[2025-03-19 10:05:17,258] INFO: Iter 15000 Summary: 
[2025-03-19 10:05:17,258] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04259636342525482
[2025-03-19 10:05:51,821] INFO: Iter 15100 Summary: 
[2025-03-19 10:05:51,821] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04187404114753008
[2025-03-19 10:06:19,635] INFO: Iter 15200 Summary: 
[2025-03-19 10:06:19,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04229228679090738
[2025-03-19 10:06:47,593] INFO: Iter 15300 Summary: 
[2025-03-19 10:06:47,593] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04232952505350113
[2025-03-19 10:07:15,477] INFO: Iter 15400 Summary: 
[2025-03-19 10:07:15,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04256001405417919
[2025-03-19 10:07:43,309] INFO: Iter 15500 Summary: 
[2025-03-19 10:07:43,309] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042025483772158624
[2025-03-19 10:08:11,118] INFO: Iter 15600 Summary: 
[2025-03-19 10:08:11,118] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042312452010810374
[2025-03-19 10:08:38,964] INFO: Iter 15700 Summary: 
[2025-03-19 10:08:38,964] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04207139130681753
[2025-03-19 10:09:06,834] INFO: Iter 15800 Summary: 
[2025-03-19 10:09:06,834] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042142093367874625
[2025-03-19 10:09:34,658] INFO: Iter 15900 Summary: 
[2025-03-19 10:09:34,658] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04197003711014986
[2025-03-19 10:10:02,466] INFO: Iter 16000 Summary: 
[2025-03-19 10:10:02,466] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042036726325750354
[2025-03-19 10:10:30,320] INFO: Iter 16100 Summary: 
[2025-03-19 10:10:30,320] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0418594803661108
[2025-03-19 10:10:58,265] INFO: Iter 16200 Summary: 
[2025-03-19 10:10:58,265] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041890981309115886
[2025-03-19 10:11:26,231] INFO: Iter 16300 Summary: 
[2025-03-19 10:11:26,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041949711292982104
[2025-03-19 10:11:54,086] INFO: Iter 16400 Summary: 
[2025-03-19 10:11:54,086] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04225358419120312
[2025-03-19 10:12:21,898] INFO: Iter 16500 Summary: 
[2025-03-19 10:12:21,898] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04201393220573664
[2025-03-19 10:12:49,843] INFO: Iter 16600 Summary: 
[2025-03-19 10:12:49,843] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165809899568558
[2025-03-19 10:13:17,685] INFO: Iter 16700 Summary: 
[2025-03-19 10:13:17,685] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04182924184948206
[2025-03-19 10:13:45,751] INFO: Iter 16800 Summary: 
[2025-03-19 10:13:45,751] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041823956221342086
[2025-03-19 10:14:13,671] INFO: Iter 16900 Summary: 
[2025-03-19 10:14:13,671] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04194178730249405
[2025-03-19 10:14:41,472] INFO: Iter 17000 Summary: 
[2025-03-19 10:14:41,472] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04149287268519401
[2025-03-19 10:15:09,303] INFO: Iter 17100 Summary: 
[2025-03-19 10:15:09,303] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042000486440956594
[2025-03-19 10:15:37,137] INFO: Iter 17200 Summary: 
[2025-03-19 10:15:37,137] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04181637395173311
[2025-03-19 10:16:04,999] INFO: Iter 17300 Summary: 
[2025-03-19 10:16:04,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168375056236982
[2025-03-19 10:16:32,910] INFO: Iter 17400 Summary: 
[2025-03-19 10:16:32,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041925440393388275
[2025-03-19 10:17:00,677] INFO: Iter 17500 Summary: 
[2025-03-19 10:17:00,678] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041909749507904055
[2025-03-19 10:17:28,704] INFO: Iter 17600 Summary: 
[2025-03-19 10:17:28,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041702653244137765
[2025-03-19 10:17:56,748] INFO: Iter 17700 Summary: 
[2025-03-19 10:17:56,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04154967576265335
[2025-03-19 10:18:24,648] INFO: Iter 17800 Summary: 
[2025-03-19 10:18:24,648] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041657634638249876
[2025-03-19 10:18:52,534] INFO: Iter 17900 Summary: 
[2025-03-19 10:18:52,535] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041485720165073875
[2025-03-19 10:19:20,341] INFO: Iter 18000 Summary: 
[2025-03-19 10:19:20,341] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04167863667011261
[2025-03-19 10:19:48,221] INFO: Iter 18100 Summary: 
[2025-03-19 10:19:48,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153503213077783
[2025-03-19 10:20:16,037] INFO: Iter 18200 Summary: 
[2025-03-19 10:20:16,037] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04175286322832108
[2025-03-19 10:20:43,896] INFO: Iter 18300 Summary: 
[2025-03-19 10:20:43,896] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165306735783816
[2025-03-19 10:21:11,922] INFO: Iter 18400 Summary: 
[2025-03-19 10:21:11,922] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155572850257158
[2025-03-19 10:21:39,990] INFO: Iter 18500 Summary: 
[2025-03-19 10:21:39,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04146904997527599
[2025-03-19 10:22:07,985] INFO: Iter 18600 Summary: 
[2025-03-19 10:22:07,985] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04163960196077823
[2025-03-19 10:22:35,941] INFO: Iter 18700 Summary: 
[2025-03-19 10:22:35,941] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04148095481097698
[2025-03-19 10:23:03,800] INFO: Iter 18800 Summary: 
[2025-03-19 10:23:03,800] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04139668181538582
[2025-03-19 10:23:31,572] INFO: Iter 18900 Summary: 
[2025-03-19 10:23:31,572] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155068412423134
[2025-03-19 10:23:59,606] INFO: Iter 19000 Summary: 
[2025-03-19 10:23:59,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04157551810145378
[2025-03-19 10:24:27,447] INFO: Iter 19100 Summary: 
[2025-03-19 10:24:27,447] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041412553004920485
[2025-03-19 10:24:55,281] INFO: Iter 19200 Summary: 
[2025-03-19 10:24:55,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04140574678778648
[2025-03-19 10:25:23,178] INFO: Iter 19300 Summary: 
[2025-03-19 10:25:23,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145525500178337
[2025-03-19 10:25:51,042] INFO: Iter 19400 Summary: 
[2025-03-19 10:25:51,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041232666708528994
[2025-03-19 10:26:18,990] INFO: Iter 19500 Summary: 
[2025-03-19 10:26:18,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041279917657375334
[2025-03-19 10:26:46,837] INFO: Iter 19600 Summary: 
[2025-03-19 10:26:46,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153652708977461
[2025-03-19 10:27:14,674] INFO: Iter 19700 Summary: 
[2025-03-19 10:27:14,674] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041608084812760356
[2025-03-19 10:27:42,584] INFO: Iter 19800 Summary: 
[2025-03-19 10:27:42,585] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04137492183595896
[2025-03-19 10:28:10,503] INFO: Iter 19900 Summary: 
[2025-03-19 10:28:10,503] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041093552894890306
[2025-03-19 10:28:38,382] INFO: Iter 20000 Summary: 
[2025-03-19 10:28:38,382] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04138925150036812
[2025-03-19 10:29:12,998] INFO: Iter 20100 Summary: 
[2025-03-19 10:29:12,998] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041542060039937496
[2025-03-19 10:29:40,708] INFO: Iter 20200 Summary: 
[2025-03-19 10:29:40,708] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041299914196133616
[2025-03-19 10:30:08,584] INFO: Iter 20300 Summary: 
[2025-03-19 10:30:08,584] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041041623018682
[2025-03-19 10:30:36,341] INFO: Iter 20400 Summary: 
[2025-03-19 10:30:36,341] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168195985257626
[2025-03-19 10:31:04,215] INFO: Iter 20500 Summary: 
[2025-03-19 10:31:04,215] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04116824604570866
[2025-03-19 10:31:32,125] INFO: Iter 20600 Summary: 
[2025-03-19 10:31:32,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04130369149148464
[2025-03-19 10:31:59,953] INFO: Iter 20700 Summary: 
[2025-03-19 10:31:59,953] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04113837849348784
[2025-03-19 10:32:27,760] INFO: Iter 20800 Summary: 
[2025-03-19 10:32:27,760] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041218544393777846
[2025-03-19 10:32:55,681] INFO: Iter 20900 Summary: 
[2025-03-19 10:32:55,681] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068817496299744
[2025-03-19 10:33:23,455] INFO: Iter 21000 Summary: 
[2025-03-19 10:33:23,455] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110053025186062
[2025-03-19 10:33:51,535] INFO: Iter 21100 Summary: 
[2025-03-19 10:33:51,535] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041341228820383547
[2025-03-19 10:34:19,350] INFO: Iter 21200 Summary: 
[2025-03-19 10:34:19,350] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145940162241459
[2025-03-19 10:34:47,163] INFO: Iter 21300 Summary: 
[2025-03-19 10:34:47,163] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04136125065386295
[2025-03-19 10:35:15,084] INFO: Iter 21400 Summary: 
[2025-03-19 10:35:15,085] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040995805896818636
[2025-03-19 10:35:42,947] INFO: Iter 21500 Summary: 
[2025-03-19 10:35:42,947] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04089998587965965
[2025-03-19 10:36:10,772] INFO: Iter 21600 Summary: 
[2025-03-19 10:36:10,772] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0412916674092412
[2025-03-19 10:36:38,584] INFO: Iter 21700 Summary: 
[2025-03-19 10:36:38,584] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04096870582550764
[2025-03-19 10:37:06,444] INFO: Iter 21800 Summary: 
[2025-03-19 10:37:06,445] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118104003369808
[2025-03-19 10:37:34,310] INFO: Iter 21900 Summary: 
[2025-03-19 10:37:34,310] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04112858448177576
[2025-03-19 10:38:02,207] INFO: Iter 22000 Summary: 
[2025-03-19 10:38:02,207] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041169697567820546
[2025-03-19 10:38:30,802] INFO: Iter 22100 Summary: 
[2025-03-19 10:38:30,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405092404410243
[2025-03-19 10:38:58,802] INFO: Iter 22200 Summary: 
[2025-03-19 10:38:58,802] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0407518195733428
[2025-03-19 10:39:26,645] INFO: Iter 22300 Summary: 
[2025-03-19 10:39:26,645] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041050582490861416
[2025-03-19 10:39:54,518] INFO: Iter 22400 Summary: 
[2025-03-19 10:39:54,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408492099493742
[2025-03-19 10:40:22,424] INFO: Iter 22500 Summary: 
[2025-03-19 10:40:22,424] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0409961424395442
[2025-03-19 10:40:50,268] INFO: Iter 22600 Summary: 
[2025-03-19 10:40:50,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04135877802968025
[2025-03-19 10:41:18,075] INFO: Iter 22700 Summary: 
[2025-03-19 10:41:18,075] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04158662248402834
[2025-03-19 10:41:45,952] INFO: Iter 22800 Summary: 
[2025-03-19 10:41:45,953] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04100227441638708
[2025-03-19 10:42:13,899] INFO: Iter 22900 Summary: 
[2025-03-19 10:42:13,900] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04090706624090672
[2025-03-19 10:42:41,795] INFO: Iter 23000 Summary: 
[2025-03-19 10:42:41,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093770138919353
[2025-03-19 10:43:09,751] INFO: Iter 23100 Summary: 
[2025-03-19 10:43:09,751] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408388651162386
[2025-03-19 10:43:37,604] INFO: Iter 23200 Summary: 
[2025-03-19 10:43:37,604] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080089289695024
[2025-03-19 10:44:05,530] INFO: Iter 23300 Summary: 
[2025-03-19 10:44:05,530] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110186990350485
[2025-03-19 10:44:33,444] INFO: Iter 23400 Summary: 
[2025-03-19 10:44:33,445] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118562676012516
[2025-03-19 10:45:01,292] INFO: Iter 23500 Summary: 
[2025-03-19 10:45:01,292] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067198559641838
[2025-03-19 10:45:29,233] INFO: Iter 23600 Summary: 
[2025-03-19 10:45:29,233] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04073971249163151
[2025-03-19 10:45:57,112] INFO: Iter 23700 Summary: 
[2025-03-19 10:45:57,113] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118783336132765
[2025-03-19 10:46:24,969] INFO: Iter 23800 Summary: 
[2025-03-19 10:46:24,969] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04081785511225462
[2025-03-19 10:46:52,812] INFO: Iter 23900 Summary: 
[2025-03-19 10:46:52,812] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04091232247650623
[2025-03-19 10:47:20,676] INFO: Iter 24000 Summary: 
[2025-03-19 10:47:20,677] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040950687900185585
[2025-03-19 10:47:48,499] INFO: Iter 24100 Summary: 
[2025-03-19 10:47:48,500] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093142617493868
[2025-03-19 10:48:16,386] INFO: Iter 24200 Summary: 
[2025-03-19 10:48:16,386] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04088189199566841
[2025-03-19 10:48:44,166] INFO: Iter 24300 Summary: 
[2025-03-19 10:48:44,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060963463038206
[2025-03-19 10:49:12,141] INFO: Iter 24400 Summary: 
[2025-03-19 10:49:12,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053705725818872
[2025-03-19 10:49:39,936] INFO: Iter 24500 Summary: 
[2025-03-19 10:49:39,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040698427967727185
[2025-03-19 10:50:07,988] INFO: Iter 24600 Summary: 
[2025-03-19 10:50:07,988] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095322467386722
[2025-03-19 10:50:35,973] INFO: Iter 24700 Summary: 
[2025-03-19 10:50:35,973] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080451674759388
[2025-03-19 10:51:03,878] INFO: Iter 24800 Summary: 
[2025-03-19 10:51:03,878] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408536421507597
[2025-03-19 10:51:31,691] INFO: Iter 24900 Summary: 
[2025-03-19 10:51:31,691] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040972840487957
[2025-03-19 10:51:59,631] INFO: Iter 25000 Summary: 
[2025-03-19 10:51:59,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084993671625853
[2025-03-19 10:52:34,137] INFO: Iter 25100 Summary: 
[2025-03-19 10:52:34,137] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061872538179159
[2025-03-19 10:53:01,952] INFO: Iter 25200 Summary: 
[2025-03-19 10:53:01,952] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04043906804174185
[2025-03-19 10:53:29,786] INFO: Iter 25300 Summary: 
[2025-03-19 10:53:29,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040832564756274226
[2025-03-19 10:53:57,613] INFO: Iter 25400 Summary: 
[2025-03-19 10:53:57,613] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074161302298308
[2025-03-19 10:54:25,574] INFO: Iter 25500 Summary: 
[2025-03-19 10:54:25,574] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041060957312583926
[2025-03-19 10:54:53,413] INFO: Iter 25600 Summary: 
[2025-03-19 10:54:53,413] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404279800504446
[2025-03-19 10:55:21,465] INFO: Iter 25700 Summary: 
[2025-03-19 10:55:21,465] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068828206509352
[2025-03-19 10:55:49,422] INFO: Iter 25800 Summary: 
[2025-03-19 10:55:49,422] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069079078733921
[2025-03-19 10:56:17,175] INFO: Iter 25900 Summary: 
[2025-03-19 10:56:17,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04078167922794819
[2025-03-19 10:56:45,032] INFO: Iter 26000 Summary: 
[2025-03-19 10:56:45,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040673726387321946
[2025-03-19 10:57:12,846] INFO: Iter 26100 Summary: 
[2025-03-19 10:57:12,846] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095667663961649
[2025-03-19 10:57:40,809] INFO: Iter 26200 Summary: 
[2025-03-19 10:57:40,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048498272895813
[2025-03-19 10:58:08,773] INFO: Iter 26300 Summary: 
[2025-03-19 10:58:08,773] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068731527775526
[2025-03-19 10:58:36,565] INFO: Iter 26400 Summary: 
[2025-03-19 10:58:36,565] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408188908547163
[2025-03-19 10:59:04,370] INFO: Iter 26500 Summary: 
[2025-03-19 10:59:04,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04085890453308821
[2025-03-19 10:59:32,252] INFO: Iter 26600 Summary: 
[2025-03-19 10:59:32,252] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040534629449248315
[2025-03-19 11:00:00,112] INFO: Iter 26700 Summary: 
[2025-03-19 11:00:00,112] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084464132785797
[2025-03-19 11:00:27,976] INFO: Iter 26800 Summary: 
[2025-03-19 11:00:27,976] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053069945424795
[2025-03-19 11:00:55,797] INFO: Iter 26900 Summary: 
[2025-03-19 11:00:55,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040677514709532264
[2025-03-19 11:01:23,717] INFO: Iter 27000 Summary: 
[2025-03-19 11:01:23,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069628566503525
[2025-03-19 11:01:51,529] INFO: Iter 27100 Summary: 
[2025-03-19 11:01:51,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040576146207749846
[2025-03-19 11:02:19,387] INFO: Iter 27200 Summary: 
[2025-03-19 11:02:19,387] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061826728284359
[2025-03-19 11:02:47,258] INFO: Iter 27300 Summary: 
[2025-03-19 11:02:47,259] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040502460934221746
[2025-03-19 11:03:15,114] INFO: Iter 27400 Summary: 
[2025-03-19 11:03:15,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040391418300569054
[2025-03-19 11:03:43,011] INFO: Iter 27500 Summary: 
[2025-03-19 11:03:43,012] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060128532350063
[2025-03-19 11:04:10,890] INFO: Iter 27600 Summary: 
[2025-03-19 11:04:10,890] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04030371893197298
[2025-03-19 11:04:38,735] INFO: Iter 27700 Summary: 
[2025-03-19 11:04:38,735] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04055089380592108
[2025-03-19 11:05:06,744] INFO: Iter 27800 Summary: 
[2025-03-19 11:05:06,744] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0406524433195591
[2025-03-19 11:05:35,105] INFO: Iter 27900 Summary: 
[2025-03-19 11:05:35,105] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058287750929594
[2025-03-19 11:06:03,354] INFO: Iter 28000 Summary: 
[2025-03-19 11:06:03,354] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067306347191334
[2025-03-19 11:06:31,296] INFO: Iter 28100 Summary: 
[2025-03-19 11:06:31,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04037954095751047
[2025-03-19 11:06:59,195] INFO: Iter 28200 Summary: 
[2025-03-19 11:06:59,195] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04038192193955183
[2025-03-19 11:07:27,212] INFO: Iter 28300 Summary: 
[2025-03-19 11:07:27,212] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040238456577062605
[2025-03-19 11:07:55,076] INFO: Iter 28400 Summary: 
[2025-03-19 11:07:55,076] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04046010196208954
[2025-03-19 11:08:23,039] INFO: Iter 28500 Summary: 
[2025-03-19 11:08:23,040] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04056391391903162
[2025-03-19 11:08:50,935] INFO: Iter 28600 Summary: 
[2025-03-19 11:08:50,935] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040525965727865695
[2025-03-19 11:09:18,803] INFO: Iter 28700 Summary: 
[2025-03-19 11:09:18,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074303459376097
[2025-03-19 11:09:46,721] INFO: Iter 28800 Summary: 
[2025-03-19 11:09:46,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405935624986887
[2025-03-19 11:10:14,752] INFO: Iter 28900 Summary: 
[2025-03-19 11:10:14,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084486674517393
[2025-03-19 11:10:42,554] INFO: Iter 29000 Summary: 
[2025-03-19 11:10:42,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040621427595615385
[2025-03-19 11:11:17,062] INFO: Iter 29100 Summary: 
[2025-03-19 11:11:17,063] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04022378820925951
[2025-03-19 11:11:44,960] INFO: Iter 29200 Summary: 
[2025-03-19 11:11:44,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040208502225577834
[2025-03-19 11:12:12,851] INFO: Iter 29300 Summary: 
[2025-03-19 11:12:12,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040462910942733285
[2025-03-19 11:12:40,993] INFO: Iter 29400 Summary: 
[2025-03-19 11:12:40,993] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04041025843471289
[2025-03-19 11:13:08,847] INFO: Iter 29500 Summary: 
[2025-03-19 11:13:08,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404819118976593
[2025-03-19 11:13:36,757] INFO: Iter 29600 Summary: 
[2025-03-19 11:13:36,757] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04064005315303802
[2025-03-19 11:14:04,642] INFO: Iter 29700 Summary: 
[2025-03-19 11:14:04,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048785202205181
[2025-03-19 11:14:32,471] INFO: Iter 29800 Summary: 
[2025-03-19 11:14:32,471] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04010299898684025
[2025-03-19 11:15:00,321] INFO: Iter 29900 Summary: 
[2025-03-19 11:15:00,322] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04034921482205391
[2025-03-19 11:15:28,178] INFO: Iter 30000 Summary: 
[2025-03-19 11:15:28,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058229215443134
[2025-03-19 11:16:02,480] INFO: Iter 30100 Summary: 
[2025-03-19 11:16:02,480] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.03973235040903091
[2025-03-19 11:16:30,359] INFO: Iter 30200 Summary: 
[2025-03-19 11:16:30,359] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039118692390620706
[2025-03-19 11:16:58,311] INFO: Iter 30300 Summary: 
[2025-03-19 11:16:58,312] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03928719613701105
[2025-03-19 11:17:26,277] INFO: Iter 30400 Summary: 
[2025-03-19 11:17:26,278] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038738011457026
[2025-03-19 11:17:54,209] INFO: Iter 30500 Summary: 
[2025-03-19 11:17:54,209] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03889032088220119
[2025-03-19 11:18:22,239] INFO: Iter 30600 Summary: 
[2025-03-19 11:18:22,239] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03861566469073296
[2025-03-19 11:18:50,184] INFO: Iter 30700 Summary: 
[2025-03-19 11:18:50,184] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03872161664068699
[2025-03-19 11:19:18,074] INFO: Iter 30800 Summary: 
[2025-03-19 11:19:18,074] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038844494484364986
[2025-03-19 11:19:46,037] INFO: Iter 30900 Summary: 
[2025-03-19 11:19:46,038] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038778597079217436
[2025-03-19 11:20:13,954] INFO: Iter 31000 Summary: 
[2025-03-19 11:20:13,954] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038575371727347374
[2025-03-19 11:20:48,299] INFO: Iter 31100 Summary: 
[2025-03-19 11:20:48,299] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038529667183756826
[2025-03-19 11:21:16,275] INFO: Iter 31200 Summary: 
[2025-03-19 11:21:16,275] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038481415137648585
[2025-03-19 11:21:44,087] INFO: Iter 31300 Summary: 
[2025-03-19 11:21:44,087] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03858761254698038
[2025-03-19 11:22:12,053] INFO: Iter 31400 Summary: 
[2025-03-19 11:22:12,053] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038527636267244814
[2025-03-19 11:22:39,941] INFO: Iter 31500 Summary: 
[2025-03-19 11:22:39,941] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859547201544047
[2025-03-19 11:23:07,823] INFO: Iter 31600 Summary: 
[2025-03-19 11:23:07,824] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03838738303631544
[2025-03-19 11:23:35,720] INFO: Iter 31700 Summary: 
[2025-03-19 11:23:35,721] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855763614177704
[2025-03-19 11:24:03,692] INFO: Iter 31800 Summary: 
[2025-03-19 11:24:03,692] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843981295824051
[2025-03-19 11:24:31,533] INFO: Iter 31900 Summary: 
[2025-03-19 11:24:31,534] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038455543145537376
[2025-03-19 11:24:59,370] INFO: Iter 32000 Summary: 
[2025-03-19 11:24:59,371] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03880749378353357
[2025-03-19 11:25:33,723] INFO: Iter 32100 Summary: 
[2025-03-19 11:25:33,724] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851503048092127
[2025-03-19 11:26:01,843] INFO: Iter 32200 Summary: 
[2025-03-19 11:26:01,843] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03850121602416039
[2025-03-19 11:26:29,695] INFO: Iter 32300 Summary: 
[2025-03-19 11:26:29,695] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816764749586582
[2025-03-19 11:26:57,530] INFO: Iter 32400 Summary: 
[2025-03-19 11:26:57,530] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03848932571709156
[2025-03-19 11:27:25,579] INFO: Iter 32500 Summary: 
[2025-03-19 11:27:25,579] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859693259000778
[2025-03-19 11:27:53,430] INFO: Iter 32600 Summary: 
[2025-03-19 11:27:53,430] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038555137366056445
[2025-03-19 11:28:21,359] INFO: Iter 32700 Summary: 
[2025-03-19 11:28:21,359] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837333284318447
[2025-03-19 11:28:49,278] INFO: Iter 32800 Summary: 
[2025-03-19 11:28:49,278] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03845605220645666
[2025-03-19 11:29:17,193] INFO: Iter 32900 Summary: 
[2025-03-19 11:29:17,193] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03870507031679153
[2025-03-19 11:29:44,987] INFO: Iter 33000 Summary: 
[2025-03-19 11:29:44,987] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817356590181589
[2025-03-19 11:30:19,285] INFO: Iter 33100 Summary: 
[2025-03-19 11:30:19,285] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383727602660656
[2025-03-19 11:30:47,120] INFO: Iter 33200 Summary: 
[2025-03-19 11:30:47,120] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03840629003942013
[2025-03-19 11:31:15,078] INFO: Iter 33300 Summary: 
[2025-03-19 11:31:15,078] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837062247097492
[2025-03-19 11:31:42,861] INFO: Iter 33400 Summary: 
[2025-03-19 11:31:42,862] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03860242497175932
[2025-03-19 11:32:10,683] INFO: Iter 33500 Summary: 
[2025-03-19 11:32:10,683] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03846450563520193
[2025-03-19 11:32:38,688] INFO: Iter 33600 Summary: 
[2025-03-19 11:32:38,688] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804883323609829
[2025-03-19 11:33:06,801] INFO: Iter 33700 Summary: 
[2025-03-19 11:33:06,802] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038162836283445356
[2025-03-19 11:33:34,745] INFO: Iter 33800 Summary: 
[2025-03-19 11:33:34,746] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03809049587696791
[2025-03-19 11:34:02,774] INFO: Iter 33900 Summary: 
[2025-03-19 11:34:02,774] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833961393684149
[2025-03-19 11:34:30,814] INFO: Iter 34000 Summary: 
[2025-03-19 11:34:30,814] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03821747232228517
[2025-03-19 11:35:05,282] INFO: Iter 34100 Summary: 
[2025-03-19 11:35:05,282] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843688197433948
[2025-03-19 11:35:33,184] INFO: Iter 34200 Summary: 
[2025-03-19 11:35:33,184] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03820082072168589
[2025-03-19 11:36:01,144] INFO: Iter 34300 Summary: 
[2025-03-19 11:36:01,144] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038492189683020116
[2025-03-19 11:36:29,097] INFO: Iter 34400 Summary: 
[2025-03-19 11:36:29,097] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851904425770045
[2025-03-19 11:36:57,019] INFO: Iter 34500 Summary: 
[2025-03-19 11:36:57,019] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038166586831212046
[2025-03-19 11:37:24,895] INFO: Iter 34600 Summary: 
[2025-03-19 11:37:24,896] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836640920490027
[2025-03-19 11:37:52,847] INFO: Iter 34700 Summary: 
[2025-03-19 11:37:52,847] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814776938408613
[2025-03-19 11:38:20,757] INFO: Iter 34800 Summary: 
[2025-03-19 11:38:20,757] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03841575048863888
[2025-03-19 11:38:48,664] INFO: Iter 34900 Summary: 
[2025-03-19 11:38:48,664] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837616860866547
[2025-03-19 11:39:16,689] INFO: Iter 35000 Summary: 
[2025-03-19 11:39:16,689] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825340036302805
[2025-03-19 11:39:50,962] INFO: Iter 35100 Summary: 
[2025-03-19 11:39:50,962] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836090188473463
[2025-03-19 11:40:19,354] INFO: Iter 35200 Summary: 
[2025-03-19 11:40:19,355] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038214359544217584
[2025-03-19 11:40:47,286] INFO: Iter 35300 Summary: 
[2025-03-19 11:40:47,287] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855342034250498
[2025-03-19 11:41:15,295] INFO: Iter 35400 Summary: 
[2025-03-19 11:41:15,295] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03805466830730438
[2025-03-19 11:41:43,210] INFO: Iter 35500 Summary: 
[2025-03-19 11:41:43,210] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822425208985805
[2025-03-19 11:42:11,107] INFO: Iter 35600 Summary: 
[2025-03-19 11:42:11,107] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038065870590507984
[2025-03-19 11:42:38,951] INFO: Iter 35700 Summary: 
[2025-03-19 11:42:38,952] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814086750149727
[2025-03-19 11:43:06,879] INFO: Iter 35800 Summary: 
[2025-03-19 11:43:06,880] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817138198763132
[2025-03-19 11:43:34,807] INFO: Iter 35900 Summary: 
[2025-03-19 11:43:34,807] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818887110799551
[2025-03-19 11:44:02,688] INFO: Iter 36000 Summary: 
[2025-03-19 11:44:02,688] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825825229287148
[2025-03-19 11:44:37,255] INFO: Iter 36100 Summary: 
[2025-03-19 11:44:37,255] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843956135213375
[2025-03-19 11:45:05,246] INFO: Iter 36200 Summary: 
[2025-03-19 11:45:05,246] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038188811540603634
[2025-03-19 11:45:33,383] INFO: Iter 36300 Summary: 
[2025-03-19 11:45:33,383] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816936280578375
[2025-03-19 11:46:01,267] INFO: Iter 36400 Summary: 
[2025-03-19 11:46:01,267] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03824284512549639
[2025-03-19 11:46:29,295] INFO: Iter 36500 Summary: 
[2025-03-19 11:46:29,295] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038122556991875174
[2025-03-19 11:46:57,209] INFO: Iter 36600 Summary: 
[2025-03-19 11:46:57,209] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381810512393713
[2025-03-19 11:47:25,246] INFO: Iter 36700 Summary: 
[2025-03-19 11:47:25,246] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822219908237457
[2025-03-19 11:47:53,257] INFO: Iter 36800 Summary: 
[2025-03-19 11:47:53,257] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038186385147273544
[2025-03-19 11:48:21,330] INFO: Iter 36900 Summary: 
[2025-03-19 11:48:21,330] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03811948042362928
[2025-03-19 11:48:49,231] INFO: Iter 37000 Summary: 
[2025-03-19 11:48:49,231] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037993066012859344
[2025-03-19 11:49:17,439] INFO: Iter 37100 Summary: 
[2025-03-19 11:49:17,440] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822493091225624
[2025-03-19 11:49:45,485] INFO: Iter 37200 Summary: 
[2025-03-19 11:49:45,486] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815663915127516
[2025-03-19 11:50:13,523] INFO: Iter 37300 Summary: 
[2025-03-19 11:50:13,523] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038145889826118946
[2025-03-19 11:50:41,480] INFO: Iter 37400 Summary: 
[2025-03-19 11:50:41,480] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038162118792533874
[2025-03-19 11:51:09,484] INFO: Iter 37500 Summary: 
[2025-03-19 11:51:09,484] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03792085308581591
[2025-03-19 11:51:37,358] INFO: Iter 37600 Summary: 
[2025-03-19 11:51:37,358] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817931767553091
[2025-03-19 11:52:05,257] INFO: Iter 37700 Summary: 
[2025-03-19 11:52:05,257] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822246626019478
[2025-03-19 11:52:33,249] INFO: Iter 37800 Summary: 
[2025-03-19 11:52:33,249] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038163269944489
[2025-03-19 11:53:01,326] INFO: Iter 37900 Summary: 
[2025-03-19 11:53:01,326] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03788881104439497
[2025-03-19 11:53:29,259] INFO: Iter 38000 Summary: 
[2025-03-19 11:53:29,259] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833088543266058
[2025-03-19 11:54:03,729] INFO: Iter 38100 Summary: 
[2025-03-19 11:54:03,730] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03810572013258934
[2025-03-19 11:54:31,796] INFO: Iter 38200 Summary: 
[2025-03-19 11:54:31,796] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038000543266534806
[2025-03-19 11:54:59,666] INFO: Iter 38300 Summary: 
[2025-03-19 11:54:59,666] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03830586340278387
[2025-03-19 11:55:27,767] INFO: Iter 38400 Summary: 
[2025-03-19 11:55:27,767] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03799467198550701
[2025-03-19 11:55:55,880] INFO: Iter 38500 Summary: 
[2025-03-19 11:55:55,880] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038415354825556275
[2025-03-19 11:56:24,095] INFO: Iter 38600 Summary: 
[2025-03-19 11:56:24,095] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038089186288416385
[2025-03-19 11:56:52,149] INFO: Iter 38700 Summary: 
[2025-03-19 11:56:52,149] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03842436783015728
[2025-03-19 11:57:20,057] INFO: Iter 38800 Summary: 
[2025-03-19 11:57:20,057] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038075441904366014
[2025-03-19 11:57:48,201] INFO: Iter 38900 Summary: 
[2025-03-19 11:57:48,201] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816499762237072
[2025-03-19 11:58:17,136] INFO: Iter 39000 Summary: 
[2025-03-19 11:58:17,136] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814488835632801
[2025-03-19 11:58:45,507] INFO: Iter 39100 Summary: 
[2025-03-19 11:58:45,507] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804252605885267
[2025-03-19 11:59:13,636] INFO: Iter 39200 Summary: 
[2025-03-19 11:59:13,636] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037957499362528326
[2025-03-19 11:59:41,691] INFO: Iter 39300 Summary: 
[2025-03-19 11:59:41,691] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03790841404348612
[2025-03-19 12:00:09,771] INFO: Iter 39400 Summary: 
[2025-03-19 12:00:09,772] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03830685757100582
[2025-03-19 12:00:37,851] INFO: Iter 39500 Summary: 
[2025-03-19 12:00:37,851] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817176405340433
[2025-03-19 12:01:05,841] INFO: Iter 39600 Summary: 
[2025-03-19 12:01:05,841] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816769782453775
[2025-03-19 12:01:33,985] INFO: Iter 39700 Summary: 
[2025-03-19 12:01:33,985] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037901812009513376
[2025-03-19 12:02:01,988] INFO: Iter 39800 Summary: 
[2025-03-19 12:02:01,989] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037788297086954116
[2025-03-19 12:02:30,208] INFO: Iter 39900 Summary: 
[2025-03-19 12:02:30,209] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037743132561445236
[2025-03-19 12:02:58,274] INFO: Iter 40000 Summary: 
[2025-03-19 12:02:58,274] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037958942987024784
[2025-03-27 10:35:24,760] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_27_10_35_01.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_27_10_35_01.log",
    "weight_decay": 0.0001
}
[2025-03-27 10:35:53,620] INFO: Iter 100 Summary: 
[2025-03-27 10:35:53,620] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-27 10:36:21,611] INFO: Iter 200 Summary: 
[2025-03-27 10:36:21,611] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-27 10:36:49,736] INFO: Iter 300 Summary: 
[2025-03-27 10:36:49,737] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-27 10:37:18,272] INFO: Iter 400 Summary: 
[2025-03-27 10:37:18,272] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-27 10:37:46,780] INFO: Iter 500 Summary: 
[2025-03-27 10:37:46,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-27 10:38:14,899] INFO: Iter 600 Summary: 
[2025-03-27 10:38:14,899] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-27 10:38:43,353] INFO: Iter 700 Summary: 
[2025-03-27 10:38:43,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-27 10:39:11,761] INFO: Iter 800 Summary: 
[2025-03-27 10:39:11,761] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-27 10:39:40,150] INFO: Iter 900 Summary: 
[2025-03-27 10:39:40,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07361114241182803
[2025-03-27 10:40:10,092] INFO: Iter 1000 Summary: 
[2025-03-27 10:40:10,093] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0712160461395979
[2025-03-27 10:40:38,714] INFO: Iter 1100 Summary: 
[2025-03-27 10:40:38,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0662378877773881
[2025-03-27 10:41:07,175] INFO: Iter 1200 Summary: 
[2025-03-27 10:41:07,175] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06260723497718573
[2025-03-27 10:41:36,017] INFO: Iter 1300 Summary: 
[2025-03-27 10:41:36,017] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06077460564672947
[2025-03-27 10:42:08,266] INFO: Iter 1400 Summary: 
[2025-03-27 10:42:08,266] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059101610518991946
[2025-03-27 10:42:41,301] INFO: Iter 1500 Summary: 
[2025-03-27 10:42:41,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05815540604293346
[2025-03-27 10:43:15,405] INFO: Iter 1600 Summary: 
[2025-03-27 10:43:15,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768500231206417
[2025-03-27 10:43:50,753] INFO: Iter 1700 Summary: 
[2025-03-27 10:43:50,754] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05612747546285391
[2025-03-27 10:44:20,781] INFO: Iter 1800 Summary: 
[2025-03-27 10:44:20,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0559469723328948
[2025-03-27 10:44:49,420] INFO: Iter 1900 Summary: 
[2025-03-27 10:44:49,421] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055011885352432725
[2025-03-27 10:45:18,821] INFO: Iter 2000 Summary: 
[2025-03-27 10:45:18,821] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054927452243864534
[2025-03-27 10:45:47,881] INFO: Iter 2100 Summary: 
[2025-03-27 10:45:47,881] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05467277605086565
[2025-03-27 10:46:16,876] INFO: Iter 2200 Summary: 
[2025-03-27 10:46:16,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05439220994710922
[2025-03-27 10:46:45,660] INFO: Iter 2300 Summary: 
[2025-03-27 10:46:45,660] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05411341149359941
[2025-03-27 10:47:14,149] INFO: Iter 2400 Summary: 
[2025-03-27 10:47:14,149] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05394661214202642
[2025-03-27 10:47:42,852] INFO: Iter 2500 Summary: 
[2025-03-27 10:47:42,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05381324980407953
[2025-03-27 10:48:11,729] INFO: Iter 2600 Summary: 
[2025-03-27 10:48:11,729] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053067084886133674
[2025-03-27 10:48:42,151] INFO: Iter 2700 Summary: 
[2025-03-27 10:48:42,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05291106801480055
[2025-03-27 10:49:17,393] INFO: Iter 2800 Summary: 
[2025-03-27 10:49:17,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268501613289118
[2025-03-27 10:49:45,993] INFO: Iter 2900 Summary: 
[2025-03-27 10:49:45,993] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05244477484375239
[2025-03-27 10:50:17,698] INFO: Iter 3000 Summary: 
[2025-03-27 10:50:17,698] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052546527571976184
[2025-03-27 10:50:45,903] INFO: Iter 3100 Summary: 
[2025-03-27 10:50:45,903] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051631263718008996
[2025-03-27 10:51:14,306] INFO: Iter 3200 Summary: 
[2025-03-27 10:51:14,306] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05216690514236689
[2025-03-27 10:51:42,555] INFO: Iter 3300 Summary: 
[2025-03-27 10:51:42,555] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05180593386292458
[2025-03-27 10:52:10,882] INFO: Iter 3400 Summary: 
[2025-03-27 10:52:10,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128339599817991
[2025-03-27 10:52:39,178] INFO: Iter 3500 Summary: 
[2025-03-27 10:52:39,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05161994945257902
[2025-03-27 10:53:07,712] INFO: Iter 3600 Summary: 
[2025-03-27 10:53:07,712] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051079065576195716
[2025-03-27 10:53:35,901] INFO: Iter 3700 Summary: 
[2025-03-27 10:53:35,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05093852359801531
[2025-03-27 10:54:04,239] INFO: Iter 3800 Summary: 
[2025-03-27 10:54:04,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050728333853185176
[2025-03-27 10:54:32,395] INFO: Iter 3900 Summary: 
[2025-03-27 10:54:32,395] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051007907390594485
[2025-03-27 10:55:00,450] INFO: Iter 4000 Summary: 
[2025-03-27 10:55:00,450] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128511141985655
[2025-03-27 10:55:28,765] INFO: Iter 4100 Summary: 
[2025-03-27 10:55:28,765] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050616980902850627
[2025-03-27 10:55:56,851] INFO: Iter 4200 Summary: 
[2025-03-27 10:55:56,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05099097285419703
[2025-03-27 10:56:24,850] INFO: Iter 4300 Summary: 
[2025-03-27 10:56:24,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05006971627473831
[2025-03-27 10:56:52,905] INFO: Iter 4400 Summary: 
[2025-03-27 10:56:52,905] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05037182774394751
[2025-03-27 10:57:21,046] INFO: Iter 4500 Summary: 
[2025-03-27 10:57:21,047] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050234943144023415
[2025-03-27 10:57:49,260] INFO: Iter 4600 Summary: 
[2025-03-27 10:57:49,261] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05055206254124642
[2025-03-27 10:58:17,624] INFO: Iter 4700 Summary: 
[2025-03-27 10:58:17,625] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04994991168379784
[2025-03-27 10:58:45,742] INFO: Iter 4800 Summary: 
[2025-03-27 10:58:45,743] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05021801430732012
[2025-03-27 10:59:13,865] INFO: Iter 4900 Summary: 
[2025-03-27 10:59:13,865] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049785437248647216
[2025-03-27 10:59:42,008] INFO: Iter 5000 Summary: 
[2025-03-27 10:59:42,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04928127493709326
[2025-03-27 11:16:03,290] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_27_11_15_46.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_27_11_15_46.log",
    "weight_decay": 0.0001
}
[2025-03-27 11:16:31,449] INFO: Iter 100 Summary: 
[2025-03-27 11:16:31,450] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-27 11:16:59,121] INFO: Iter 200 Summary: 
[2025-03-27 11:16:59,121] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-27 11:17:26,863] INFO: Iter 300 Summary: 
[2025-03-27 11:17:26,863] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-27 11:17:54,672] INFO: Iter 400 Summary: 
[2025-03-27 11:17:54,672] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-27 11:18:22,373] INFO: Iter 500 Summary: 
[2025-03-27 11:18:22,373] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-27 11:18:50,284] INFO: Iter 600 Summary: 
[2025-03-27 11:18:50,284] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-27 11:19:18,328] INFO: Iter 700 Summary: 
[2025-03-27 11:19:18,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-27 11:19:46,314] INFO: Iter 800 Summary: 
[2025-03-27 11:19:46,314] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-27 11:20:14,472] INFO: Iter 900 Summary: 
[2025-03-27 11:20:14,472] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07361114241182803
[2025-03-27 11:20:42,554] INFO: Iter 1000 Summary: 
[2025-03-27 11:20:42,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0712160461395979
[2025-03-27 11:21:10,522] INFO: Iter 1100 Summary: 
[2025-03-27 11:21:10,522] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0662378877773881
[2025-03-27 11:21:38,566] INFO: Iter 1200 Summary: 
[2025-03-27 11:21:38,566] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06260723497718573
[2025-03-27 11:22:06,558] INFO: Iter 1300 Summary: 
[2025-03-27 11:22:06,558] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06077460564672947
[2025-03-27 11:22:34,613] INFO: Iter 1400 Summary: 
[2025-03-27 11:22:34,613] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059101610518991946
[2025-03-27 11:23:02,777] INFO: Iter 1500 Summary: 
[2025-03-27 11:23:02,777] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05815540604293346
[2025-03-27 11:23:32,455] INFO: Iter 1600 Summary: 
[2025-03-27 11:23:32,455] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768500231206417
[2025-03-27 11:24:00,623] INFO: Iter 1700 Summary: 
[2025-03-27 11:24:00,623] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05612747546285391
[2025-03-27 11:24:28,736] INFO: Iter 1800 Summary: 
[2025-03-27 11:24:28,736] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0559469723328948
[2025-03-27 11:24:56,817] INFO: Iter 1900 Summary: 
[2025-03-27 11:24:56,817] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055011885352432725
[2025-03-27 11:25:25,160] INFO: Iter 2000 Summary: 
[2025-03-27 11:25:25,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054927452243864534
[2025-03-27 11:25:53,896] INFO: Iter 2100 Summary: 
[2025-03-27 11:25:53,897] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05467277605086565
[2025-03-27 11:26:21,995] INFO: Iter 2200 Summary: 
[2025-03-27 11:26:21,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05439220994710922
[2025-03-27 11:26:50,078] INFO: Iter 2300 Summary: 
[2025-03-27 11:26:50,078] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05411341149359941
[2025-03-27 11:27:18,206] INFO: Iter 2400 Summary: 
[2025-03-27 11:27:18,206] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05394661214202642
[2025-03-27 11:27:46,153] INFO: Iter 2500 Summary: 
[2025-03-27 11:27:46,153] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05381324980407953
[2025-03-27 11:28:14,144] INFO: Iter 2600 Summary: 
[2025-03-27 11:28:14,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053067084886133674
[2025-03-27 11:28:42,474] INFO: Iter 2700 Summary: 
[2025-03-27 11:28:42,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05291106801480055
[2025-03-27 11:29:10,852] INFO: Iter 2800 Summary: 
[2025-03-27 11:29:10,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268501613289118
[2025-03-27 11:29:39,031] INFO: Iter 2900 Summary: 
[2025-03-27 11:29:39,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05244477484375239
[2025-03-27 11:30:07,127] INFO: Iter 3000 Summary: 
[2025-03-27 11:30:07,127] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052546527571976184
[2025-03-27 11:30:35,243] INFO: Iter 3100 Summary: 
[2025-03-27 11:30:35,243] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051631263718008996
[2025-03-27 11:31:03,306] INFO: Iter 3200 Summary: 
[2025-03-27 11:31:03,307] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05216690514236689
[2025-03-27 11:31:31,414] INFO: Iter 3300 Summary: 
[2025-03-27 11:31:31,414] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05180593386292458
[2025-03-27 11:31:59,591] INFO: Iter 3400 Summary: 
[2025-03-27 11:31:59,591] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128339599817991
[2025-03-27 11:32:27,760] INFO: Iter 3500 Summary: 
[2025-03-27 11:32:27,761] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05161994945257902
[2025-03-27 11:32:56,000] INFO: Iter 3600 Summary: 
[2025-03-27 11:32:56,000] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051079065576195716
[2025-03-27 11:33:24,301] INFO: Iter 3700 Summary: 
[2025-03-27 11:33:24,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05093852359801531
[2025-03-27 11:33:52,564] INFO: Iter 3800 Summary: 
[2025-03-27 11:33:52,564] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050728333853185176
[2025-03-27 11:34:21,937] INFO: Iter 3900 Summary: 
[2025-03-27 11:34:21,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051007907390594485
[2025-03-27 11:34:50,691] INFO: Iter 4000 Summary: 
[2025-03-27 11:34:50,691] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128511141985655
[2025-03-27 11:35:19,463] INFO: Iter 4100 Summary: 
[2025-03-27 11:35:19,463] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050616980902850627
[2025-03-27 11:35:48,175] INFO: Iter 4200 Summary: 
[2025-03-27 11:35:48,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05099097285419703
[2025-03-27 11:36:16,838] INFO: Iter 4300 Summary: 
[2025-03-27 11:36:16,838] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05006971627473831
[2025-03-27 11:36:45,577] INFO: Iter 4400 Summary: 
[2025-03-27 11:36:45,577] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05037182774394751
[2025-03-27 11:37:14,304] INFO: Iter 4500 Summary: 
[2025-03-27 11:37:14,304] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050234943144023415
[2025-03-27 11:37:42,466] INFO: Iter 4600 Summary: 
[2025-03-27 11:37:42,467] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05055206254124642
[2025-03-27 11:38:10,877] INFO: Iter 4700 Summary: 
[2025-03-27 11:38:10,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04994991168379784
[2025-03-27 11:38:38,993] INFO: Iter 4800 Summary: 
[2025-03-27 11:38:38,993] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05021801430732012
[2025-03-27 11:39:07,261] INFO: Iter 4900 Summary: 
[2025-03-27 11:39:07,262] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049785437248647216
[2025-03-27 11:39:36,254] INFO: Iter 5000 Summary: 
[2025-03-27 11:39:36,254] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04928127493709326
[2025-03-27 11:40:11,197] INFO: Iter 5100 Summary: 
[2025-03-27 11:40:11,197] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04932282216846943
[2025-03-27 11:40:39,445] INFO: Iter 5200 Summary: 
[2025-03-27 11:40:39,445] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0498723316937685
[2025-03-27 11:41:07,705] INFO: Iter 5300 Summary: 
[2025-03-27 11:41:07,705] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049440183341503144
[2025-03-27 11:41:35,936] INFO: Iter 5400 Summary: 
[2025-03-27 11:41:35,936] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049165061116218566
[2025-03-27 11:42:04,368] INFO: Iter 5500 Summary: 
[2025-03-27 11:42:04,368] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049324527978897095
[2025-03-27 11:42:32,631] INFO: Iter 5600 Summary: 
[2025-03-27 11:42:32,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487074363976717
[2025-03-27 11:43:01,477] INFO: Iter 5700 Summary: 
[2025-03-27 11:43:01,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04891889002174139
[2025-03-27 11:43:30,524] INFO: Iter 5800 Summary: 
[2025-03-27 11:43:30,524] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487964428216219
[2025-03-27 11:43:58,867] INFO: Iter 5900 Summary: 
[2025-03-27 11:43:58,867] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048565466962754725
[2025-03-27 11:44:27,122] INFO: Iter 6000 Summary: 
[2025-03-27 11:44:27,122] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04910611681640148
[2025-03-27 11:44:55,861] INFO: Iter 6100 Summary: 
[2025-03-27 11:44:55,862] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04909805666655302
[2025-03-27 11:45:24,995] INFO: Iter 6200 Summary: 
[2025-03-27 11:45:24,996] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048542876280844215
[2025-03-27 11:45:53,009] INFO: Iter 6300 Summary: 
[2025-03-27 11:45:53,009] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048175370916724206
[2025-03-27 11:46:21,223] INFO: Iter 6400 Summary: 
[2025-03-27 11:46:21,223] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048257682099938394
[2025-03-27 11:46:49,267] INFO: Iter 6500 Summary: 
[2025-03-27 11:46:49,267] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04780059069395065
[2025-03-27 11:47:17,376] INFO: Iter 6600 Summary: 
[2025-03-27 11:47:17,376] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04812747694551945
[2025-03-27 11:47:45,536] INFO: Iter 6700 Summary: 
[2025-03-27 11:47:45,536] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04789849977940321
[2025-03-27 11:48:13,703] INFO: Iter 6800 Summary: 
[2025-03-27 11:48:13,703] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04803676519542933
[2025-03-27 11:48:41,900] INFO: Iter 6900 Summary: 
[2025-03-27 11:48:41,900] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047816569320857524
[2025-03-27 11:49:10,087] INFO: Iter 7000 Summary: 
[2025-03-27 11:49:10,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04756761498749256
[2025-03-27 11:49:38,204] INFO: Iter 7100 Summary: 
[2025-03-27 11:49:38,204] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047559353597462176
[2025-03-27 11:50:06,362] INFO: Iter 7200 Summary: 
[2025-03-27 11:50:06,362] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047472536452114585
[2025-03-27 11:50:34,498] INFO: Iter 7300 Summary: 
[2025-03-27 11:50:34,498] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04747529577463865
[2025-03-27 11:51:02,685] INFO: Iter 7400 Summary: 
[2025-03-27 11:51:02,685] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047281279265880584
[2025-03-27 11:51:30,810] INFO: Iter 7500 Summary: 
[2025-03-27 11:51:30,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04714436192065477
[2025-03-27 11:51:58,931] INFO: Iter 7600 Summary: 
[2025-03-27 11:51:58,931] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04699894677847624
[2025-03-27 11:52:27,081] INFO: Iter 7700 Summary: 
[2025-03-27 11:52:27,081] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04676682282239199
[2025-03-27 11:52:55,340] INFO: Iter 7800 Summary: 
[2025-03-27 11:52:55,340] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04656113795936108
[2025-03-27 11:53:23,659] INFO: Iter 7900 Summary: 
[2025-03-27 11:53:23,659] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04667622290551662
[2025-03-27 11:53:51,836] INFO: Iter 8000 Summary: 
[2025-03-27 11:53:51,836] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04621357962489128
[2025-03-27 11:54:19,839] INFO: Iter 8100 Summary: 
[2025-03-27 11:54:19,839] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04603332590311766
[2025-03-27 11:54:47,886] INFO: Iter 8200 Summary: 
[2025-03-27 11:54:47,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04624975711107254
[2025-03-27 11:55:15,924] INFO: Iter 8300 Summary: 
[2025-03-27 11:55:15,924] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04620081443339586
[2025-03-27 11:55:44,014] INFO: Iter 8400 Summary: 
[2025-03-27 11:55:44,015] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04588279686868191
[2025-03-27 11:56:12,040] INFO: Iter 8500 Summary: 
[2025-03-27 11:56:12,040] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045783929899334906
[2025-03-27 11:56:40,099] INFO: Iter 8600 Summary: 
[2025-03-27 11:56:40,099] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04578123599290848
[2025-03-27 11:57:08,407] INFO: Iter 8700 Summary: 
[2025-03-27 11:57:08,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0458084799349308
[2025-03-27 11:57:36,497] INFO: Iter 8800 Summary: 
[2025-03-27 11:57:36,497] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550562381744385
[2025-03-27 11:58:04,618] INFO: Iter 8900 Summary: 
[2025-03-27 11:58:04,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539607536047697
[2025-03-27 11:58:32,747] INFO: Iter 9000 Summary: 
[2025-03-27 11:58:32,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045433243066072465
[2025-03-27 11:59:00,878] INFO: Iter 9100 Summary: 
[2025-03-27 11:59:00,878] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04509299598634243
[2025-03-27 11:59:29,223] INFO: Iter 9200 Summary: 
[2025-03-27 11:59:29,223] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045294758789241316
[2025-03-27 11:59:57,405] INFO: Iter 9300 Summary: 
[2025-03-27 11:59:57,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044912171848118305
[2025-03-27 12:00:25,632] INFO: Iter 9400 Summary: 
[2025-03-27 12:00:25,632] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04520781476050615
[2025-03-27 12:00:53,714] INFO: Iter 9500 Summary: 
[2025-03-27 12:00:53,715] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04517476037144661
[2025-03-27 12:01:21,927] INFO: Iter 9600 Summary: 
[2025-03-27 12:01:21,927] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04456259276717901
[2025-03-27 12:01:50,135] INFO: Iter 9700 Summary: 
[2025-03-27 12:01:50,135] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044481374770402905
[2025-03-27 12:02:18,332] INFO: Iter 9800 Summary: 
[2025-03-27 12:02:18,332] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423113480210304
[2025-03-27 12:02:46,481] INFO: Iter 9900 Summary: 
[2025-03-27 12:02:46,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459886740893126
[2025-03-27 12:03:14,653] INFO: Iter 10000 Summary: 
[2025-03-27 12:03:14,653] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044652320109307767
[2025-03-27 12:03:49,674] INFO: Iter 10100 Summary: 
[2025-03-27 12:03:49,674] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449293956160545
[2025-03-27 12:04:17,988] INFO: Iter 10200 Summary: 
[2025-03-27 12:04:17,988] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436818648129701
[2025-03-27 12:04:46,130] INFO: Iter 10300 Summary: 
[2025-03-27 12:04:46,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044031220078468325
[2025-03-27 12:05:14,286] INFO: Iter 10400 Summary: 
[2025-03-27 12:05:14,286] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444341254606843
[2025-03-27 12:05:42,432] INFO: Iter 10500 Summary: 
[2025-03-27 12:05:42,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434599511325359
[2025-03-27 12:06:10,539] INFO: Iter 10600 Summary: 
[2025-03-27 12:06:10,539] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044023889005184176
[2025-03-27 12:06:38,685] INFO: Iter 10700 Summary: 
[2025-03-27 12:06:38,686] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0438985838368535
[2025-03-27 12:07:06,748] INFO: Iter 10800 Summary: 
[2025-03-27 12:07:06,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044074077159166336
[2025-03-27 12:07:34,887] INFO: Iter 10900 Summary: 
[2025-03-27 12:07:34,887] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043824025094509125
[2025-03-27 12:08:03,041] INFO: Iter 11000 Summary: 
[2025-03-27 12:08:03,041] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04427321132272482
[2025-03-27 12:08:31,147] INFO: Iter 11100 Summary: 
[2025-03-27 12:08:31,147] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043777071200311185
[2025-03-27 12:08:59,278] INFO: Iter 11200 Summary: 
[2025-03-27 12:08:59,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043537504561245444
[2025-03-27 12:09:27,280] INFO: Iter 11300 Summary: 
[2025-03-27 12:09:27,280] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043300241380929944
[2025-03-27 12:09:55,482] INFO: Iter 11400 Summary: 
[2025-03-27 12:09:55,482] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043706214837729934
[2025-03-27 12:10:23,587] INFO: Iter 11500 Summary: 
[2025-03-27 12:10:23,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043423142954707145
[2025-03-27 12:10:51,722] INFO: Iter 11600 Summary: 
[2025-03-27 12:10:51,722] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371713496744633
[2025-03-27 12:11:20,261] INFO: Iter 11700 Summary: 
[2025-03-27 12:11:20,261] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366573859006166
[2025-03-27 12:11:48,444] INFO: Iter 11800 Summary: 
[2025-03-27 12:11:48,444] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327931553125382
[2025-03-27 12:12:16,742] INFO: Iter 11900 Summary: 
[2025-03-27 12:12:16,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043304727971553804
[2025-03-27 12:12:44,959] INFO: Iter 12000 Summary: 
[2025-03-27 12:12:44,959] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351072043180466
[2025-03-27 12:13:13,290] INFO: Iter 12100 Summary: 
[2025-03-27 12:13:13,290] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324725206941366
[2025-03-27 12:13:41,617] INFO: Iter 12200 Summary: 
[2025-03-27 12:13:41,617] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326152633875609
[2025-03-27 12:14:10,376] INFO: Iter 12300 Summary: 
[2025-03-27 12:14:10,377] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043234428763389586
[2025-03-27 12:14:38,567] INFO: Iter 12400 Summary: 
[2025-03-27 12:14:38,567] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042961729168891905
[2025-03-27 12:15:06,707] INFO: Iter 12500 Summary: 
[2025-03-27 12:15:06,707] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043149159252643586
[2025-03-27 12:15:34,785] INFO: Iter 12600 Summary: 
[2025-03-27 12:15:34,785] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346694178879261
[2025-03-27 12:16:02,901] INFO: Iter 12700 Summary: 
[2025-03-27 12:16:02,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04286337073892355
[2025-03-27 12:16:31,025] INFO: Iter 12800 Summary: 
[2025-03-27 12:16:31,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042945913523435596
[2025-03-27 12:16:59,149] INFO: Iter 12900 Summary: 
[2025-03-27 12:16:59,149] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296220101416111
[2025-03-27 12:17:27,172] INFO: Iter 13000 Summary: 
[2025-03-27 12:17:27,173] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316669885069132
[2025-03-27 12:17:55,418] INFO: Iter 13100 Summary: 
[2025-03-27 12:17:55,418] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04283607821911573
[2025-03-27 12:18:23,752] INFO: Iter 13200 Summary: 
[2025-03-27 12:18:23,753] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04284658413380384
[2025-03-27 12:18:51,799] INFO: Iter 13300 Summary: 
[2025-03-27 12:18:51,799] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04274474676698446
[2025-03-27 12:19:20,079] INFO: Iter 13400 Summary: 
[2025-03-27 12:19:20,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042780365869402884
[2025-03-27 12:19:48,209] INFO: Iter 13500 Summary: 
[2025-03-27 12:19:48,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042822097018361094
[2025-03-27 12:20:16,301] INFO: Iter 13600 Summary: 
[2025-03-27 12:20:16,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04263179611414671
[2025-03-27 12:20:44,511] INFO: Iter 13700 Summary: 
[2025-03-27 12:20:44,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0425363589450717
[2025-03-27 12:21:12,633] INFO: Iter 13800 Summary: 
[2025-03-27 12:21:12,633] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042644629068672654
[2025-03-27 12:21:40,701] INFO: Iter 13900 Summary: 
[2025-03-27 12:21:40,701] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042574195228517055
[2025-03-27 12:22:08,788] INFO: Iter 14000 Summary: 
[2025-03-27 12:22:08,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042417608462274076
[2025-03-27 12:22:37,089] INFO: Iter 14100 Summary: 
[2025-03-27 12:22:37,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04244217522442341
[2025-03-27 12:23:05,218] INFO: Iter 14200 Summary: 
[2025-03-27 12:23:05,218] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04276251219213009
[2025-03-27 12:23:33,458] INFO: Iter 14300 Summary: 
[2025-03-27 12:23:33,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04227613303810358
[2025-03-27 12:24:01,637] INFO: Iter 14400 Summary: 
[2025-03-27 12:24:01,637] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04257926631718874
[2025-03-27 12:24:29,792] INFO: Iter 14500 Summary: 
[2025-03-27 12:24:29,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0423652433976531
[2025-03-27 12:24:57,909] INFO: Iter 14600 Summary: 
[2025-03-27 12:24:57,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04202359989285469
[2025-03-27 12:25:26,130] INFO: Iter 14700 Summary: 
[2025-03-27 12:25:26,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04252763915807009
[2025-03-27 12:25:54,257] INFO: Iter 14800 Summary: 
[2025-03-27 12:25:54,257] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0424383956566453
[2025-03-27 12:26:22,369] INFO: Iter 14900 Summary: 
[2025-03-27 12:26:22,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04254950750619173
[2025-03-27 12:26:50,445] INFO: Iter 15000 Summary: 
[2025-03-27 12:26:50,445] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04259636342525482
[2025-03-27 12:27:25,323] INFO: Iter 15100 Summary: 
[2025-03-27 12:27:25,323] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04187404114753008
[2025-03-27 12:27:53,594] INFO: Iter 15200 Summary: 
[2025-03-27 12:27:53,594] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04229228679090738
[2025-03-27 12:28:21,711] INFO: Iter 15300 Summary: 
[2025-03-27 12:28:21,711] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04232952505350113
[2025-03-27 12:28:49,886] INFO: Iter 15400 Summary: 
[2025-03-27 12:28:49,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04256001405417919
[2025-03-27 12:29:18,159] INFO: Iter 15500 Summary: 
[2025-03-27 12:29:18,159] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042025483772158624
[2025-03-27 12:29:46,473] INFO: Iter 15600 Summary: 
[2025-03-27 12:29:46,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042312452010810374
[2025-03-27 12:30:14,598] INFO: Iter 15700 Summary: 
[2025-03-27 12:30:14,598] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04207139130681753
[2025-03-27 12:30:42,837] INFO: Iter 15800 Summary: 
[2025-03-27 12:30:42,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042142093367874625
[2025-03-27 12:31:11,091] INFO: Iter 15900 Summary: 
[2025-03-27 12:31:11,091] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04197003711014986
[2025-03-27 12:31:39,231] INFO: Iter 16000 Summary: 
[2025-03-27 12:31:39,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042036726325750354
[2025-03-27 12:32:07,428] INFO: Iter 16100 Summary: 
[2025-03-27 12:32:07,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0418594803661108
[2025-03-27 12:32:35,410] INFO: Iter 16200 Summary: 
[2025-03-27 12:32:35,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041890981309115886
[2025-03-27 12:33:03,643] INFO: Iter 16300 Summary: 
[2025-03-27 12:33:03,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041949711292982104
[2025-03-27 12:33:31,862] INFO: Iter 16400 Summary: 
[2025-03-27 12:33:31,862] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04225358419120312
[2025-03-27 12:34:00,082] INFO: Iter 16500 Summary: 
[2025-03-27 12:34:00,083] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04201393220573664
[2025-03-27 12:34:28,281] INFO: Iter 16600 Summary: 
[2025-03-27 12:34:28,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165809899568558
[2025-03-27 12:34:56,544] INFO: Iter 16700 Summary: 
[2025-03-27 12:34:56,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04182924184948206
[2025-03-27 12:35:24,664] INFO: Iter 16800 Summary: 
[2025-03-27 12:35:24,664] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041823956221342086
[2025-03-27 12:35:52,752] INFO: Iter 16900 Summary: 
[2025-03-27 12:35:52,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04194178730249405
[2025-03-27 12:36:20,809] INFO: Iter 17000 Summary: 
[2025-03-27 12:36:20,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04149287268519401
[2025-03-27 12:36:48,926] INFO: Iter 17100 Summary: 
[2025-03-27 12:36:48,926] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042000486440956594
[2025-03-27 12:37:17,132] INFO: Iter 17200 Summary: 
[2025-03-27 12:37:17,132] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04181637395173311
[2025-03-27 12:37:45,291] INFO: Iter 17300 Summary: 
[2025-03-27 12:37:45,291] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168375056236982
[2025-03-27 12:38:13,504] INFO: Iter 17400 Summary: 
[2025-03-27 12:38:13,505] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041925440393388275
[2025-03-27 12:38:41,623] INFO: Iter 17500 Summary: 
[2025-03-27 12:38:41,624] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041909749507904055
[2025-03-27 12:39:09,786] INFO: Iter 17600 Summary: 
[2025-03-27 12:39:09,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041702653244137765
[2025-03-27 12:39:37,809] INFO: Iter 17700 Summary: 
[2025-03-27 12:39:37,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04154967576265335
[2025-03-27 12:40:05,974] INFO: Iter 17800 Summary: 
[2025-03-27 12:40:05,975] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041657634638249876
[2025-03-27 12:40:34,124] INFO: Iter 17900 Summary: 
[2025-03-27 12:40:34,124] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041485720165073875
[2025-03-27 12:41:02,374] INFO: Iter 18000 Summary: 
[2025-03-27 12:41:02,374] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04167863667011261
[2025-03-27 12:41:30,660] INFO: Iter 18100 Summary: 
[2025-03-27 12:41:30,660] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153503213077783
[2025-03-27 12:41:58,963] INFO: Iter 18200 Summary: 
[2025-03-27 12:41:58,963] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04175286322832108
[2025-03-27 12:42:27,249] INFO: Iter 18300 Summary: 
[2025-03-27 12:42:27,249] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165306735783816
[2025-03-27 12:42:56,006] INFO: Iter 18400 Summary: 
[2025-03-27 12:42:56,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155572850257158
[2025-03-27 12:43:24,503] INFO: Iter 18500 Summary: 
[2025-03-27 12:43:24,503] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04146904997527599
[2025-03-27 12:43:52,790] INFO: Iter 18600 Summary: 
[2025-03-27 12:43:52,790] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04163960196077823
[2025-03-27 12:44:21,063] INFO: Iter 18700 Summary: 
[2025-03-27 12:44:21,064] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04148095481097698
[2025-03-27 12:44:49,469] INFO: Iter 18800 Summary: 
[2025-03-27 12:44:49,469] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04139668181538582
[2025-03-27 12:45:17,953] INFO: Iter 18900 Summary: 
[2025-03-27 12:45:17,953] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155068412423134
[2025-03-27 12:45:46,319] INFO: Iter 19000 Summary: 
[2025-03-27 12:45:46,319] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04157551810145378
[2025-03-27 12:46:14,630] INFO: Iter 19100 Summary: 
[2025-03-27 12:46:14,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041412553004920485
[2025-03-27 12:46:43,018] INFO: Iter 19200 Summary: 
[2025-03-27 12:46:43,018] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04140574678778648
[2025-03-27 12:47:11,503] INFO: Iter 19300 Summary: 
[2025-03-27 12:47:11,503] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145525500178337
[2025-03-27 12:47:39,799] INFO: Iter 19400 Summary: 
[2025-03-27 12:47:39,799] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041232666708528994
[2025-03-27 12:48:08,151] INFO: Iter 19500 Summary: 
[2025-03-27 12:48:08,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041279917657375334
[2025-03-27 12:48:36,540] INFO: Iter 19600 Summary: 
[2025-03-27 12:48:36,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153652708977461
[2025-03-27 12:49:04,865] INFO: Iter 19700 Summary: 
[2025-03-27 12:49:04,865] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041608084812760356
[2025-03-27 12:49:33,127] INFO: Iter 19800 Summary: 
[2025-03-27 12:49:33,127] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04137492183595896
[2025-03-27 12:50:01,545] INFO: Iter 19900 Summary: 
[2025-03-27 12:50:01,545] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041093552894890306
[2025-03-27 12:50:29,850] INFO: Iter 20000 Summary: 
[2025-03-27 12:50:29,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04138925150036812
[2025-03-27 12:51:04,980] INFO: Iter 20100 Summary: 
[2025-03-27 12:51:04,981] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041542060039937496
[2025-03-27 12:51:34,766] INFO: Iter 20200 Summary: 
[2025-03-27 12:51:34,767] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041299914196133616
[2025-03-27 12:52:04,060] INFO: Iter 20300 Summary: 
[2025-03-27 12:52:04,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041041623018682
[2025-03-27 12:52:33,339] INFO: Iter 20400 Summary: 
[2025-03-27 12:52:33,339] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168195985257626
[2025-03-27 12:53:01,904] INFO: Iter 20500 Summary: 
[2025-03-27 12:53:01,904] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04116824604570866
[2025-03-27 12:53:30,022] INFO: Iter 20600 Summary: 
[2025-03-27 12:53:30,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04130369149148464
[2025-03-27 12:53:58,396] INFO: Iter 20700 Summary: 
[2025-03-27 12:53:58,396] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04113837849348784
[2025-03-27 12:54:26,526] INFO: Iter 20800 Summary: 
[2025-03-27 12:54:26,526] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041218544393777846
[2025-03-27 12:54:54,714] INFO: Iter 20900 Summary: 
[2025-03-27 12:54:54,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068817496299744
[2025-03-27 12:55:23,017] INFO: Iter 21000 Summary: 
[2025-03-27 12:55:23,017] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110053025186062
[2025-03-27 12:55:51,276] INFO: Iter 21100 Summary: 
[2025-03-27 12:55:51,276] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041341228820383547
[2025-03-27 12:56:19,512] INFO: Iter 21200 Summary: 
[2025-03-27 12:56:19,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145940162241459
[2025-03-27 12:56:47,764] INFO: Iter 21300 Summary: 
[2025-03-27 12:56:47,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04136125065386295
[2025-03-27 12:57:15,933] INFO: Iter 21400 Summary: 
[2025-03-27 12:57:15,933] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040995805896818636
[2025-03-27 12:57:44,119] INFO: Iter 21500 Summary: 
[2025-03-27 12:57:44,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04089998587965965
[2025-03-27 12:58:12,295] INFO: Iter 21600 Summary: 
[2025-03-27 12:58:12,295] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0412916674092412
[2025-03-27 12:58:40,666] INFO: Iter 21700 Summary: 
[2025-03-27 12:58:40,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04096870582550764
[2025-03-27 12:59:08,913] INFO: Iter 21800 Summary: 
[2025-03-27 12:59:08,914] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118104003369808
[2025-03-27 12:59:37,125] INFO: Iter 21900 Summary: 
[2025-03-27 12:59:37,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04112858448177576
[2025-03-27 13:00:05,511] INFO: Iter 22000 Summary: 
[2025-03-27 13:00:05,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041169697567820546
[2025-03-27 13:00:33,820] INFO: Iter 22100 Summary: 
[2025-03-27 13:00:33,820] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405092404410243
[2025-03-27 13:01:03,125] INFO: Iter 22200 Summary: 
[2025-03-27 13:01:03,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0407518195733428
[2025-03-27 13:01:31,458] INFO: Iter 22300 Summary: 
[2025-03-27 13:01:31,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041050582490861416
[2025-03-27 13:01:59,675] INFO: Iter 22400 Summary: 
[2025-03-27 13:01:59,676] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408492099493742
[2025-03-27 13:02:27,979] INFO: Iter 22500 Summary: 
[2025-03-27 13:02:27,979] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0409961424395442
[2025-03-27 13:02:56,338] INFO: Iter 22600 Summary: 
[2025-03-27 13:02:56,338] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04135877802968025
[2025-03-27 13:03:24,661] INFO: Iter 22700 Summary: 
[2025-03-27 13:03:24,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04158662248402834
[2025-03-27 13:03:52,995] INFO: Iter 22800 Summary: 
[2025-03-27 13:03:52,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04100227441638708
[2025-03-27 13:04:21,322] INFO: Iter 22900 Summary: 
[2025-03-27 13:04:21,322] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04090706624090672
[2025-03-27 13:04:49,519] INFO: Iter 23000 Summary: 
[2025-03-27 13:04:49,519] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093770138919353
[2025-03-27 13:05:17,805] INFO: Iter 23100 Summary: 
[2025-03-27 13:05:17,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408388651162386
[2025-03-27 13:05:46,189] INFO: Iter 23200 Summary: 
[2025-03-27 13:05:46,190] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080089289695024
[2025-03-27 13:06:14,470] INFO: Iter 23300 Summary: 
[2025-03-27 13:06:14,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110186990350485
[2025-03-27 13:06:42,865] INFO: Iter 23400 Summary: 
[2025-03-27 13:06:42,865] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118562676012516
[2025-03-27 13:07:11,107] INFO: Iter 23500 Summary: 
[2025-03-27 13:07:11,107] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067198559641838
[2025-03-27 13:07:39,313] INFO: Iter 23600 Summary: 
[2025-03-27 13:07:39,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04073971249163151
[2025-03-27 13:08:07,484] INFO: Iter 23700 Summary: 
[2025-03-27 13:08:07,484] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118783336132765
[2025-03-27 13:08:35,576] INFO: Iter 23800 Summary: 
[2025-03-27 13:08:35,576] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04081785511225462
[2025-03-27 13:09:03,750] INFO: Iter 23900 Summary: 
[2025-03-27 13:09:03,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04091232247650623
[2025-03-27 13:09:31,886] INFO: Iter 24000 Summary: 
[2025-03-27 13:09:31,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040950687900185585
[2025-03-27 13:10:00,029] INFO: Iter 24100 Summary: 
[2025-03-27 13:10:00,030] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093142617493868
[2025-03-27 13:10:28,209] INFO: Iter 24200 Summary: 
[2025-03-27 13:10:28,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04088189199566841
[2025-03-27 13:10:56,372] INFO: Iter 24300 Summary: 
[2025-03-27 13:10:56,372] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060963463038206
[2025-03-27 13:11:24,508] INFO: Iter 24400 Summary: 
[2025-03-27 13:11:24,508] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053705725818872
[2025-03-27 13:11:52,721] INFO: Iter 24500 Summary: 
[2025-03-27 13:11:52,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040698427967727185
[2025-03-27 13:12:20,821] INFO: Iter 24600 Summary: 
[2025-03-27 13:12:20,822] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095322467386722
[2025-03-27 13:12:48,956] INFO: Iter 24700 Summary: 
[2025-03-27 13:12:48,957] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080451674759388
[2025-03-27 13:13:17,162] INFO: Iter 24800 Summary: 
[2025-03-27 13:13:17,162] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408536421507597
[2025-03-27 13:13:45,293] INFO: Iter 24900 Summary: 
[2025-03-27 13:13:45,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040972840487957
[2025-03-27 13:14:13,420] INFO: Iter 25000 Summary: 
[2025-03-27 13:14:13,421] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084993671625853
[2025-03-27 13:14:48,474] INFO: Iter 25100 Summary: 
[2025-03-27 13:14:48,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061872538179159
[2025-03-27 13:15:16,640] INFO: Iter 25200 Summary: 
[2025-03-27 13:15:16,640] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04043906804174185
[2025-03-27 13:15:44,882] INFO: Iter 25300 Summary: 
[2025-03-27 13:15:44,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040832564756274226
[2025-03-27 13:16:13,016] INFO: Iter 25400 Summary: 
[2025-03-27 13:16:13,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074161302298308
[2025-03-27 13:16:41,195] INFO: Iter 25500 Summary: 
[2025-03-27 13:16:41,195] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041060957312583926
[2025-03-27 13:17:09,482] INFO: Iter 25600 Summary: 
[2025-03-27 13:17:09,482] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404279800504446
[2025-03-27 13:17:37,637] INFO: Iter 25700 Summary: 
[2025-03-27 13:17:37,637] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068828206509352
[2025-03-27 13:18:05,789] INFO: Iter 25800 Summary: 
[2025-03-27 13:18:05,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069079078733921
[2025-03-27 13:18:34,080] INFO: Iter 25900 Summary: 
[2025-03-27 13:18:34,080] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04078167922794819
[2025-03-27 13:19:02,315] INFO: Iter 26000 Summary: 
[2025-03-27 13:19:02,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040673726387321946
[2025-03-27 13:19:30,515] INFO: Iter 26100 Summary: 
[2025-03-27 13:19:30,515] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095667663961649
[2025-03-27 13:19:58,714] INFO: Iter 26200 Summary: 
[2025-03-27 13:19:58,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048498272895813
[2025-03-27 13:20:27,002] INFO: Iter 26300 Summary: 
[2025-03-27 13:20:27,002] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068731527775526
[2025-03-27 13:20:55,053] INFO: Iter 26400 Summary: 
[2025-03-27 13:20:55,053] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408188908547163
[2025-03-27 13:21:23,121] INFO: Iter 26500 Summary: 
[2025-03-27 13:21:23,121] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04085890453308821
[2025-03-27 13:21:51,269] INFO: Iter 26600 Summary: 
[2025-03-27 13:21:51,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040534629449248315
[2025-03-27 13:22:19,387] INFO: Iter 26700 Summary: 
[2025-03-27 13:22:19,387] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084464132785797
[2025-03-27 13:22:47,540] INFO: Iter 26800 Summary: 
[2025-03-27 13:22:47,540] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053069945424795
[2025-03-27 13:23:15,880] INFO: Iter 26900 Summary: 
[2025-03-27 13:23:15,880] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040677514709532264
[2025-03-27 13:23:44,032] INFO: Iter 27000 Summary: 
[2025-03-27 13:23:44,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069628566503525
[2025-03-27 13:24:12,305] INFO: Iter 27100 Summary: 
[2025-03-27 13:24:12,305] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040576146207749846
[2025-03-27 13:24:40,508] INFO: Iter 27200 Summary: 
[2025-03-27 13:24:40,508] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061826728284359
[2025-03-27 13:25:08,598] INFO: Iter 27300 Summary: 
[2025-03-27 13:25:08,598] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040502460934221746
[2025-03-27 13:25:36,801] INFO: Iter 27400 Summary: 
[2025-03-27 13:25:36,802] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040391418300569054
[2025-03-27 13:26:04,912] INFO: Iter 27500 Summary: 
[2025-03-27 13:26:04,913] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060128532350063
[2025-03-27 13:26:33,139] INFO: Iter 27600 Summary: 
[2025-03-27 13:26:33,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04030371893197298
[2025-03-27 13:27:01,291] INFO: Iter 27700 Summary: 
[2025-03-27 13:27:01,291] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04055089380592108
[2025-03-27 13:27:29,443] INFO: Iter 27800 Summary: 
[2025-03-27 13:27:29,444] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0406524433195591
[2025-03-27 13:27:57,628] INFO: Iter 27900 Summary: 
[2025-03-27 13:27:57,628] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058287750929594
[2025-03-27 13:28:25,945] INFO: Iter 28000 Summary: 
[2025-03-27 13:28:25,945] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067306347191334
[2025-03-27 13:28:54,127] INFO: Iter 28100 Summary: 
[2025-03-27 13:28:54,127] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04037954095751047
[2025-03-27 13:29:22,355] INFO: Iter 28200 Summary: 
[2025-03-27 13:29:22,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04038192193955183
[2025-03-27 13:29:50,503] INFO: Iter 28300 Summary: 
[2025-03-27 13:29:50,503] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040238456577062605
[2025-03-27 13:30:18,675] INFO: Iter 28400 Summary: 
[2025-03-27 13:30:18,675] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04046010196208954
[2025-03-27 13:30:46,980] INFO: Iter 28500 Summary: 
[2025-03-27 13:30:46,980] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04056391391903162
[2025-03-27 13:31:15,233] INFO: Iter 28600 Summary: 
[2025-03-27 13:31:15,233] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040525965727865695
[2025-03-27 13:31:43,401] INFO: Iter 28700 Summary: 
[2025-03-27 13:31:43,401] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074303459376097
[2025-03-27 13:32:11,470] INFO: Iter 28800 Summary: 
[2025-03-27 13:32:11,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405935624986887
[2025-03-27 13:32:39,797] INFO: Iter 28900 Summary: 
[2025-03-27 13:32:39,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084486674517393
[2025-03-27 13:33:08,146] INFO: Iter 29000 Summary: 
[2025-03-27 13:33:08,146] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040621427595615385
[2025-03-27 13:33:43,919] INFO: Iter 29100 Summary: 
[2025-03-27 13:33:43,920] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04022378820925951
[2025-03-27 13:34:12,360] INFO: Iter 29200 Summary: 
[2025-03-27 13:34:12,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040208502225577834
[2025-03-27 13:34:40,669] INFO: Iter 29300 Summary: 
[2025-03-27 13:34:40,669] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040462910942733285
[2025-03-27 13:35:08,770] INFO: Iter 29400 Summary: 
[2025-03-27 13:35:08,770] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04041025843471289
[2025-03-27 13:35:36,855] INFO: Iter 29500 Summary: 
[2025-03-27 13:35:36,855] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404819118976593
[2025-03-27 13:36:05,016] INFO: Iter 29600 Summary: 
[2025-03-27 13:36:05,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04064005315303802
[2025-03-27 13:36:33,283] INFO: Iter 29700 Summary: 
[2025-03-27 13:36:33,283] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048785202205181
[2025-03-27 13:37:01,552] INFO: Iter 29800 Summary: 
[2025-03-27 13:37:01,552] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04010299898684025
[2025-03-27 13:37:29,823] INFO: Iter 29900 Summary: 
[2025-03-27 13:37:29,823] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04034921482205391
[2025-03-27 13:37:57,966] INFO: Iter 30000 Summary: 
[2025-03-27 13:37:57,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058229215443134
[2025-03-27 13:38:32,958] INFO: Iter 30100 Summary: 
[2025-03-27 13:38:32,958] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.03973235040903091
[2025-03-27 13:39:01,227] INFO: Iter 30200 Summary: 
[2025-03-27 13:39:01,227] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039118692390620706
[2025-03-27 13:39:29,384] INFO: Iter 30300 Summary: 
[2025-03-27 13:39:29,384] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03928719613701105
[2025-03-27 13:39:57,563] INFO: Iter 30400 Summary: 
[2025-03-27 13:39:57,563] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038738011457026
[2025-03-27 13:40:25,631] INFO: Iter 30500 Summary: 
[2025-03-27 13:40:25,631] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03889032088220119
[2025-03-27 13:40:53,757] INFO: Iter 30600 Summary: 
[2025-03-27 13:40:53,757] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03861566469073296
[2025-03-27 13:41:21,992] INFO: Iter 30700 Summary: 
[2025-03-27 13:41:21,993] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03872161664068699
[2025-03-27 13:41:50,173] INFO: Iter 30800 Summary: 
[2025-03-27 13:41:50,173] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038844494484364986
[2025-03-27 13:42:18,452] INFO: Iter 30900 Summary: 
[2025-03-27 13:42:18,452] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038778597079217436
[2025-03-27 13:42:46,582] INFO: Iter 31000 Summary: 
[2025-03-27 13:42:46,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038575371727347374
[2025-03-27 13:43:21,709] INFO: Iter 31100 Summary: 
[2025-03-27 13:43:21,710] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038529667183756826
[2025-03-27 13:43:49,839] INFO: Iter 31200 Summary: 
[2025-03-27 13:43:49,839] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038481415137648585
[2025-03-27 13:44:18,020] INFO: Iter 31300 Summary: 
[2025-03-27 13:44:18,020] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03858761254698038
[2025-03-27 13:44:46,218] INFO: Iter 31400 Summary: 
[2025-03-27 13:44:46,219] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038527636267244814
[2025-03-27 13:45:14,686] INFO: Iter 31500 Summary: 
[2025-03-27 13:45:14,686] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859547201544047
[2025-03-27 13:45:42,944] INFO: Iter 31600 Summary: 
[2025-03-27 13:45:42,944] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03838738303631544
[2025-03-27 13:46:11,158] INFO: Iter 31700 Summary: 
[2025-03-27 13:46:11,158] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855763614177704
[2025-03-27 13:46:39,375] INFO: Iter 31800 Summary: 
[2025-03-27 13:46:39,376] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843981295824051
[2025-03-27 13:47:07,569] INFO: Iter 31900 Summary: 
[2025-03-27 13:47:07,569] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038455543145537376
[2025-03-27 13:47:35,743] INFO: Iter 32000 Summary: 
[2025-03-27 13:47:35,743] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03880749378353357
[2025-03-27 13:48:10,906] INFO: Iter 32100 Summary: 
[2025-03-27 13:48:10,906] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851503048092127
[2025-03-27 13:48:39,262] INFO: Iter 32200 Summary: 
[2025-03-27 13:48:39,262] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03850121602416039
[2025-03-27 13:49:07,465] INFO: Iter 32300 Summary: 
[2025-03-27 13:49:07,465] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816764749586582
[2025-03-27 13:49:35,767] INFO: Iter 32400 Summary: 
[2025-03-27 13:49:35,767] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03848932571709156
[2025-03-27 13:50:03,954] INFO: Iter 32500 Summary: 
[2025-03-27 13:50:03,954] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859693259000778
[2025-03-27 13:50:32,214] INFO: Iter 32600 Summary: 
[2025-03-27 13:50:32,214] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038555137366056445
[2025-03-27 13:51:00,356] INFO: Iter 32700 Summary: 
[2025-03-27 13:51:00,356] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837333284318447
[2025-03-27 13:51:28,417] INFO: Iter 32800 Summary: 
[2025-03-27 13:51:28,417] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03845605220645666
[2025-03-27 13:51:56,618] INFO: Iter 32900 Summary: 
[2025-03-27 13:51:56,618] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03870507031679153
[2025-03-27 13:52:24,947] INFO: Iter 33000 Summary: 
[2025-03-27 13:52:24,947] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817356590181589
[2025-03-27 13:53:00,021] INFO: Iter 33100 Summary: 
[2025-03-27 13:53:00,021] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383727602660656
[2025-03-27 13:53:28,231] INFO: Iter 33200 Summary: 
[2025-03-27 13:53:28,231] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03840629003942013
[2025-03-27 13:53:56,489] INFO: Iter 33300 Summary: 
[2025-03-27 13:53:56,489] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837062247097492
[2025-03-27 13:54:24,712] INFO: Iter 33400 Summary: 
[2025-03-27 13:54:24,712] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03860242497175932
[2025-03-27 13:54:52,920] INFO: Iter 33500 Summary: 
[2025-03-27 13:54:52,920] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03846450563520193
[2025-03-27 13:55:20,961] INFO: Iter 33600 Summary: 
[2025-03-27 13:55:20,961] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804883323609829
[2025-03-27 13:55:49,036] INFO: Iter 33700 Summary: 
[2025-03-27 13:55:49,036] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038162836283445356
[2025-03-27 13:56:17,163] INFO: Iter 33800 Summary: 
[2025-03-27 13:56:17,163] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03809049587696791
[2025-03-27 13:56:45,263] INFO: Iter 33900 Summary: 
[2025-03-27 13:56:45,263] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833961393684149
[2025-03-27 13:57:13,499] INFO: Iter 34000 Summary: 
[2025-03-27 13:57:13,499] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03821747232228517
[2025-03-27 13:57:48,338] INFO: Iter 34100 Summary: 
[2025-03-27 13:57:48,338] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843688197433948
[2025-03-27 13:58:16,507] INFO: Iter 34200 Summary: 
[2025-03-27 13:58:16,507] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03820082072168589
[2025-03-27 13:58:44,614] INFO: Iter 34300 Summary: 
[2025-03-27 13:58:44,616] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038492189683020116
[2025-03-27 13:59:12,769] INFO: Iter 34400 Summary: 
[2025-03-27 13:59:12,769] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851904425770045
[2025-03-27 13:59:40,952] INFO: Iter 34500 Summary: 
[2025-03-27 13:59:40,952] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038166586831212046
[2025-03-27 14:00:09,218] INFO: Iter 34600 Summary: 
[2025-03-27 14:00:09,218] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836640920490027
[2025-03-27 14:00:37,486] INFO: Iter 34700 Summary: 
[2025-03-27 14:00:37,486] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814776938408613
[2025-03-27 14:01:05,661] INFO: Iter 34800 Summary: 
[2025-03-27 14:01:05,661] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03841575048863888
[2025-03-27 14:01:33,919] INFO: Iter 34900 Summary: 
[2025-03-27 14:01:33,919] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837616860866547
[2025-03-27 14:02:02,338] INFO: Iter 35000 Summary: 
[2025-03-27 14:02:02,338] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825340036302805
[2025-03-27 14:02:37,528] INFO: Iter 35100 Summary: 
[2025-03-27 14:02:37,529] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836090188473463
[2025-03-27 14:03:05,813] INFO: Iter 35200 Summary: 
[2025-03-27 14:03:05,813] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038214359544217584
[2025-03-27 14:03:33,991] INFO: Iter 35300 Summary: 
[2025-03-27 14:03:33,991] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855342034250498
[2025-03-27 14:04:02,275] INFO: Iter 35400 Summary: 
[2025-03-27 14:04:02,275] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03805466830730438
[2025-03-27 14:04:30,419] INFO: Iter 35500 Summary: 
[2025-03-27 14:04:30,419] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822425208985805
[2025-03-27 14:04:58,695] INFO: Iter 35600 Summary: 
[2025-03-27 14:04:58,695] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038065870590507984
[2025-03-27 14:05:26,934] INFO: Iter 35700 Summary: 
[2025-03-27 14:05:26,934] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814086750149727
[2025-03-27 14:05:55,158] INFO: Iter 35800 Summary: 
[2025-03-27 14:05:55,158] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817138198763132
[2025-03-27 14:06:23,433] INFO: Iter 35900 Summary: 
[2025-03-27 14:06:23,433] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818887110799551
[2025-03-27 14:06:51,612] INFO: Iter 36000 Summary: 
[2025-03-27 14:06:51,613] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825825229287148
[2025-03-27 14:07:26,932] INFO: Iter 36100 Summary: 
[2025-03-27 14:07:26,932] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843956135213375
[2025-03-27 14:09:53,103] INFO: {
    "abs_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 85000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\data/h36m/",
    "link_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_last.log",
    "link_val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_last.log",
    "log_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log",
    "log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/log_2025_03_27_14_09_34.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 0,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "GCNext",
    "root_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log\\snapshot",
    "this_dir": "run",
    "use_relative_loss": true,
    "val_log_file": "C:\\Users\\hutania\\Documents\\Thesis\\GCNext\\exps\\run\\log/val_2025_03_27_14_09_34.log",
    "weight_decay": 0.0001
}
[2025-03-27 14:10:21,295] INFO: Iter 100 Summary: 
[2025-03-27 14:10:21,295] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.15287387140095235
[2025-03-27 14:10:49,033] INFO: Iter 200 Summary: 
[2025-03-27 14:10:49,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10449402175843715
[2025-03-27 14:11:16,824] INFO: Iter 300 Summary: 
[2025-03-27 14:11:16,824] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09972120471298694
[2025-03-27 14:11:44,509] INFO: Iter 400 Summary: 
[2025-03-27 14:11:44,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09364615313708782
[2025-03-27 14:12:12,381] INFO: Iter 500 Summary: 
[2025-03-27 14:12:12,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09216870330274105
[2025-03-27 14:12:40,630] INFO: Iter 600 Summary: 
[2025-03-27 14:12:40,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0913225582242012
[2025-03-27 14:13:08,791] INFO: Iter 700 Summary: 
[2025-03-27 14:13:08,791] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0892957766354084
[2025-03-27 14:13:36,948] INFO: Iter 800 Summary: 
[2025-03-27 14:13:36,948] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08028836622834205
[2025-03-27 14:14:05,048] INFO: Iter 900 Summary: 
[2025-03-27 14:14:05,048] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07361114241182803
[2025-03-27 14:14:33,148] INFO: Iter 1000 Summary: 
[2025-03-27 14:14:33,148] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0712160461395979
[2025-03-27 14:15:01,271] INFO: Iter 1100 Summary: 
[2025-03-27 14:15:01,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0662378877773881
[2025-03-27 14:15:29,666] INFO: Iter 1200 Summary: 
[2025-03-27 14:15:29,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06260723497718573
[2025-03-27 14:15:57,803] INFO: Iter 1300 Summary: 
[2025-03-27 14:15:57,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06077460564672947
[2025-03-27 14:16:25,892] INFO: Iter 1400 Summary: 
[2025-03-27 14:16:25,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059101610518991946
[2025-03-27 14:16:54,192] INFO: Iter 1500 Summary: 
[2025-03-27 14:16:54,192] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05815540604293346
[2025-03-27 14:17:22,285] INFO: Iter 1600 Summary: 
[2025-03-27 14:17:22,285] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768500231206417
[2025-03-27 14:17:50,422] INFO: Iter 1700 Summary: 
[2025-03-27 14:17:50,422] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05612747546285391
[2025-03-27 14:18:18,650] INFO: Iter 1800 Summary: 
[2025-03-27 14:18:18,650] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0559469723328948
[2025-03-27 14:18:46,784] INFO: Iter 1900 Summary: 
[2025-03-27 14:18:46,785] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055011885352432725
[2025-03-27 14:19:15,069] INFO: Iter 2000 Summary: 
[2025-03-27 14:19:15,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054927452243864534
[2025-03-27 14:19:43,437] INFO: Iter 2100 Summary: 
[2025-03-27 14:19:43,438] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05467277605086565
[2025-03-27 14:20:11,666] INFO: Iter 2200 Summary: 
[2025-03-27 14:20:11,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05439220994710922
[2025-03-27 14:20:39,868] INFO: Iter 2300 Summary: 
[2025-03-27 14:20:39,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05411341149359941
[2025-03-27 14:21:08,161] INFO: Iter 2400 Summary: 
[2025-03-27 14:21:08,161] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05394661214202642
[2025-03-27 14:21:36,428] INFO: Iter 2500 Summary: 
[2025-03-27 14:21:36,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05381324980407953
[2025-03-27 14:22:04,552] INFO: Iter 2600 Summary: 
[2025-03-27 14:22:04,552] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053067084886133674
[2025-03-27 14:22:32,733] INFO: Iter 2700 Summary: 
[2025-03-27 14:22:32,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05291106801480055
[2025-03-27 14:23:00,929] INFO: Iter 2800 Summary: 
[2025-03-27 14:23:00,929] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268501613289118
[2025-03-27 14:23:29,184] INFO: Iter 2900 Summary: 
[2025-03-27 14:23:29,184] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05244477484375239
[2025-03-27 14:23:57,399] INFO: Iter 3000 Summary: 
[2025-03-27 14:23:57,399] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052546527571976184
[2025-03-27 14:24:25,690] INFO: Iter 3100 Summary: 
[2025-03-27 14:24:25,690] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051631263718008996
[2025-03-27 14:24:53,886] INFO: Iter 3200 Summary: 
[2025-03-27 14:24:53,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05216690514236689
[2025-03-27 14:25:22,178] INFO: Iter 3300 Summary: 
[2025-03-27 14:25:22,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05180593386292458
[2025-03-27 14:25:50,350] INFO: Iter 3400 Summary: 
[2025-03-27 14:25:50,350] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128339599817991
[2025-03-27 14:26:18,508] INFO: Iter 3500 Summary: 
[2025-03-27 14:26:18,508] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05161994945257902
[2025-03-27 14:26:46,791] INFO: Iter 3600 Summary: 
[2025-03-27 14:26:46,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051079065576195716
[2025-03-27 14:27:14,865] INFO: Iter 3700 Summary: 
[2025-03-27 14:27:14,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05093852359801531
[2025-03-27 14:27:43,119] INFO: Iter 3800 Summary: 
[2025-03-27 14:27:43,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050728333853185176
[2025-03-27 14:28:11,413] INFO: Iter 3900 Summary: 
[2025-03-27 14:28:11,413] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051007907390594485
[2025-03-27 14:28:39,660] INFO: Iter 4000 Summary: 
[2025-03-27 14:28:39,660] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05128511141985655
[2025-03-27 14:29:08,006] INFO: Iter 4100 Summary: 
[2025-03-27 14:29:08,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050616980902850627
[2025-03-27 14:29:36,425] INFO: Iter 4200 Summary: 
[2025-03-27 14:29:36,425] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05099097285419703
[2025-03-27 14:30:04,619] INFO: Iter 4300 Summary: 
[2025-03-27 14:30:04,619] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05006971627473831
[2025-03-27 14:30:32,834] INFO: Iter 4400 Summary: 
[2025-03-27 14:30:32,834] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05037182774394751
[2025-03-27 14:31:00,937] INFO: Iter 4500 Summary: 
[2025-03-27 14:31:00,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050234943144023415
[2025-03-27 14:31:29,294] INFO: Iter 4600 Summary: 
[2025-03-27 14:31:29,294] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05055206254124642
[2025-03-27 14:31:57,408] INFO: Iter 4700 Summary: 
[2025-03-27 14:31:57,408] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04994991168379784
[2025-03-27 14:32:25,558] INFO: Iter 4800 Summary: 
[2025-03-27 14:32:25,558] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05021801430732012
[2025-03-27 14:32:53,745] INFO: Iter 4900 Summary: 
[2025-03-27 14:32:53,746] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049785437248647216
[2025-03-27 14:33:22,000] INFO: Iter 5000 Summary: 
[2025-03-27 14:33:22,001] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04928127493709326
[2025-03-27 14:33:57,031] INFO: Iter 5100 Summary: 
[2025-03-27 14:33:57,031] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04932282216846943
[2025-03-27 14:34:25,084] INFO: Iter 5200 Summary: 
[2025-03-27 14:34:25,084] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0498723316937685
[2025-03-27 14:34:53,348] INFO: Iter 5300 Summary: 
[2025-03-27 14:34:53,348] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049440183341503144
[2025-03-27 14:35:21,539] INFO: Iter 5400 Summary: 
[2025-03-27 14:35:21,539] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049165061116218566
[2025-03-27 14:35:49,594] INFO: Iter 5500 Summary: 
[2025-03-27 14:35:49,595] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049324527978897095
[2025-03-27 14:36:17,706] INFO: Iter 5600 Summary: 
[2025-03-27 14:36:17,707] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487074363976717
[2025-03-27 14:36:45,865] INFO: Iter 5700 Summary: 
[2025-03-27 14:36:45,865] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04891889002174139
[2025-03-27 14:37:14,027] INFO: Iter 5800 Summary: 
[2025-03-27 14:37:14,027] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0487964428216219
[2025-03-27 14:37:42,158] INFO: Iter 5900 Summary: 
[2025-03-27 14:37:42,159] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048565466962754725
[2025-03-27 14:38:10,343] INFO: Iter 6000 Summary: 
[2025-03-27 14:38:10,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04910611681640148
[2025-03-27 14:38:38,448] INFO: Iter 6100 Summary: 
[2025-03-27 14:38:38,448] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04909805666655302
[2025-03-27 14:39:06,847] INFO: Iter 6200 Summary: 
[2025-03-27 14:39:06,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048542876280844215
[2025-03-27 14:39:34,872] INFO: Iter 6300 Summary: 
[2025-03-27 14:39:34,872] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048175370916724206
[2025-03-27 14:40:03,116] INFO: Iter 6400 Summary: 
[2025-03-27 14:40:03,116] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048257682099938394
[2025-03-27 14:40:31,269] INFO: Iter 6500 Summary: 
[2025-03-27 14:40:31,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04780059069395065
[2025-03-27 14:40:59,422] INFO: Iter 6600 Summary: 
[2025-03-27 14:40:59,422] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04812747694551945
[2025-03-27 14:41:27,622] INFO: Iter 6700 Summary: 
[2025-03-27 14:41:27,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04789849977940321
[2025-03-27 14:41:55,831] INFO: Iter 6800 Summary: 
[2025-03-27 14:41:55,831] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04803676519542933
[2025-03-27 14:42:23,946] INFO: Iter 6900 Summary: 
[2025-03-27 14:42:23,946] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047816569320857524
[2025-03-27 14:42:52,236] INFO: Iter 7000 Summary: 
[2025-03-27 14:42:52,237] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04756761498749256
[2025-03-27 14:43:21,119] INFO: Iter 7100 Summary: 
[2025-03-27 14:43:21,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047559353597462176
[2025-03-27 14:43:49,275] INFO: Iter 7200 Summary: 
[2025-03-27 14:43:49,275] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047472536452114585
[2025-03-27 14:44:17,455] INFO: Iter 7300 Summary: 
[2025-03-27 14:44:17,455] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04747529577463865
[2025-03-27 14:44:45,653] INFO: Iter 7400 Summary: 
[2025-03-27 14:44:45,653] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047281279265880584
[2025-03-27 14:45:13,883] INFO: Iter 7500 Summary: 
[2025-03-27 14:45:13,883] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04714436192065477
[2025-03-27 14:45:42,136] INFO: Iter 7600 Summary: 
[2025-03-27 14:45:42,136] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04699894677847624
[2025-03-27 14:46:10,359] INFO: Iter 7700 Summary: 
[2025-03-27 14:46:10,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04676682282239199
[2025-03-27 14:46:38,429] INFO: Iter 7800 Summary: 
[2025-03-27 14:46:38,430] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04656113795936108
[2025-03-27 14:47:06,729] INFO: Iter 7900 Summary: 
[2025-03-27 14:47:06,729] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04667622290551662
[2025-03-27 14:47:34,862] INFO: Iter 8000 Summary: 
[2025-03-27 14:47:34,862] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04621357962489128
[2025-03-27 14:48:03,057] INFO: Iter 8100 Summary: 
[2025-03-27 14:48:03,057] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04603332590311766
[2025-03-27 14:48:31,131] INFO: Iter 8200 Summary: 
[2025-03-27 14:48:31,131] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04624975711107254
[2025-03-27 14:48:59,337] INFO: Iter 8300 Summary: 
[2025-03-27 14:48:59,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04620081443339586
[2025-03-27 14:49:27,487] INFO: Iter 8400 Summary: 
[2025-03-27 14:49:27,487] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04588279686868191
[2025-03-27 14:49:55,600] INFO: Iter 8500 Summary: 
[2025-03-27 14:49:55,600] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045783929899334906
[2025-03-27 14:50:23,866] INFO: Iter 8600 Summary: 
[2025-03-27 14:50:23,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04578123599290848
[2025-03-27 14:50:52,088] INFO: Iter 8700 Summary: 
[2025-03-27 14:50:52,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0458084799349308
[2025-03-27 14:51:20,271] INFO: Iter 8800 Summary: 
[2025-03-27 14:51:20,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550562381744385
[2025-03-27 14:51:48,215] INFO: Iter 8900 Summary: 
[2025-03-27 14:51:48,215] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539607536047697
[2025-03-27 14:52:16,268] INFO: Iter 9000 Summary: 
[2025-03-27 14:52:16,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045433243066072465
[2025-03-27 14:52:44,635] INFO: Iter 9100 Summary: 
[2025-03-27 14:52:44,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04509299598634243
[2025-03-27 14:53:12,622] INFO: Iter 9200 Summary: 
[2025-03-27 14:53:12,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045294758789241316
[2025-03-27 14:53:40,701] INFO: Iter 9300 Summary: 
[2025-03-27 14:53:40,701] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044912171848118305
[2025-03-27 14:54:08,853] INFO: Iter 9400 Summary: 
[2025-03-27 14:54:08,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04520781476050615
[2025-03-27 14:54:36,887] INFO: Iter 9500 Summary: 
[2025-03-27 14:54:36,887] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04517476037144661
[2025-03-27 14:55:04,841] INFO: Iter 9600 Summary: 
[2025-03-27 14:55:04,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04456259276717901
[2025-03-27 14:55:33,048] INFO: Iter 9700 Summary: 
[2025-03-27 14:55:33,048] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044481374770402905
[2025-03-27 14:56:01,178] INFO: Iter 9800 Summary: 
[2025-03-27 14:56:01,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423113480210304
[2025-03-27 14:56:29,373] INFO: Iter 9900 Summary: 
[2025-03-27 14:56:29,373] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459886740893126
[2025-03-27 14:56:57,529] INFO: Iter 10000 Summary: 
[2025-03-27 14:56:57,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044652320109307767
[2025-03-27 14:57:32,628] INFO: Iter 10100 Summary: 
[2025-03-27 14:57:32,628] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449293956160545
[2025-03-27 14:58:00,766] INFO: Iter 10200 Summary: 
[2025-03-27 14:58:00,766] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436818648129701
[2025-03-27 14:58:29,142] INFO: Iter 10300 Summary: 
[2025-03-27 14:58:29,142] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044031220078468325
[2025-03-27 14:58:57,371] INFO: Iter 10400 Summary: 
[2025-03-27 14:58:57,371] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444341254606843
[2025-03-27 14:59:25,380] INFO: Iter 10500 Summary: 
[2025-03-27 14:59:25,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434599511325359
[2025-03-27 14:59:53,344] INFO: Iter 10600 Summary: 
[2025-03-27 14:59:53,344] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044023889005184176
[2025-03-27 15:00:21,445] INFO: Iter 10700 Summary: 
[2025-03-27 15:00:21,445] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0438985838368535
[2025-03-27 15:00:49,596] INFO: Iter 10800 Summary: 
[2025-03-27 15:00:49,596] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044074077159166336
[2025-03-27 15:01:17,756] INFO: Iter 10900 Summary: 
[2025-03-27 15:01:17,756] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043824025094509125
[2025-03-27 15:01:45,882] INFO: Iter 11000 Summary: 
[2025-03-27 15:01:45,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04427321132272482
[2025-03-27 15:02:13,971] INFO: Iter 11100 Summary: 
[2025-03-27 15:02:13,972] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043777071200311185
[2025-03-27 15:02:42,119] INFO: Iter 11200 Summary: 
[2025-03-27 15:02:42,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043537504561245444
[2025-03-27 15:03:10,341] INFO: Iter 11300 Summary: 
[2025-03-27 15:03:10,341] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043300241380929944
[2025-03-27 15:03:38,720] INFO: Iter 11400 Summary: 
[2025-03-27 15:03:38,720] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043706214837729934
[2025-03-27 15:04:06,811] INFO: Iter 11500 Summary: 
[2025-03-27 15:04:06,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043423142954707145
[2025-03-27 15:04:34,989] INFO: Iter 11600 Summary: 
[2025-03-27 15:04:34,989] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371713496744633
[2025-03-27 15:05:03,031] INFO: Iter 11700 Summary: 
[2025-03-27 15:05:03,031] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366573859006166
[2025-03-27 15:05:30,911] INFO: Iter 11800 Summary: 
[2025-03-27 15:05:30,911] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327931553125382
[2025-03-27 15:05:58,915] INFO: Iter 11900 Summary: 
[2025-03-27 15:05:58,916] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043304727971553804
[2025-03-27 15:06:27,917] INFO: Iter 12000 Summary: 
[2025-03-27 15:06:27,917] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351072043180466
[2025-03-27 15:06:56,114] INFO: Iter 12100 Summary: 
[2025-03-27 15:06:56,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324725206941366
[2025-03-27 15:07:24,225] INFO: Iter 12200 Summary: 
[2025-03-27 15:07:24,225] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326152633875609
[2025-03-27 15:07:52,304] INFO: Iter 12300 Summary: 
[2025-03-27 15:07:52,304] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043234428763389586
[2025-03-27 15:08:20,904] INFO: Iter 12400 Summary: 
[2025-03-27 15:08:20,904] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042961729168891905
[2025-03-27 15:08:48,995] INFO: Iter 12500 Summary: 
[2025-03-27 15:08:48,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043149159252643586
[2025-03-27 15:09:17,235] INFO: Iter 12600 Summary: 
[2025-03-27 15:09:17,235] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346694178879261
[2025-03-27 15:09:45,451] INFO: Iter 12700 Summary: 
[2025-03-27 15:09:45,451] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04286337073892355
[2025-03-27 15:10:13,738] INFO: Iter 12800 Summary: 
[2025-03-27 15:10:13,738] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042945913523435596
[2025-03-27 15:10:41,939] INFO: Iter 12900 Summary: 
[2025-03-27 15:10:41,939] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296220101416111
[2025-03-27 15:11:10,143] INFO: Iter 13000 Summary: 
[2025-03-27 15:11:10,143] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316669885069132
[2025-03-27 15:11:38,262] INFO: Iter 13100 Summary: 
[2025-03-27 15:11:38,262] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04283607821911573
[2025-03-27 15:12:06,520] INFO: Iter 13200 Summary: 
[2025-03-27 15:12:06,521] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04284658413380384
[2025-03-27 15:12:34,634] INFO: Iter 13300 Summary: 
[2025-03-27 15:12:34,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04274474676698446
[2025-03-27 15:13:02,877] INFO: Iter 13400 Summary: 
[2025-03-27 15:13:02,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042780365869402884
[2025-03-27 15:13:31,138] INFO: Iter 13500 Summary: 
[2025-03-27 15:13:31,138] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042822097018361094
[2025-03-27 15:13:59,426] INFO: Iter 13600 Summary: 
[2025-03-27 15:13:59,426] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04263179611414671
[2025-03-27 15:14:27,560] INFO: Iter 13700 Summary: 
[2025-03-27 15:14:27,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0425363589450717
[2025-03-27 15:14:55,652] INFO: Iter 13800 Summary: 
[2025-03-27 15:14:55,652] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042644629068672654
[2025-03-27 15:15:23,902] INFO: Iter 13900 Summary: 
[2025-03-27 15:15:23,902] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042574195228517055
[2025-03-27 15:15:52,083] INFO: Iter 14000 Summary: 
[2025-03-27 15:15:52,083] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042417608462274076
[2025-03-27 15:16:20,296] INFO: Iter 14100 Summary: 
[2025-03-27 15:16:20,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04244217522442341
[2025-03-27 15:16:48,643] INFO: Iter 14200 Summary: 
[2025-03-27 15:16:48,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04276251219213009
[2025-03-27 15:17:16,810] INFO: Iter 14300 Summary: 
[2025-03-27 15:17:16,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04227613303810358
[2025-03-27 15:17:45,003] INFO: Iter 14400 Summary: 
[2025-03-27 15:17:45,003] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04257926631718874
[2025-03-27 15:18:13,167] INFO: Iter 14500 Summary: 
[2025-03-27 15:18:13,167] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0423652433976531
[2025-03-27 15:18:41,310] INFO: Iter 14600 Summary: 
[2025-03-27 15:18:41,311] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04202359989285469
[2025-03-27 15:19:09,546] INFO: Iter 14700 Summary: 
[2025-03-27 15:19:09,546] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04252763915807009
[2025-03-27 15:19:37,688] INFO: Iter 14800 Summary: 
[2025-03-27 15:19:37,689] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0424383956566453
[2025-03-27 15:20:06,046] INFO: Iter 14900 Summary: 
[2025-03-27 15:20:06,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04254950750619173
[2025-03-27 15:20:34,166] INFO: Iter 15000 Summary: 
[2025-03-27 15:20:34,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04259636342525482
[2025-03-27 15:21:09,437] INFO: Iter 15100 Summary: 
[2025-03-27 15:21:09,437] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04187404114753008
[2025-03-27 15:21:37,860] INFO: Iter 15200 Summary: 
[2025-03-27 15:21:37,861] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04229228679090738
[2025-03-27 15:22:06,136] INFO: Iter 15300 Summary: 
[2025-03-27 15:22:06,136] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04232952505350113
[2025-03-27 15:22:34,267] INFO: Iter 15400 Summary: 
[2025-03-27 15:22:34,267] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04256001405417919
[2025-03-27 15:23:02,515] INFO: Iter 15500 Summary: 
[2025-03-27 15:23:02,515] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042025483772158624
[2025-03-27 15:23:30,751] INFO: Iter 15600 Summary: 
[2025-03-27 15:23:30,751] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042312452010810374
[2025-03-27 15:23:58,910] INFO: Iter 15700 Summary: 
[2025-03-27 15:23:58,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04207139130681753
[2025-03-27 15:24:27,169] INFO: Iter 15800 Summary: 
[2025-03-27 15:24:27,169] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042142093367874625
[2025-03-27 15:24:55,343] INFO: Iter 15900 Summary: 
[2025-03-27 15:24:55,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04197003711014986
[2025-03-27 15:25:23,628] INFO: Iter 16000 Summary: 
[2025-03-27 15:25:23,628] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042036726325750354
[2025-03-27 15:25:51,832] INFO: Iter 16100 Summary: 
[2025-03-27 15:25:51,832] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0418594803661108
[2025-03-27 15:26:20,033] INFO: Iter 16200 Summary: 
[2025-03-27 15:26:20,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041890981309115886
[2025-03-27 15:26:48,240] INFO: Iter 16300 Summary: 
[2025-03-27 15:26:48,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041949711292982104
[2025-03-27 15:27:16,392] INFO: Iter 16400 Summary: 
[2025-03-27 15:27:16,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04225358419120312
[2025-03-27 15:27:44,544] INFO: Iter 16500 Summary: 
[2025-03-27 15:27:44,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04201393220573664
[2025-03-27 15:28:12,748] INFO: Iter 16600 Summary: 
[2025-03-27 15:28:12,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165809899568558
[2025-03-27 15:28:41,050] INFO: Iter 16700 Summary: 
[2025-03-27 15:28:41,050] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04182924184948206
[2025-03-27 15:29:09,289] INFO: Iter 16800 Summary: 
[2025-03-27 15:29:09,289] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041823956221342086
[2025-03-27 15:29:37,456] INFO: Iter 16900 Summary: 
[2025-03-27 15:29:37,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04194178730249405
[2025-03-27 15:30:05,717] INFO: Iter 17000 Summary: 
[2025-03-27 15:30:05,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04149287268519401
[2025-03-27 15:30:33,984] INFO: Iter 17100 Summary: 
[2025-03-27 15:30:33,984] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042000486440956594
[2025-03-27 15:31:02,110] INFO: Iter 17200 Summary: 
[2025-03-27 15:31:02,110] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04181637395173311
[2025-03-27 15:31:30,283] INFO: Iter 17300 Summary: 
[2025-03-27 15:31:30,283] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168375056236982
[2025-03-27 15:31:58,540] INFO: Iter 17400 Summary: 
[2025-03-27 15:31:58,540] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041925440393388275
[2025-03-27 15:32:26,713] INFO: Iter 17500 Summary: 
[2025-03-27 15:32:26,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041909749507904055
[2025-03-27 15:32:55,057] INFO: Iter 17600 Summary: 
[2025-03-27 15:32:55,057] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041702653244137765
[2025-03-27 15:33:23,420] INFO: Iter 17700 Summary: 
[2025-03-27 15:33:23,420] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04154967576265335
[2025-03-27 15:33:51,632] INFO: Iter 17800 Summary: 
[2025-03-27 15:33:51,632] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041657634638249876
[2025-03-27 15:34:19,963] INFO: Iter 17900 Summary: 
[2025-03-27 15:34:19,963] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041485720165073875
[2025-03-27 15:34:48,148] INFO: Iter 18000 Summary: 
[2025-03-27 15:34:48,148] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04167863667011261
[2025-03-27 15:35:16,314] INFO: Iter 18100 Summary: 
[2025-03-27 15:35:16,314] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153503213077783
[2025-03-27 15:35:44,561] INFO: Iter 18200 Summary: 
[2025-03-27 15:35:44,561] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04175286322832108
[2025-03-27 15:36:12,788] INFO: Iter 18300 Summary: 
[2025-03-27 15:36:12,788] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04165306735783816
[2025-03-27 15:36:41,098] INFO: Iter 18400 Summary: 
[2025-03-27 15:36:41,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155572850257158
[2025-03-27 15:37:09,263] INFO: Iter 18500 Summary: 
[2025-03-27 15:37:09,263] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04146904997527599
[2025-03-27 15:37:37,471] INFO: Iter 18600 Summary: 
[2025-03-27 15:37:37,471] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04163960196077823
[2025-03-27 15:38:05,737] INFO: Iter 18700 Summary: 
[2025-03-27 15:38:05,737] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04148095481097698
[2025-03-27 15:38:33,999] INFO: Iter 18800 Summary: 
[2025-03-27 15:38:33,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04139668181538582
[2025-03-27 15:39:02,282] INFO: Iter 18900 Summary: 
[2025-03-27 15:39:02,282] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155068412423134
[2025-03-27 15:39:30,510] INFO: Iter 19000 Summary: 
[2025-03-27 15:39:30,510] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04157551810145378
[2025-03-27 15:39:58,661] INFO: Iter 19100 Summary: 
[2025-03-27 15:39:58,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041412553004920485
[2025-03-27 15:40:27,948] INFO: Iter 19200 Summary: 
[2025-03-27 15:40:27,949] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04140574678778648
[2025-03-27 15:40:56,098] INFO: Iter 19300 Summary: 
[2025-03-27 15:40:56,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145525500178337
[2025-03-27 15:41:24,156] INFO: Iter 19400 Summary: 
[2025-03-27 15:41:24,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041232666708528994
[2025-03-27 15:41:52,342] INFO: Iter 19500 Summary: 
[2025-03-27 15:41:52,342] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041279917657375334
[2025-03-27 15:42:20,501] INFO: Iter 19600 Summary: 
[2025-03-27 15:42:20,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04153652708977461
[2025-03-27 15:42:48,754] INFO: Iter 19700 Summary: 
[2025-03-27 15:42:48,754] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041608084812760356
[2025-03-27 15:43:17,054] INFO: Iter 19800 Summary: 
[2025-03-27 15:43:17,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04137492183595896
[2025-03-27 15:43:45,232] INFO: Iter 19900 Summary: 
[2025-03-27 15:43:45,232] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041093552894890306
[2025-03-27 15:44:13,367] INFO: Iter 20000 Summary: 
[2025-03-27 15:44:13,368] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04138925150036812
[2025-03-27 15:44:48,266] INFO: Iter 20100 Summary: 
[2025-03-27 15:44:48,266] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041542060039937496
[2025-03-27 15:45:16,471] INFO: Iter 20200 Summary: 
[2025-03-27 15:45:16,471] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041299914196133616
[2025-03-27 15:45:44,732] INFO: Iter 20300 Summary: 
[2025-03-27 15:45:44,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041041623018682
[2025-03-27 15:46:12,836] INFO: Iter 20400 Summary: 
[2025-03-27 15:46:12,836] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168195985257626
[2025-03-27 15:46:41,036] INFO: Iter 20500 Summary: 
[2025-03-27 15:46:41,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04116824604570866
[2025-03-27 15:47:09,213] INFO: Iter 20600 Summary: 
[2025-03-27 15:47:09,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04130369149148464
[2025-03-27 15:47:37,463] INFO: Iter 20700 Summary: 
[2025-03-27 15:47:37,464] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04113837849348784
[2025-03-27 15:48:05,670] INFO: Iter 20800 Summary: 
[2025-03-27 15:48:05,670] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041218544393777846
[2025-03-27 15:48:33,777] INFO: Iter 20900 Summary: 
[2025-03-27 15:48:33,777] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068817496299744
[2025-03-27 15:49:02,038] INFO: Iter 21000 Summary: 
[2025-03-27 15:49:02,038] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110053025186062
[2025-03-27 15:49:30,148] INFO: Iter 21100 Summary: 
[2025-03-27 15:49:30,148] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041341228820383547
[2025-03-27 15:49:58,248] INFO: Iter 21200 Summary: 
[2025-03-27 15:49:58,248] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145940162241459
[2025-03-27 15:50:26,230] INFO: Iter 21300 Summary: 
[2025-03-27 15:50:26,230] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04136125065386295
[2025-03-27 15:50:54,302] INFO: Iter 21400 Summary: 
[2025-03-27 15:50:54,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040995805896818636
[2025-03-27 15:51:22,353] INFO: Iter 21500 Summary: 
[2025-03-27 15:51:22,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04089998587965965
[2025-03-27 15:51:50,523] INFO: Iter 21600 Summary: 
[2025-03-27 15:51:50,523] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0412916674092412
[2025-03-27 15:52:18,630] INFO: Iter 21700 Summary: 
[2025-03-27 15:52:18,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04096870582550764
[2025-03-27 15:52:46,839] INFO: Iter 21800 Summary: 
[2025-03-27 15:52:46,839] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118104003369808
[2025-03-27 15:53:15,082] INFO: Iter 21900 Summary: 
[2025-03-27 15:53:15,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04112858448177576
[2025-03-27 15:53:43,167] INFO: Iter 22000 Summary: 
[2025-03-27 15:53:43,167] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041169697567820546
[2025-03-27 15:54:11,367] INFO: Iter 22100 Summary: 
[2025-03-27 15:54:11,368] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405092404410243
[2025-03-27 15:54:39,560] INFO: Iter 22200 Summary: 
[2025-03-27 15:54:39,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0407518195733428
[2025-03-27 15:55:07,645] INFO: Iter 22300 Summary: 
[2025-03-27 15:55:07,645] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041050582490861416
[2025-03-27 15:55:35,886] INFO: Iter 22400 Summary: 
[2025-03-27 15:55:35,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408492099493742
[2025-03-27 15:56:04,226] INFO: Iter 22500 Summary: 
[2025-03-27 15:56:04,226] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0409961424395442
[2025-03-27 15:56:32,338] INFO: Iter 22600 Summary: 
[2025-03-27 15:56:32,339] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04135877802968025
[2025-03-27 15:57:00,464] INFO: Iter 22700 Summary: 
[2025-03-27 15:57:00,464] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04158662248402834
[2025-03-27 15:57:28,577] INFO: Iter 22800 Summary: 
[2025-03-27 15:57:28,577] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04100227441638708
[2025-03-27 15:57:56,557] INFO: Iter 22900 Summary: 
[2025-03-27 15:57:56,557] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04090706624090672
[2025-03-27 15:58:24,704] INFO: Iter 23000 Summary: 
[2025-03-27 15:58:24,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093770138919353
[2025-03-27 15:58:52,837] INFO: Iter 23100 Summary: 
[2025-03-27 15:58:52,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408388651162386
[2025-03-27 15:59:20,897] INFO: Iter 23200 Summary: 
[2025-03-27 15:59:20,897] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080089289695024
[2025-03-27 15:59:48,984] INFO: Iter 23300 Summary: 
[2025-03-27 15:59:48,984] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04110186990350485
[2025-03-27 16:00:17,293] INFO: Iter 23400 Summary: 
[2025-03-27 16:00:17,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118562676012516
[2025-03-27 16:00:45,717] INFO: Iter 23500 Summary: 
[2025-03-27 16:00:45,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067198559641838
[2025-03-27 16:01:13,909] INFO: Iter 23600 Summary: 
[2025-03-27 16:01:13,909] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04073971249163151
[2025-03-27 16:01:42,165] INFO: Iter 23700 Summary: 
[2025-03-27 16:01:42,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118783336132765
[2025-03-27 16:02:10,556] INFO: Iter 23800 Summary: 
[2025-03-27 16:02:10,556] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04081785511225462
[2025-03-27 16:02:38,789] INFO: Iter 23900 Summary: 
[2025-03-27 16:02:38,790] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04091232247650623
[2025-03-27 16:03:07,328] INFO: Iter 24000 Summary: 
[2025-03-27 16:03:07,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040950687900185585
[2025-03-27 16:03:35,529] INFO: Iter 24100 Summary: 
[2025-03-27 16:03:35,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093142617493868
[2025-03-27 16:04:03,724] INFO: Iter 24200 Summary: 
[2025-03-27 16:04:03,725] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04088189199566841
[2025-03-27 16:04:32,156] INFO: Iter 24300 Summary: 
[2025-03-27 16:04:32,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060963463038206
[2025-03-27 16:05:00,317] INFO: Iter 24400 Summary: 
[2025-03-27 16:05:00,317] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053705725818872
[2025-03-27 16:05:28,583] INFO: Iter 24500 Summary: 
[2025-03-27 16:05:28,584] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040698427967727185
[2025-03-27 16:05:56,844] INFO: Iter 24600 Summary: 
[2025-03-27 16:05:56,844] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095322467386722
[2025-03-27 16:06:25,051] INFO: Iter 24700 Summary: 
[2025-03-27 16:06:25,052] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04080451674759388
[2025-03-27 16:06:53,229] INFO: Iter 24800 Summary: 
[2025-03-27 16:06:53,229] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408536421507597
[2025-03-27 16:07:21,463] INFO: Iter 24900 Summary: 
[2025-03-27 16:07:21,464] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040972840487957
[2025-03-27 16:07:49,609] INFO: Iter 25000 Summary: 
[2025-03-27 16:07:49,609] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084993671625853
[2025-03-27 16:08:24,694] INFO: Iter 25100 Summary: 
[2025-03-27 16:08:24,694] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061872538179159
[2025-03-27 16:08:53,061] INFO: Iter 25200 Summary: 
[2025-03-27 16:08:53,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04043906804174185
[2025-03-27 16:09:21,437] INFO: Iter 25300 Summary: 
[2025-03-27 16:09:21,437] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040832564756274226
[2025-03-27 16:09:49,819] INFO: Iter 25400 Summary: 
[2025-03-27 16:09:49,819] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074161302298308
[2025-03-27 16:10:18,037] INFO: Iter 25500 Summary: 
[2025-03-27 16:10:18,037] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041060957312583926
[2025-03-27 16:10:46,325] INFO: Iter 25600 Summary: 
[2025-03-27 16:10:46,325] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404279800504446
[2025-03-27 16:11:14,466] INFO: Iter 25700 Summary: 
[2025-03-27 16:11:14,466] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068828206509352
[2025-03-27 16:11:42,821] INFO: Iter 25800 Summary: 
[2025-03-27 16:11:42,821] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069079078733921
[2025-03-27 16:12:11,157] INFO: Iter 25900 Summary: 
[2025-03-27 16:12:11,157] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04078167922794819
[2025-03-27 16:12:39,240] INFO: Iter 26000 Summary: 
[2025-03-27 16:12:39,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040673726387321946
[2025-03-27 16:13:07,495] INFO: Iter 26100 Summary: 
[2025-03-27 16:13:07,496] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095667663961649
[2025-03-27 16:13:35,663] INFO: Iter 26200 Summary: 
[2025-03-27 16:13:35,663] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048498272895813
[2025-03-27 16:14:04,512] INFO: Iter 26300 Summary: 
[2025-03-27 16:14:04,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068731527775526
[2025-03-27 16:14:32,750] INFO: Iter 26400 Summary: 
[2025-03-27 16:14:32,751] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0408188908547163
[2025-03-27 16:15:01,151] INFO: Iter 26500 Summary: 
[2025-03-27 16:15:01,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04085890453308821
[2025-03-27 16:15:29,431] INFO: Iter 26600 Summary: 
[2025-03-27 16:15:29,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040534629449248315
[2025-03-27 16:15:57,782] INFO: Iter 26700 Summary: 
[2025-03-27 16:15:57,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084464132785797
[2025-03-27 16:16:26,100] INFO: Iter 26800 Summary: 
[2025-03-27 16:16:26,100] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04053069945424795
[2025-03-27 16:16:54,404] INFO: Iter 26900 Summary: 
[2025-03-27 16:16:54,404] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040677514709532264
[2025-03-27 16:17:22,680] INFO: Iter 27000 Summary: 
[2025-03-27 16:17:22,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04069628566503525
[2025-03-27 16:17:50,994] INFO: Iter 27100 Summary: 
[2025-03-27 16:17:50,994] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040576146207749846
[2025-03-27 16:18:19,369] INFO: Iter 27200 Summary: 
[2025-03-27 16:18:19,369] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061826728284359
[2025-03-27 16:18:47,646] INFO: Iter 27300 Summary: 
[2025-03-27 16:18:47,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040502460934221746
[2025-03-27 16:19:15,939] INFO: Iter 27400 Summary: 
[2025-03-27 16:19:15,939] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040391418300569054
[2025-03-27 16:19:44,169] INFO: Iter 27500 Summary: 
[2025-03-27 16:19:44,169] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04060128532350063
[2025-03-27 16:20:12,546] INFO: Iter 27600 Summary: 
[2025-03-27 16:20:12,546] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04030371893197298
[2025-03-27 16:20:40,823] INFO: Iter 27700 Summary: 
[2025-03-27 16:20:40,823] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04055089380592108
[2025-03-27 16:21:09,297] INFO: Iter 27800 Summary: 
[2025-03-27 16:21:09,297] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0406524433195591
[2025-03-27 16:21:37,591] INFO: Iter 27900 Summary: 
[2025-03-27 16:21:37,591] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058287750929594
[2025-03-27 16:22:05,913] INFO: Iter 28000 Summary: 
[2025-03-27 16:22:05,913] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067306347191334
[2025-03-27 16:22:34,008] INFO: Iter 28100 Summary: 
[2025-03-27 16:22:34,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04037954095751047
[2025-03-27 16:23:02,285] INFO: Iter 28200 Summary: 
[2025-03-27 16:23:02,285] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04038192193955183
[2025-03-27 16:23:30,689] INFO: Iter 28300 Summary: 
[2025-03-27 16:23:30,689] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040238456577062605
[2025-03-27 16:23:59,006] INFO: Iter 28400 Summary: 
[2025-03-27 16:23:59,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04046010196208954
[2025-03-27 16:24:27,320] INFO: Iter 28500 Summary: 
[2025-03-27 16:24:27,320] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04056391391903162
[2025-03-27 16:24:55,575] INFO: Iter 28600 Summary: 
[2025-03-27 16:24:55,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040525965727865695
[2025-03-27 16:25:23,853] INFO: Iter 28700 Summary: 
[2025-03-27 16:25:23,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074303459376097
[2025-03-27 16:25:52,065] INFO: Iter 28800 Summary: 
[2025-03-27 16:25:52,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405935624986887
[2025-03-27 16:26:20,374] INFO: Iter 28900 Summary: 
[2025-03-27 16:26:20,374] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04084486674517393
[2025-03-27 16:26:48,687] INFO: Iter 29000 Summary: 
[2025-03-27 16:26:48,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040621427595615385
[2025-03-27 16:27:23,738] INFO: Iter 29100 Summary: 
[2025-03-27 16:27:23,738] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04022378820925951
[2025-03-27 16:27:52,054] INFO: Iter 29200 Summary: 
[2025-03-27 16:27:52,054] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040208502225577834
[2025-03-27 16:28:20,538] INFO: Iter 29300 Summary: 
[2025-03-27 16:28:20,538] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040462910942733285
[2025-03-27 16:28:48,699] INFO: Iter 29400 Summary: 
[2025-03-27 16:28:48,699] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04041025843471289
[2025-03-27 16:29:17,019] INFO: Iter 29500 Summary: 
[2025-03-27 16:29:17,019] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0404819118976593
[2025-03-27 16:29:45,267] INFO: Iter 29600 Summary: 
[2025-03-27 16:29:45,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04064005315303802
[2025-03-27 16:30:13,504] INFO: Iter 29700 Summary: 
[2025-03-27 16:30:13,504] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04048785202205181
[2025-03-27 16:30:41,786] INFO: Iter 29800 Summary: 
[2025-03-27 16:30:41,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04010299898684025
[2025-03-27 16:31:10,089] INFO: Iter 29900 Summary: 
[2025-03-27 16:31:10,090] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04034921482205391
[2025-03-27 16:31:38,309] INFO: Iter 30000 Summary: 
[2025-03-27 16:31:38,310] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04058229215443134
[2025-03-27 16:32:13,338] INFO: Iter 30100 Summary: 
[2025-03-27 16:32:13,338] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.03973235040903091
[2025-03-27 16:32:41,831] INFO: Iter 30200 Summary: 
[2025-03-27 16:32:41,831] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039118692390620706
[2025-03-27 16:33:10,221] INFO: Iter 30300 Summary: 
[2025-03-27 16:33:10,221] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03928719613701105
[2025-03-27 16:33:38,442] INFO: Iter 30400 Summary: 
[2025-03-27 16:33:38,443] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038738011457026
[2025-03-27 16:34:06,718] INFO: Iter 30500 Summary: 
[2025-03-27 16:34:06,719] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03889032088220119
[2025-03-27 16:34:35,040] INFO: Iter 30600 Summary: 
[2025-03-27 16:34:35,040] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03861566469073296
[2025-03-27 16:35:03,522] INFO: Iter 30700 Summary: 
[2025-03-27 16:35:03,523] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03872161664068699
[2025-03-27 16:35:31,791] INFO: Iter 30800 Summary: 
[2025-03-27 16:35:31,791] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038844494484364986
[2025-03-27 16:36:00,100] INFO: Iter 30900 Summary: 
[2025-03-27 16:36:00,100] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038778597079217436
[2025-03-27 16:36:28,370] INFO: Iter 31000 Summary: 
[2025-03-27 16:36:28,370] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038575371727347374
[2025-03-27 16:37:03,646] INFO: Iter 31100 Summary: 
[2025-03-27 16:37:03,646] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038529667183756826
[2025-03-27 16:37:31,917] INFO: Iter 31200 Summary: 
[2025-03-27 16:37:31,917] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038481415137648585
[2025-03-27 16:38:00,286] INFO: Iter 31300 Summary: 
[2025-03-27 16:38:00,286] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03858761254698038
[2025-03-27 16:38:28,491] INFO: Iter 31400 Summary: 
[2025-03-27 16:38:28,491] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038527636267244814
[2025-03-27 16:38:56,831] INFO: Iter 31500 Summary: 
[2025-03-27 16:38:56,831] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859547201544047
[2025-03-27 16:39:25,098] INFO: Iter 31600 Summary: 
[2025-03-27 16:39:25,098] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03838738303631544
[2025-03-27 16:39:53,372] INFO: Iter 31700 Summary: 
[2025-03-27 16:39:53,372] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855763614177704
[2025-03-27 16:40:21,766] INFO: Iter 31800 Summary: 
[2025-03-27 16:40:21,766] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843981295824051
[2025-03-27 16:40:49,967] INFO: Iter 31900 Summary: 
[2025-03-27 16:40:49,967] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038455543145537376
[2025-03-27 16:41:18,351] INFO: Iter 32000 Summary: 
[2025-03-27 16:41:18,351] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03880749378353357
[2025-03-27 16:41:53,681] INFO: Iter 32100 Summary: 
[2025-03-27 16:41:53,681] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851503048092127
[2025-03-27 16:42:22,009] INFO: Iter 32200 Summary: 
[2025-03-27 16:42:22,009] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03850121602416039
[2025-03-27 16:42:50,270] INFO: Iter 32300 Summary: 
[2025-03-27 16:42:50,270] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816764749586582
[2025-03-27 16:43:18,779] INFO: Iter 32400 Summary: 
[2025-03-27 16:43:18,780] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03848932571709156
[2025-03-27 16:43:47,129] INFO: Iter 32500 Summary: 
[2025-03-27 16:43:47,129] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03859693259000778
[2025-03-27 16:44:15,444] INFO: Iter 32600 Summary: 
[2025-03-27 16:44:15,444] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038555137366056445
[2025-03-27 16:44:43,724] INFO: Iter 32700 Summary: 
[2025-03-27 16:44:43,724] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837333284318447
[2025-03-27 16:45:12,039] INFO: Iter 32800 Summary: 
[2025-03-27 16:45:12,039] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03845605220645666
[2025-03-27 16:45:40,397] INFO: Iter 32900 Summary: 
[2025-03-27 16:45:40,397] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03870507031679153
[2025-03-27 16:46:08,731] INFO: Iter 33000 Summary: 
[2025-03-27 16:46:08,731] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817356590181589
[2025-03-27 16:46:43,657] INFO: Iter 33100 Summary: 
[2025-03-27 16:46:43,658] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383727602660656
[2025-03-27 16:47:11,996] INFO: Iter 33200 Summary: 
[2025-03-27 16:47:11,996] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03840629003942013
[2025-03-27 16:47:40,397] INFO: Iter 33300 Summary: 
[2025-03-27 16:47:40,398] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837062247097492
[2025-03-27 16:48:08,718] INFO: Iter 33400 Summary: 
[2025-03-27 16:48:08,718] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03860242497175932
[2025-03-27 16:48:37,016] INFO: Iter 33500 Summary: 
[2025-03-27 16:48:37,016] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03846450563520193
[2025-03-27 16:49:05,483] INFO: Iter 33600 Summary: 
[2025-03-27 16:49:05,483] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804883323609829
[2025-03-27 16:49:33,699] INFO: Iter 33700 Summary: 
[2025-03-27 16:49:33,699] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038162836283445356
[2025-03-27 16:50:02,001] INFO: Iter 33800 Summary: 
[2025-03-27 16:50:02,001] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03809049587696791
[2025-03-27 16:50:30,389] INFO: Iter 33900 Summary: 
[2025-03-27 16:50:30,389] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833961393684149
[2025-03-27 16:50:58,763] INFO: Iter 34000 Summary: 
[2025-03-27 16:50:58,764] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03821747232228517
[2025-03-27 16:51:33,709] INFO: Iter 34100 Summary: 
[2025-03-27 16:51:33,709] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843688197433948
[2025-03-27 16:52:01,942] INFO: Iter 34200 Summary: 
[2025-03-27 16:52:01,942] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03820082072168589
[2025-03-27 16:52:30,225] INFO: Iter 34300 Summary: 
[2025-03-27 16:52:30,225] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038492189683020116
[2025-03-27 16:52:58,697] INFO: Iter 34400 Summary: 
[2025-03-27 16:52:58,697] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03851904425770045
[2025-03-27 16:53:27,031] INFO: Iter 34500 Summary: 
[2025-03-27 16:53:27,031] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038166586831212046
[2025-03-27 16:53:55,352] INFO: Iter 34600 Summary: 
[2025-03-27 16:53:55,352] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836640920490027
[2025-03-27 16:54:23,582] INFO: Iter 34700 Summary: 
[2025-03-27 16:54:23,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814776938408613
[2025-03-27 16:54:51,912] INFO: Iter 34800 Summary: 
[2025-03-27 16:54:51,912] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03841575048863888
[2025-03-27 16:55:20,350] INFO: Iter 34900 Summary: 
[2025-03-27 16:55:20,350] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837616860866547
[2025-03-27 16:55:48,754] INFO: Iter 35000 Summary: 
[2025-03-27 16:55:48,754] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825340036302805
[2025-03-27 16:56:23,982] INFO: Iter 35100 Summary: 
[2025-03-27 16:56:23,983] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03836090188473463
[2025-03-27 16:56:52,194] INFO: Iter 35200 Summary: 
[2025-03-27 16:56:52,195] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038214359544217584
[2025-03-27 16:57:20,388] INFO: Iter 35300 Summary: 
[2025-03-27 16:57:20,388] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03855342034250498
[2025-03-27 16:57:48,768] INFO: Iter 35400 Summary: 
[2025-03-27 16:57:48,768] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03805466830730438
[2025-03-27 16:58:17,158] INFO: Iter 35500 Summary: 
[2025-03-27 16:58:17,158] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822425208985805
[2025-03-27 16:58:45,367] INFO: Iter 35600 Summary: 
[2025-03-27 16:58:45,367] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038065870590507984
[2025-03-27 16:59:13,608] INFO: Iter 35700 Summary: 
[2025-03-27 16:59:13,608] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814086750149727
[2025-03-27 16:59:41,844] INFO: Iter 35800 Summary: 
[2025-03-27 16:59:41,844] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817138198763132
[2025-03-27 17:00:10,202] INFO: Iter 35900 Summary: 
[2025-03-27 17:00:10,202] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818887110799551
[2025-03-27 17:00:38,486] INFO: Iter 36000 Summary: 
[2025-03-27 17:00:38,486] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825825229287148
[2025-03-27 17:01:13,669] INFO: Iter 36100 Summary: 
[2025-03-27 17:01:13,669] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03843956135213375
[2025-03-27 17:01:42,080] INFO: Iter 36200 Summary: 
[2025-03-27 17:01:42,080] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038188811540603634
[2025-03-27 17:02:10,381] INFO: Iter 36300 Summary: 
[2025-03-27 17:02:10,381] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816936280578375
[2025-03-27 17:02:38,999] INFO: Iter 36400 Summary: 
[2025-03-27 17:02:38,999] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03824284512549639
[2025-03-27 17:03:07,434] INFO: Iter 36500 Summary: 
[2025-03-27 17:03:07,434] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038122556991875174
[2025-03-27 17:03:35,765] INFO: Iter 36600 Summary: 
[2025-03-27 17:03:35,765] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381810512393713
[2025-03-27 17:04:04,011] INFO: Iter 36700 Summary: 
[2025-03-27 17:04:04,011] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822219908237457
[2025-03-27 17:04:32,303] INFO: Iter 36800 Summary: 
[2025-03-27 17:04:32,303] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038186385147273544
[2025-03-27 17:05:00,624] INFO: Iter 36900 Summary: 
[2025-03-27 17:05:00,624] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03811948042362928
[2025-03-27 17:05:28,967] INFO: Iter 37000 Summary: 
[2025-03-27 17:05:28,967] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037993066012859344
[2025-03-27 17:05:57,455] INFO: Iter 37100 Summary: 
[2025-03-27 17:05:57,455] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822493091225624
[2025-03-27 17:06:25,843] INFO: Iter 37200 Summary: 
[2025-03-27 17:06:25,843] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815663915127516
[2025-03-27 17:06:54,037] INFO: Iter 37300 Summary: 
[2025-03-27 17:06:54,037] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038145889826118946
[2025-03-27 17:07:22,248] INFO: Iter 37400 Summary: 
[2025-03-27 17:07:22,248] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038162118792533874
[2025-03-27 17:07:50,655] INFO: Iter 37500 Summary: 
[2025-03-27 17:07:50,655] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03792085308581591
[2025-03-27 17:08:18,993] INFO: Iter 37600 Summary: 
[2025-03-27 17:08:18,993] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817931767553091
[2025-03-27 17:08:47,246] INFO: Iter 37700 Summary: 
[2025-03-27 17:08:47,246] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822246626019478
[2025-03-27 17:09:15,482] INFO: Iter 37800 Summary: 
[2025-03-27 17:09:15,482] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038163269944489
[2025-03-27 17:09:43,675] INFO: Iter 37900 Summary: 
[2025-03-27 17:09:43,676] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03788881104439497
[2025-03-27 17:10:11,915] INFO: Iter 38000 Summary: 
[2025-03-27 17:10:11,915] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833088543266058
[2025-03-27 17:10:47,030] INFO: Iter 38100 Summary: 
[2025-03-27 17:10:47,030] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03810572013258934
[2025-03-27 17:11:16,409] INFO: Iter 38200 Summary: 
[2025-03-27 17:11:16,409] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038000543266534806
[2025-03-27 17:11:44,582] INFO: Iter 38300 Summary: 
[2025-03-27 17:11:44,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03830586340278387
[2025-03-27 17:12:12,739] INFO: Iter 38400 Summary: 
[2025-03-27 17:12:12,739] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03799467198550701
[2025-03-27 17:12:41,129] INFO: Iter 38500 Summary: 
[2025-03-27 17:12:41,129] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038415354825556275
[2025-03-27 17:13:09,453] INFO: Iter 38600 Summary: 
[2025-03-27 17:13:09,453] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038089186288416385
[2025-03-27 17:13:37,797] INFO: Iter 38700 Summary: 
[2025-03-27 17:13:37,797] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03842436783015728
[2025-03-27 17:14:05,996] INFO: Iter 38800 Summary: 
[2025-03-27 17:14:05,997] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038075441904366014
[2025-03-27 17:14:34,299] INFO: Iter 38900 Summary: 
[2025-03-27 17:14:34,299] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816499762237072
[2025-03-27 17:15:02,820] INFO: Iter 39000 Summary: 
[2025-03-27 17:15:02,820] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814488835632801
[2025-03-27 17:15:30,954] INFO: Iter 39100 Summary: 
[2025-03-27 17:15:30,954] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804252605885267
[2025-03-27 17:15:59,189] INFO: Iter 39200 Summary: 
[2025-03-27 17:15:59,189] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037957499362528326
[2025-03-27 17:16:27,394] INFO: Iter 39300 Summary: 
[2025-03-27 17:16:27,394] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03790841404348612
[2025-03-27 17:16:55,641] INFO: Iter 39400 Summary: 
[2025-03-27 17:16:55,641] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03830685757100582
[2025-03-27 17:17:23,874] INFO: Iter 39500 Summary: 
[2025-03-27 17:17:23,874] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03817176405340433
[2025-03-27 17:17:52,020] INFO: Iter 39600 Summary: 
[2025-03-27 17:17:52,021] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03816769782453775
[2025-03-27 17:18:20,139] INFO: Iter 39700 Summary: 
[2025-03-27 17:18:20,139] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037901812009513376
[2025-03-27 17:18:48,452] INFO: Iter 39800 Summary: 
[2025-03-27 17:18:48,452] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037788297086954116
[2025-03-27 17:19:16,735] INFO: Iter 39900 Summary: 
[2025-03-27 17:19:16,735] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037743132561445236
[2025-03-27 17:19:44,889] INFO: Iter 40000 Summary: 
[2025-03-27 17:19:44,889] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037958942987024784
[2025-03-27 17:20:19,891] INFO: Iter 40100 Summary: 
[2025-03-27 17:20:19,891] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03767644211649895
[2025-03-27 17:20:48,192] INFO: Iter 40200 Summary: 
[2025-03-27 17:20:48,194] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03819663405418396
[2025-03-27 17:21:16,582] INFO: Iter 40300 Summary: 
[2025-03-27 17:21:16,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03824318036437035
[2025-03-27 17:21:44,800] INFO: Iter 40400 Summary: 
[2025-03-27 17:21:44,800] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381899605691433
[2025-03-27 17:22:12,944] INFO: Iter 40500 Summary: 
[2025-03-27 17:22:12,944] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03804013248533011
[2025-03-27 17:22:41,256] INFO: Iter 40600 Summary: 
[2025-03-27 17:22:41,257] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03832611236721277
[2025-03-27 17:23:09,559] INFO: Iter 40700 Summary: 
[2025-03-27 17:23:09,559] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038095967397093776
[2025-03-27 17:23:37,862] INFO: Iter 40800 Summary: 
[2025-03-27 17:23:37,862] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03810315452516079
[2025-03-27 17:24:06,099] INFO: Iter 40900 Summary: 
[2025-03-27 17:24:06,099] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037965510115027426
[2025-03-27 17:24:34,351] INFO: Iter 41000 Summary: 
[2025-03-27 17:24:34,351] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03798871256411076
[2025-03-27 17:25:02,693] INFO: Iter 41100 Summary: 
[2025-03-27 17:25:02,693] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03797790851444006
[2025-03-27 17:25:31,047] INFO: Iter 41200 Summary: 
[2025-03-27 17:25:31,047] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037793961577117445
[2025-03-27 17:25:59,230] INFO: Iter 41300 Summary: 
[2025-03-27 17:25:59,230] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03794153042137623
[2025-03-27 17:26:27,534] INFO: Iter 41400 Summary: 
[2025-03-27 17:26:27,535] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03788082890212536
[2025-03-27 17:26:55,692] INFO: Iter 41500 Summary: 
[2025-03-27 17:26:55,693] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038089055195450784
[2025-03-27 17:27:23,956] INFO: Iter 41600 Summary: 
[2025-03-27 17:27:23,956] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038062816858291625
[2025-03-27 17:27:52,229] INFO: Iter 41700 Summary: 
[2025-03-27 17:27:52,229] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03791824720799923
[2025-03-27 17:28:20,477] INFO: Iter 41800 Summary: 
[2025-03-27 17:28:20,477] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0380317485705018
[2025-03-27 17:28:48,702] INFO: Iter 41900 Summary: 
[2025-03-27 17:28:48,702] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03775338150560856
[2025-03-27 17:29:17,709] INFO: Iter 42000 Summary: 
[2025-03-27 17:29:17,709] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037717398330569266
[2025-03-27 17:29:52,574] INFO: Iter 42100 Summary: 
[2025-03-27 17:29:52,574] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037819820009171966
[2025-03-27 17:30:20,683] INFO: Iter 42200 Summary: 
[2025-03-27 17:30:20,684] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03801875278353691
[2025-03-27 17:30:48,971] INFO: Iter 42300 Summary: 
[2025-03-27 17:30:48,971] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03786640636622906
[2025-03-27 17:31:17,147] INFO: Iter 42400 Summary: 
[2025-03-27 17:31:17,147] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03785699047148228
[2025-03-27 17:31:45,328] INFO: Iter 42500 Summary: 
[2025-03-27 17:31:45,328] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03806159347295761
[2025-03-27 17:32:13,698] INFO: Iter 42600 Summary: 
[2025-03-27 17:32:13,698] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03791495252400637
[2025-03-27 17:32:41,768] INFO: Iter 42700 Summary: 
[2025-03-27 17:32:41,768] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038094873912632464
[2025-03-27 17:33:10,105] INFO: Iter 42800 Summary: 
[2025-03-27 17:33:10,105] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038240421675145624
[2025-03-27 17:33:38,358] INFO: Iter 42900 Summary: 
[2025-03-27 17:33:38,358] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037867270037531854
[2025-03-27 17:34:06,841] INFO: Iter 43000 Summary: 
[2025-03-27 17:34:06,841] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03786668051034212
[2025-03-27 17:34:35,068] INFO: Iter 43100 Summary: 
[2025-03-27 17:34:35,068] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03808466672897339
[2025-03-27 17:35:03,194] INFO: Iter 43200 Summary: 
[2025-03-27 17:35:03,194] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778085920959711
[2025-03-27 17:35:31,426] INFO: Iter 43300 Summary: 
[2025-03-27 17:35:31,426] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03800005629658699
[2025-03-27 17:35:59,520] INFO: Iter 43400 Summary: 
[2025-03-27 17:35:59,521] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03805938120931387
[2025-03-27 17:36:27,671] INFO: Iter 43500 Summary: 
[2025-03-27 17:36:27,671] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03794091686606407
[2025-03-27 17:36:55,878] INFO: Iter 43600 Summary: 
[2025-03-27 17:36:55,878] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03789343874901533
[2025-03-27 17:37:24,114] INFO: Iter 43700 Summary: 
[2025-03-27 17:37:24,114] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038057901114225384
[2025-03-27 17:37:52,263] INFO: Iter 43800 Summary: 
[2025-03-27 17:37:52,263] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037710835598409174
[2025-03-27 17:38:20,445] INFO: Iter 43900 Summary: 
[2025-03-27 17:38:20,445] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03774434681981802
[2025-03-27 17:38:48,661] INFO: Iter 44000 Summary: 
[2025-03-27 17:38:48,661] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03823329944163561
[2025-03-27 17:39:23,832] INFO: Iter 44100 Summary: 
[2025-03-27 17:39:23,832] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818944130092859
[2025-03-27 17:39:52,186] INFO: Iter 44200 Summary: 
[2025-03-27 17:39:52,187] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037938823625445366
[2025-03-27 17:40:20,495] INFO: Iter 44300 Summary: 
[2025-03-27 17:40:20,495] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03793751694262028
[2025-03-27 17:40:48,649] INFO: Iter 44400 Summary: 
[2025-03-27 17:40:48,649] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761926282197237
[2025-03-27 17:41:16,968] INFO: Iter 44500 Summary: 
[2025-03-27 17:41:16,968] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037976540587842465
[2025-03-27 17:41:45,249] INFO: Iter 44600 Summary: 
[2025-03-27 17:41:45,250] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038079715333878994
[2025-03-27 17:42:13,594] INFO: Iter 44700 Summary: 
[2025-03-27 17:42:13,594] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037914549559354783
[2025-03-27 17:42:41,836] INFO: Iter 44800 Summary: 
[2025-03-27 17:42:41,836] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03790095265954733
[2025-03-27 17:43:11,087] INFO: Iter 44900 Summary: 
[2025-03-27 17:43:11,087] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03767106655985117
[2025-03-27 17:43:39,476] INFO: Iter 45000 Summary: 
[2025-03-27 17:43:39,476] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0377390543371439
[2025-03-27 17:44:07,805] INFO: Iter 45100 Summary: 
[2025-03-27 17:44:07,805] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03796487748622894
[2025-03-27 17:44:36,147] INFO: Iter 45200 Summary: 
[2025-03-27 17:44:36,147] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037955275587737564
[2025-03-27 17:45:04,502] INFO: Iter 45300 Summary: 
[2025-03-27 17:45:04,502] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03808148559182882
[2025-03-27 17:45:32,777] INFO: Iter 45400 Summary: 
[2025-03-27 17:45:32,777] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03809086386114359
[2025-03-27 17:46:01,346] INFO: Iter 45500 Summary: 
[2025-03-27 17:46:01,346] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037785701341927054
[2025-03-27 17:46:29,526] INFO: Iter 45600 Summary: 
[2025-03-27 17:46:29,526] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03773294359445572
[2025-03-27 17:46:57,835] INFO: Iter 45700 Summary: 
[2025-03-27 17:46:57,835] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778003316372633
[2025-03-27 17:47:26,110] INFO: Iter 45800 Summary: 
[2025-03-27 17:47:26,110] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037823147773742676
[2025-03-27 17:47:54,376] INFO: Iter 45900 Summary: 
[2025-03-27 17:47:54,376] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038113719187676906
[2025-03-27 17:48:22,692] INFO: Iter 46000 Summary: 
[2025-03-27 17:48:22,692] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03779738325625658
[2025-03-27 17:48:57,699] INFO: Iter 46100 Summary: 
[2025-03-27 17:48:57,699] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037822917327284815
[2025-03-27 17:49:26,179] INFO: Iter 46200 Summary: 
[2025-03-27 17:49:26,179] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03806001227349043
[2025-03-27 17:49:54,599] INFO: Iter 46300 Summary: 
[2025-03-27 17:49:54,599] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778712753206492
[2025-03-27 17:50:22,899] INFO: Iter 46400 Summary: 
[2025-03-27 17:50:22,899] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03775126360356808
[2025-03-27 17:50:51,139] INFO: Iter 46500 Summary: 
[2025-03-27 17:50:51,139] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778865788131952
[2025-03-27 17:51:19,333] INFO: Iter 46600 Summary: 
[2025-03-27 17:51:19,333] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037754150852560994
[2025-03-27 17:51:47,543] INFO: Iter 46700 Summary: 
[2025-03-27 17:51:47,543] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03791707634925842
[2025-03-27 17:52:15,764] INFO: Iter 46800 Summary: 
[2025-03-27 17:52:15,764] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381730780005455
[2025-03-27 17:52:44,339] INFO: Iter 46900 Summary: 
[2025-03-27 17:52:44,340] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037807682529091835
[2025-03-27 17:53:12,658] INFO: Iter 47000 Summary: 
[2025-03-27 17:53:12,659] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03764043338596821
[2025-03-27 17:53:40,873] INFO: Iter 47100 Summary: 
[2025-03-27 17:53:40,874] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03746975127607584
[2025-03-27 17:54:09,236] INFO: Iter 47200 Summary: 
[2025-03-27 17:54:09,236] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03798651721328497
[2025-03-27 17:54:37,513] INFO: Iter 47300 Summary: 
[2025-03-27 17:54:37,513] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03783063221722841
[2025-03-27 17:55:05,773] INFO: Iter 47400 Summary: 
[2025-03-27 17:55:05,773] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03773277834057808
[2025-03-27 17:55:34,034] INFO: Iter 47500 Summary: 
[2025-03-27 17:55:34,034] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812611348927021
[2025-03-27 17:56:02,412] INFO: Iter 47600 Summary: 
[2025-03-27 17:56:02,412] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03800198577344418
[2025-03-27 17:56:30,710] INFO: Iter 47700 Summary: 
[2025-03-27 17:56:30,710] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03788931518793106
[2025-03-27 17:56:58,908] INFO: Iter 47800 Summary: 
[2025-03-27 17:56:58,908] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761931452900171
[2025-03-27 17:57:27,240] INFO: Iter 47900 Summary: 
[2025-03-27 17:57:27,240] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03798277381807566
[2025-03-27 17:57:55,492] INFO: Iter 48000 Summary: 
[2025-03-27 17:57:55,492] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03785350862890482
[2025-03-27 17:58:30,629] INFO: Iter 48100 Summary: 
[2025-03-27 17:58:30,629] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03802709858864546
[2025-03-27 17:58:58,967] INFO: Iter 48200 Summary: 
[2025-03-27 17:58:58,968] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03769161310046911
[2025-03-27 17:59:27,338] INFO: Iter 48300 Summary: 
[2025-03-27 17:59:27,338] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03788274794816971
[2025-03-27 17:59:55,644] INFO: Iter 48400 Summary: 
[2025-03-27 17:59:55,644] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812500856816769
[2025-03-27 18:00:23,967] INFO: Iter 48500 Summary: 
[2025-03-27 18:00:23,967] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03765542849898338
[2025-03-27 18:00:52,405] INFO: Iter 48600 Summary: 
[2025-03-27 18:00:52,405] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03785348568111658
[2025-03-27 18:01:20,807] INFO: Iter 48700 Summary: 
[2025-03-27 18:01:20,807] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03785751782357693
[2025-03-27 18:01:49,186] INFO: Iter 48800 Summary: 
[2025-03-27 18:01:49,186] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03791165515780449
[2025-03-27 18:02:17,530] INFO: Iter 48900 Summary: 
[2025-03-27 18:02:17,530] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03792872305959463
[2025-03-27 18:02:45,732] INFO: Iter 49000 Summary: 
[2025-03-27 18:02:45,732] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037973134703934194
[2025-03-27 18:03:13,949] INFO: Iter 49100 Summary: 
[2025-03-27 18:03:13,949] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03763573810458183
[2025-03-27 18:03:42,280] INFO: Iter 49200 Summary: 
[2025-03-27 18:03:42,280] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03776259198784828
[2025-03-27 18:04:10,689] INFO: Iter 49300 Summary: 
[2025-03-27 18:04:10,689] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03780611079186201
[2025-03-27 18:04:38,928] INFO: Iter 49400 Summary: 
[2025-03-27 18:04:38,928] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03809908844530582
[2025-03-27 18:05:07,169] INFO: Iter 49500 Summary: 
[2025-03-27 18:05:07,169] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03786521576344967
[2025-03-27 18:05:35,687] INFO: Iter 49600 Summary: 
[2025-03-27 18:05:35,687] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0378159923106432
[2025-03-27 18:06:03,902] INFO: Iter 49700 Summary: 
[2025-03-27 18:06:03,902] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037503351271152494
[2025-03-27 18:06:32,314] INFO: Iter 49800 Summary: 
[2025-03-27 18:06:32,314] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03753429677337408
[2025-03-27 18:07:00,587] INFO: Iter 49900 Summary: 
[2025-03-27 18:07:00,587] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037793123833835124
[2025-03-27 18:07:28,941] INFO: Iter 50000 Summary: 
[2025-03-27 18:07:28,941] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0377997611835599
[2025-03-27 18:08:03,995] INFO: Iter 50100 Summary: 
[2025-03-27 18:08:03,995] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03784345410764217
[2025-03-27 18:08:32,259] INFO: Iter 50200 Summary: 
[2025-03-27 18:08:32,259] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037974317856132984
[2025-03-27 18:09:00,542] INFO: Iter 50300 Summary: 
[2025-03-27 18:09:00,542] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03762776285409927
[2025-03-27 18:09:29,017] INFO: Iter 50400 Summary: 
[2025-03-27 18:09:29,017] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03791944429278374
[2025-03-27 18:09:57,461] INFO: Iter 50500 Summary: 
[2025-03-27 18:09:57,462] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037772537730634215
[2025-03-27 18:10:25,726] INFO: Iter 50600 Summary: 
[2025-03-27 18:10:25,726] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037712267227470876
[2025-03-27 18:10:54,036] INFO: Iter 50700 Summary: 
[2025-03-27 18:10:54,036] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0376636067405343
[2025-03-27 18:11:22,448] INFO: Iter 50800 Summary: 
[2025-03-27 18:11:22,448] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03740935247391462
[2025-03-27 18:11:50,746] INFO: Iter 50900 Summary: 
[2025-03-27 18:11:50,746] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03797240171581507
[2025-03-27 18:12:19,026] INFO: Iter 51000 Summary: 
[2025-03-27 18:12:19,026] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037865644320845604
[2025-03-27 18:12:47,295] INFO: Iter 51100 Summary: 
[2025-03-27 18:12:47,295] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03762366939336061
[2025-03-27 18:13:15,534] INFO: Iter 51200 Summary: 
[2025-03-27 18:13:15,534] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03796561561524868
[2025-03-27 18:13:43,885] INFO: Iter 51300 Summary: 
[2025-03-27 18:13:43,885] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037377260215580466
[2025-03-27 18:14:12,162] INFO: Iter 51400 Summary: 
[2025-03-27 18:14:12,162] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037656398229300975
[2025-03-27 18:14:40,491] INFO: Iter 51500 Summary: 
[2025-03-27 18:14:40,491] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0378876930102706
[2025-03-27 18:15:08,713] INFO: Iter 51600 Summary: 
[2025-03-27 18:15:08,713] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737776424735784
[2025-03-27 18:15:37,026] INFO: Iter 51700 Summary: 
[2025-03-27 18:15:37,026] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03746189776808023
[2025-03-27 18:16:05,369] INFO: Iter 51800 Summary: 
[2025-03-27 18:16:05,369] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03770326919853687
[2025-03-27 18:16:34,599] INFO: Iter 51900 Summary: 
[2025-03-27 18:16:34,599] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037973077073693275
[2025-03-27 18:17:02,730] INFO: Iter 52000 Summary: 
[2025-03-27 18:17:02,730] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0377052266523242
[2025-03-27 18:17:38,030] INFO: Iter 52100 Summary: 
[2025-03-27 18:17:38,031] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03755502350628376
[2025-03-27 18:18:06,402] INFO: Iter 52200 Summary: 
[2025-03-27 18:18:06,403] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037879992797970774
[2025-03-27 18:18:34,737] INFO: Iter 52300 Summary: 
[2025-03-27 18:18:34,737] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037729646824300286
[2025-03-27 18:19:03,075] INFO: Iter 52400 Summary: 
[2025-03-27 18:19:03,075] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03753199264407158
[2025-03-27 18:19:31,448] INFO: Iter 52500 Summary: 
[2025-03-27 18:19:31,448] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037873988077044486
[2025-03-27 18:19:59,547] INFO: Iter 52600 Summary: 
[2025-03-27 18:19:59,547] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037645878568291664
[2025-03-27 18:20:27,666] INFO: Iter 52700 Summary: 
[2025-03-27 18:20:27,666] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037954792976379395
[2025-03-27 18:20:55,925] INFO: Iter 52800 Summary: 
[2025-03-27 18:20:55,926] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03762636449187994
[2025-03-27 18:21:24,093] INFO: Iter 52900 Summary: 
[2025-03-27 18:21:24,093] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037676078863441945
[2025-03-27 18:21:52,257] INFO: Iter 53000 Summary: 
[2025-03-27 18:21:52,257] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037884967401623726
[2025-03-27 18:22:20,377] INFO: Iter 53100 Summary: 
[2025-03-27 18:22:20,378] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037942915000021456
[2025-03-27 18:22:48,516] INFO: Iter 53200 Summary: 
[2025-03-27 18:22:48,516] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03790085289627314
[2025-03-27 18:23:16,765] INFO: Iter 53300 Summary: 
[2025-03-27 18:23:16,765] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737711254507303
[2025-03-27 18:23:44,875] INFO: Iter 53400 Summary: 
[2025-03-27 18:23:44,876] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03766292747110128
[2025-03-27 18:24:13,113] INFO: Iter 53500 Summary: 
[2025-03-27 18:24:13,113] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037327073030173775
[2025-03-27 18:24:41,306] INFO: Iter 53600 Summary: 
[2025-03-27 18:24:41,307] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03765706069767475
[2025-03-27 18:25:09,385] INFO: Iter 53700 Summary: 
[2025-03-27 18:25:09,385] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03768278557807207
[2025-03-27 18:25:37,611] INFO: Iter 53800 Summary: 
[2025-03-27 18:25:37,611] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761299647390842
[2025-03-27 18:26:05,593] INFO: Iter 53900 Summary: 
[2025-03-27 18:26:05,593] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03773412637412548
[2025-03-27 18:26:33,865] INFO: Iter 54000 Summary: 
[2025-03-27 18:26:33,866] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03780734833329916
[2025-03-27 18:27:09,137] INFO: Iter 54100 Summary: 
[2025-03-27 18:27:09,137] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03772807653993368
[2025-03-27 18:27:37,435] INFO: Iter 54200 Summary: 
[2025-03-27 18:27:37,435] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037655407711863514
[2025-03-27 18:28:05,718] INFO: Iter 54300 Summary: 
[2025-03-27 18:28:05,719] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778576735407114
[2025-03-27 18:28:33,895] INFO: Iter 54400 Summary: 
[2025-03-27 18:28:33,895] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037477205730974676
[2025-03-27 18:29:02,199] INFO: Iter 54500 Summary: 
[2025-03-27 18:29:02,199] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03781277764588595
[2025-03-27 18:29:30,633] INFO: Iter 54600 Summary: 
[2025-03-27 18:29:30,633] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037786485776305195
[2025-03-27 18:29:58,915] INFO: Iter 54700 Summary: 
[2025-03-27 18:29:58,915] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03754191923886538
[2025-03-27 18:30:27,361] INFO: Iter 54800 Summary: 
[2025-03-27 18:30:27,361] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0375310680270195
[2025-03-27 18:30:55,827] INFO: Iter 54900 Summary: 
[2025-03-27 18:30:55,827] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03774977721273899
[2025-03-27 18:31:24,452] INFO: Iter 55000 Summary: 
[2025-03-27 18:31:24,452] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03767434559762478
[2025-03-27 18:31:53,112] INFO: Iter 55100 Summary: 
[2025-03-27 18:31:53,112] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03773823700845241
[2025-03-27 18:32:21,671] INFO: Iter 55200 Summary: 
[2025-03-27 18:32:21,671] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03758662812411785
[2025-03-27 18:32:50,220] INFO: Iter 55300 Summary: 
[2025-03-27 18:32:50,220] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037594226002693173
[2025-03-27 18:33:18,908] INFO: Iter 55400 Summary: 
[2025-03-27 18:33:18,908] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037480034977197645
[2025-03-27 18:33:47,514] INFO: Iter 55500 Summary: 
[2025-03-27 18:33:47,514] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03763823218643665
[2025-03-27 18:34:16,154] INFO: Iter 55600 Summary: 
[2025-03-27 18:34:16,155] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03763094283640385
[2025-03-27 18:34:44,777] INFO: Iter 55700 Summary: 
[2025-03-27 18:34:44,777] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03775267604738474
[2025-03-27 18:35:13,682] INFO: Iter 55800 Summary: 
[2025-03-27 18:35:13,682] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03785800311714411
[2025-03-27 18:35:42,382] INFO: Iter 55900 Summary: 
[2025-03-27 18:35:42,382] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03736301738768816
[2025-03-27 18:36:11,010] INFO: Iter 56000 Summary: 
[2025-03-27 18:36:11,011] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037586407326161864
[2025-03-27 18:36:46,777] INFO: Iter 56100 Summary: 
[2025-03-27 18:36:46,777] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03747418940067291
[2025-03-27 18:37:15,541] INFO: Iter 56200 Summary: 
[2025-03-27 18:37:15,542] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03781890243291855
[2025-03-27 18:37:44,043] INFO: Iter 56300 Summary: 
[2025-03-27 18:37:44,043] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03780921820551157
[2025-03-27 18:38:12,659] INFO: Iter 56400 Summary: 
[2025-03-27 18:38:12,659] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0377571627497673
[2025-03-27 18:38:41,381] INFO: Iter 56500 Summary: 
[2025-03-27 18:38:41,381] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03773605372756719
[2025-03-27 18:39:10,042] INFO: Iter 56600 Summary: 
[2025-03-27 18:39:10,042] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03771576192229986
[2025-03-27 18:39:38,685] INFO: Iter 56700 Summary: 
[2025-03-27 18:39:38,685] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761216811835766
[2025-03-27 18:40:07,204] INFO: Iter 56800 Summary: 
[2025-03-27 18:40:07,204] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742406696081162
[2025-03-27 18:40:35,806] INFO: Iter 56900 Summary: 
[2025-03-27 18:40:35,806] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037739953584969045
[2025-03-27 18:41:04,281] INFO: Iter 57000 Summary: 
[2025-03-27 18:41:04,281] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03735918741673231
[2025-03-27 18:41:32,826] INFO: Iter 57100 Summary: 
[2025-03-27 18:41:32,826] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03755208652466536
[2025-03-27 18:42:01,508] INFO: Iter 57200 Summary: 
[2025-03-27 18:42:01,508] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037859505154192445
[2025-03-27 18:42:30,178] INFO: Iter 57300 Summary: 
[2025-03-27 18:42:30,178] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037476503066718576
[2025-03-27 18:42:59,004] INFO: Iter 57400 Summary: 
[2025-03-27 18:42:59,005] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037529301382601264
[2025-03-27 18:43:27,581] INFO: Iter 57500 Summary: 
[2025-03-27 18:43:27,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03766949523240328
[2025-03-27 18:43:57,728] INFO: Iter 57600 Summary: 
[2025-03-27 18:43:57,728] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03768739078193903
[2025-03-27 18:44:26,055] INFO: Iter 57700 Summary: 
[2025-03-27 18:44:26,055] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037200153209269046
[2025-03-27 18:44:54,517] INFO: Iter 57800 Summary: 
[2025-03-27 18:44:54,518] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03780767302960157
[2025-03-27 18:45:23,511] INFO: Iter 57900 Summary: 
[2025-03-27 18:45:23,511] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03752712804824114
[2025-03-27 18:45:52,794] INFO: Iter 58000 Summary: 
[2025-03-27 18:45:52,794] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03758165594190359
[2025-03-27 18:46:28,984] INFO: Iter 58100 Summary: 
[2025-03-27 18:46:28,985] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037648831643164155
[2025-03-27 18:46:58,243] INFO: Iter 58200 Summary: 
[2025-03-27 18:46:58,243] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03758588530123234
[2025-03-27 18:47:27,430] INFO: Iter 58300 Summary: 
[2025-03-27 18:47:27,430] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03798157289624214
[2025-03-27 18:47:56,577] INFO: Iter 58400 Summary: 
[2025-03-27 18:47:56,577] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037593988105654716
[2025-03-27 18:48:25,653] INFO: Iter 58500 Summary: 
[2025-03-27 18:48:25,653] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03746341321617365
[2025-03-27 18:48:54,985] INFO: Iter 58600 Summary: 
[2025-03-27 18:48:54,985] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03758337020874023
[2025-03-27 18:49:24,222] INFO: Iter 58700 Summary: 
[2025-03-27 18:49:24,223] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037341609112918374
[2025-03-27 18:49:53,403] INFO: Iter 58800 Summary: 
[2025-03-27 18:49:53,403] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03759952664375305
[2025-03-27 18:50:22,477] INFO: Iter 58900 Summary: 
[2025-03-27 18:50:22,477] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03756716020405292
[2025-03-27 18:50:51,595] INFO: Iter 59000 Summary: 
[2025-03-27 18:50:51,596] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03772982358932495
[2025-03-27 18:51:20,697] INFO: Iter 59100 Summary: 
[2025-03-27 18:51:20,697] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037687019556760785
[2025-03-27 18:51:49,990] INFO: Iter 59200 Summary: 
[2025-03-27 18:51:49,990] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738290041685104
[2025-03-27 18:52:19,240] INFO: Iter 59300 Summary: 
[2025-03-27 18:52:19,240] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03771329369395971
[2025-03-27 18:52:48,604] INFO: Iter 59400 Summary: 
[2025-03-27 18:52:48,604] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03749585900455713
[2025-03-27 18:53:17,853] INFO: Iter 59500 Summary: 
[2025-03-27 18:53:17,853] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03759583182632923
[2025-03-27 18:53:47,100] INFO: Iter 59600 Summary: 
[2025-03-27 18:53:47,100] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03736426316201687
[2025-03-27 18:54:16,372] INFO: Iter 59700 Summary: 
[2025-03-27 18:54:16,372] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037687351889908315
[2025-03-27 18:54:45,455] INFO: Iter 59800 Summary: 
[2025-03-27 18:54:45,455] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03778277419507504
[2025-03-27 18:55:14,635] INFO: Iter 59900 Summary: 
[2025-03-27 18:55:14,636] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03711853161454201
[2025-03-27 18:55:43,090] INFO: Iter 60000 Summary: 
[2025-03-27 18:55:43,090] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03722880888730287
[2025-03-27 18:56:18,456] INFO: Iter 60100 Summary: 
[2025-03-27 18:56:18,456] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0374541312456131
[2025-03-27 18:56:46,931] INFO: Iter 60200 Summary: 
[2025-03-27 18:56:46,931] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03729713626205921
[2025-03-27 18:57:15,536] INFO: Iter 60300 Summary: 
[2025-03-27 18:57:15,537] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037842622809112075
[2025-03-27 18:57:44,032] INFO: Iter 60400 Summary: 
[2025-03-27 18:57:44,032] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037770907208323476
[2025-03-27 18:58:12,338] INFO: Iter 60500 Summary: 
[2025-03-27 18:58:12,338] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03748910166323185
[2025-03-27 18:58:40,827] INFO: Iter 60600 Summary: 
[2025-03-27 18:58:40,827] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037606469951570035
[2025-03-27 18:59:09,412] INFO: Iter 60700 Summary: 
[2025-03-27 18:59:09,413] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037611652202904224
[2025-03-27 18:59:37,751] INFO: Iter 60800 Summary: 
[2025-03-27 18:59:37,751] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03741900984197855
[2025-03-27 19:00:06,169] INFO: Iter 60900 Summary: 
[2025-03-27 19:00:06,169] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728305969387293
[2025-03-27 19:00:34,640] INFO: Iter 61000 Summary: 
[2025-03-27 19:00:34,640] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03764251295477152
[2025-03-27 19:01:02,981] INFO: Iter 61100 Summary: 
[2025-03-27 19:01:02,982] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03740172252058983
[2025-03-27 19:01:31,412] INFO: Iter 61200 Summary: 
[2025-03-27 19:01:31,412] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03786959432065487
[2025-03-27 19:01:59,729] INFO: Iter 61300 Summary: 
[2025-03-27 19:01:59,729] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0375600753352046
[2025-03-27 19:02:28,046] INFO: Iter 61400 Summary: 
[2025-03-27 19:02:28,046] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037325513511896134
[2025-03-27 19:02:56,571] INFO: Iter 61500 Summary: 
[2025-03-27 19:02:56,571] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03781301684677601
[2025-03-27 19:03:25,026] INFO: Iter 61600 Summary: 
[2025-03-27 19:03:25,026] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037919137552380563
[2025-03-27 19:03:53,532] INFO: Iter 61700 Summary: 
[2025-03-27 19:03:53,532] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037633451633155346
[2025-03-27 19:04:22,021] INFO: Iter 61800 Summary: 
[2025-03-27 19:04:22,021] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037266150563955304
[2025-03-27 19:04:50,489] INFO: Iter 61900 Summary: 
[2025-03-27 19:04:50,490] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03743003573268652
[2025-03-27 19:05:19,405] INFO: Iter 62000 Summary: 
[2025-03-27 19:05:19,405] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03743132751435042
[2025-03-27 19:05:55,751] INFO: Iter 62100 Summary: 
[2025-03-27 19:05:55,751] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037810518518090246
[2025-03-27 19:06:24,085] INFO: Iter 62200 Summary: 
[2025-03-27 19:06:24,085] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03739880040287971
[2025-03-27 19:06:52,568] INFO: Iter 62300 Summary: 
[2025-03-27 19:06:52,568] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037623185999691486
[2025-03-27 19:07:21,011] INFO: Iter 62400 Summary: 
[2025-03-27 19:07:21,011] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03740136593580246
[2025-03-27 19:07:49,251] INFO: Iter 62500 Summary: 
[2025-03-27 19:07:49,251] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03755604267120361
[2025-03-27 19:08:17,808] INFO: Iter 62600 Summary: 
[2025-03-27 19:08:17,808] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037450789511203765
[2025-03-27 19:08:46,273] INFO: Iter 62700 Summary: 
[2025-03-27 19:08:46,274] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03754935085773468
[2025-03-27 19:09:14,914] INFO: Iter 62800 Summary: 
[2025-03-27 19:09:14,914] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037386396676301954
[2025-03-27 19:09:43,350] INFO: Iter 62900 Summary: 
[2025-03-27 19:09:43,350] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742513731122017
[2025-03-27 19:10:11,891] INFO: Iter 63000 Summary: 
[2025-03-27 19:10:11,891] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03734001401811838
[2025-03-27 19:10:40,259] INFO: Iter 63100 Summary: 
[2025-03-27 19:10:40,259] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037479441799223426
[2025-03-27 19:11:08,619] INFO: Iter 63200 Summary: 
[2025-03-27 19:11:08,619] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0376776485517621
[2025-03-27 19:11:37,096] INFO: Iter 63300 Summary: 
[2025-03-27 19:11:37,097] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03752860680222511
[2025-03-27 19:12:05,627] INFO: Iter 63400 Summary: 
[2025-03-27 19:12:05,627] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03754330836236477
[2025-03-27 19:12:34,054] INFO: Iter 63500 Summary: 
[2025-03-27 19:12:34,054] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0373536979034543
[2025-03-27 19:13:02,530] INFO: Iter 63600 Summary: 
[2025-03-27 19:13:02,531] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03754509046673775
[2025-03-27 19:13:30,982] INFO: Iter 63700 Summary: 
[2025-03-27 19:13:30,982] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037505982741713524
[2025-03-27 19:13:59,542] INFO: Iter 63800 Summary: 
[2025-03-27 19:13:59,542] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03765033666044473
[2025-03-27 19:14:28,006] INFO: Iter 63900 Summary: 
[2025-03-27 19:14:28,006] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03744247786700725
[2025-03-27 19:14:56,465] INFO: Iter 64000 Summary: 
[2025-03-27 19:14:56,465] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761805884540081
[2025-03-27 19:15:31,617] INFO: Iter 64100 Summary: 
[2025-03-27 19:15:31,617] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742266800254583
[2025-03-27 19:15:59,937] INFO: Iter 64200 Summary: 
[2025-03-27 19:15:59,937] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037205725014209746
[2025-03-27 19:16:28,395] INFO: Iter 64300 Summary: 
[2025-03-27 19:16:28,395] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03756959393620491
[2025-03-27 19:16:56,882] INFO: Iter 64400 Summary: 
[2025-03-27 19:16:56,882] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03767503961920738
[2025-03-27 19:17:25,340] INFO: Iter 64500 Summary: 
[2025-03-27 19:17:25,340] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03730244178324938
[2025-03-27 19:17:53,801] INFO: Iter 64600 Summary: 
[2025-03-27 19:17:53,802] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03746554911136627
[2025-03-27 19:18:22,177] INFO: Iter 64700 Summary: 
[2025-03-27 19:18:22,177] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037562958858907224
[2025-03-27 19:18:50,707] INFO: Iter 64800 Summary: 
[2025-03-27 19:18:50,707] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037328203357756135
[2025-03-27 19:19:19,116] INFO: Iter 64900 Summary: 
[2025-03-27 19:19:19,116] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03770203325897455
[2025-03-27 19:19:47,525] INFO: Iter 65000 Summary: 
[2025-03-27 19:19:47,525] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037429256290197374
[2025-03-27 19:20:15,992] INFO: Iter 65100 Summary: 
[2025-03-27 19:20:15,993] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037403032593429086
[2025-03-27 19:20:44,368] INFO: Iter 65200 Summary: 
[2025-03-27 19:20:44,368] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037617393732070924
[2025-03-27 19:21:12,828] INFO: Iter 65300 Summary: 
[2025-03-27 19:21:12,829] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03727600824087858
[2025-03-27 19:21:41,620] INFO: Iter 65400 Summary: 
[2025-03-27 19:21:41,620] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03759686786681413
[2025-03-27 19:22:10,026] INFO: Iter 65500 Summary: 
[2025-03-27 19:22:10,027] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738278035074472
[2025-03-27 19:22:38,493] INFO: Iter 65600 Summary: 
[2025-03-27 19:22:38,493] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037752414345741274
[2025-03-27 19:23:07,076] INFO: Iter 65700 Summary: 
[2025-03-27 19:23:07,076] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03748267695307732
[2025-03-27 19:23:35,460] INFO: Iter 65800 Summary: 
[2025-03-27 19:23:35,460] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03729839332401753
[2025-03-27 19:24:03,918] INFO: Iter 65900 Summary: 
[2025-03-27 19:24:03,918] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742234315723181
[2025-03-27 19:24:32,432] INFO: Iter 66000 Summary: 
[2025-03-27 19:24:32,432] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728169124573469
[2025-03-27 19:25:07,789] INFO: Iter 66100 Summary: 
[2025-03-27 19:25:07,789] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037629354409873486
[2025-03-27 19:25:36,293] INFO: Iter 66200 Summary: 
[2025-03-27 19:25:36,293] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0373556112870574
[2025-03-27 19:26:04,703] INFO: Iter 66300 Summary: 
[2025-03-27 19:26:04,703] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03750907558947802
[2025-03-27 19:26:33,147] INFO: Iter 66400 Summary: 
[2025-03-27 19:26:33,147] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037161195278167726
[2025-03-27 19:27:01,654] INFO: Iter 66500 Summary: 
[2025-03-27 19:27:01,654] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037532818354666236
[2025-03-27 19:27:30,096] INFO: Iter 66600 Summary: 
[2025-03-27 19:27:30,096] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03761541202664375
[2025-03-27 19:27:58,621] INFO: Iter 66700 Summary: 
[2025-03-27 19:27:58,621] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738825045526028
[2025-03-27 19:28:26,953] INFO: Iter 66800 Summary: 
[2025-03-27 19:28:26,953] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742246579378843
[2025-03-27 19:28:55,347] INFO: Iter 66900 Summary: 
[2025-03-27 19:28:55,348] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03740187749266624
[2025-03-27 19:29:23,684] INFO: Iter 67000 Summary: 
[2025-03-27 19:29:23,684] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03751184653490782
[2025-03-27 19:29:52,243] INFO: Iter 67100 Summary: 
[2025-03-27 19:29:52,244] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03745021179318428
[2025-03-27 19:30:20,711] INFO: Iter 67200 Summary: 
[2025-03-27 19:30:20,711] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03748127344995737
[2025-03-27 19:30:49,080] INFO: Iter 67300 Summary: 
[2025-03-27 19:30:49,080] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037600663900375364
[2025-03-27 19:31:17,453] INFO: Iter 67400 Summary: 
[2025-03-27 19:31:17,454] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037387717850506305
[2025-03-27 19:31:45,978] INFO: Iter 67500 Summary: 
[2025-03-27 19:31:45,978] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03741531789302826
[2025-03-27 19:32:14,372] INFO: Iter 67600 Summary: 
[2025-03-27 19:32:14,372] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738651745021343
[2025-03-27 19:32:42,824] INFO: Iter 67700 Summary: 
[2025-03-27 19:32:42,824] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03739295080304146
[2025-03-27 19:33:11,594] INFO: Iter 67800 Summary: 
[2025-03-27 19:33:11,594] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03739601518958807
[2025-03-27 19:33:40,055] INFO: Iter 67900 Summary: 
[2025-03-27 19:33:40,055] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037156504802405836
[2025-03-27 19:34:08,759] INFO: Iter 68000 Summary: 
[2025-03-27 19:34:08,760] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737552497535944
[2025-03-27 19:34:45,181] INFO: Iter 68100 Summary: 
[2025-03-27 19:34:45,181] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037513047903776166
[2025-03-27 19:35:14,098] INFO: Iter 68200 Summary: 
[2025-03-27 19:35:14,098] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037346591055393216
[2025-03-27 19:35:42,865] INFO: Iter 68300 Summary: 
[2025-03-27 19:35:42,865] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03719558265060186
[2025-03-27 19:36:11,844] INFO: Iter 68400 Summary: 
[2025-03-27 19:36:11,844] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03774839047342539
[2025-03-27 19:36:40,669] INFO: Iter 68500 Summary: 
[2025-03-27 19:36:40,670] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03744913596659899
[2025-03-27 19:37:09,169] INFO: Iter 68600 Summary: 
[2025-03-27 19:37:09,169] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037350145988166335
[2025-03-27 19:37:37,560] INFO: Iter 68700 Summary: 
[2025-03-27 19:37:37,560] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03708277851343155
[2025-03-27 19:38:06,014] INFO: Iter 68800 Summary: 
[2025-03-27 19:38:06,014] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03742578059434891
[2025-03-27 19:38:34,635] INFO: Iter 68900 Summary: 
[2025-03-27 19:38:34,636] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03721944350749254
[2025-03-27 19:39:03,123] INFO: Iter 69000 Summary: 
[2025-03-27 19:39:03,123] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037456280700862404
[2025-03-27 19:39:31,552] INFO: Iter 69100 Summary: 
[2025-03-27 19:39:31,552] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03750317260622978
[2025-03-27 19:39:59,944] INFO: Iter 69200 Summary: 
[2025-03-27 19:39:59,944] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037353150621056555
[2025-03-27 19:40:28,286] INFO: Iter 69300 Summary: 
[2025-03-27 19:40:28,286] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037184915468096734
[2025-03-27 19:40:56,935] INFO: Iter 69400 Summary: 
[2025-03-27 19:40:56,935] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037546831294894216
[2025-03-27 19:41:25,376] INFO: Iter 69500 Summary: 
[2025-03-27 19:41:25,376] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037308756969869135
[2025-03-27 19:41:53,818] INFO: Iter 69600 Summary: 
[2025-03-27 19:41:53,818] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03731804259121418
[2025-03-27 19:42:22,349] INFO: Iter 69700 Summary: 
[2025-03-27 19:42:22,349] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03722899358719587
[2025-03-27 19:42:50,737] INFO: Iter 69800 Summary: 
[2025-03-27 19:42:50,737] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03734243270009756
[2025-03-27 19:43:19,403] INFO: Iter 69900 Summary: 
[2025-03-27 19:43:19,403] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03732024550437927
[2025-03-27 19:43:47,813] INFO: Iter 70000 Summary: 
[2025-03-27 19:43:47,813] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0375515266880393
[2025-03-27 19:44:23,047] INFO: Iter 70100 Summary: 
[2025-03-27 19:44:23,047] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03745015949010849
[2025-03-27 19:44:51,468] INFO: Iter 70200 Summary: 
[2025-03-27 19:44:51,468] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03713799044489861
[2025-03-27 19:45:19,974] INFO: Iter 70300 Summary: 
[2025-03-27 19:45:19,974] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03748238060623407
[2025-03-27 19:45:48,374] INFO: Iter 70400 Summary: 
[2025-03-27 19:45:48,374] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728166960179806
[2025-03-27 19:46:17,054] INFO: Iter 70500 Summary: 
[2025-03-27 19:46:17,054] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03762345865368843
[2025-03-27 19:46:45,603] INFO: Iter 70600 Summary: 
[2025-03-27 19:46:45,603] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037424486316740516
[2025-03-27 19:47:13,919] INFO: Iter 70700 Summary: 
[2025-03-27 19:47:13,919] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037420594207942484
[2025-03-27 19:47:42,203] INFO: Iter 70800 Summary: 
[2025-03-27 19:47:42,203] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03745773684233427
[2025-03-27 19:48:10,798] INFO: Iter 70900 Summary: 
[2025-03-27 19:48:10,798] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037347033135592934
[2025-03-27 19:48:39,253] INFO: Iter 71000 Summary: 
[2025-03-27 19:48:39,253] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037278693430125714
[2025-03-27 19:49:07,829] INFO: Iter 71100 Summary: 
[2025-03-27 19:49:07,829] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037371055074036125
[2025-03-27 19:49:36,196] INFO: Iter 71200 Summary: 
[2025-03-27 19:49:36,196] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03743710227310657
[2025-03-27 19:50:04,573] INFO: Iter 71300 Summary: 
[2025-03-27 19:50:04,573] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03701264578849077
[2025-03-27 19:50:32,853] INFO: Iter 71400 Summary: 
[2025-03-27 19:50:32,853] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03736434493213892
[2025-03-27 19:51:01,462] INFO: Iter 71500 Summary: 
[2025-03-27 19:51:01,462] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738327126950026
[2025-03-27 19:51:29,908] INFO: Iter 71600 Summary: 
[2025-03-27 19:51:29,908] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03716153725981712
[2025-03-27 19:51:58,343] INFO: Iter 71700 Summary: 
[2025-03-27 19:51:58,343] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037176260724663734
[2025-03-27 19:52:26,754] INFO: Iter 71800 Summary: 
[2025-03-27 19:52:26,754] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037329249642789365
[2025-03-27 19:52:55,140] INFO: Iter 71900 Summary: 
[2025-03-27 19:52:55,140] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03738328494131565
[2025-03-27 19:53:23,590] INFO: Iter 72000 Summary: 
[2025-03-27 19:53:23,590] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733661826699972
[2025-03-27 19:53:58,752] INFO: Iter 72100 Summary: 
[2025-03-27 19:53:58,752] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03721368450671434
[2025-03-27 19:54:27,164] INFO: Iter 72200 Summary: 
[2025-03-27 19:54:27,164] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733054418116808
[2025-03-27 19:54:55,609] INFO: Iter 72300 Summary: 
[2025-03-27 19:54:55,609] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037368090227246284
[2025-03-27 19:55:23,956] INFO: Iter 72400 Summary: 
[2025-03-27 19:55:23,956] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03719222668558359
[2025-03-27 19:55:52,317] INFO: Iter 72500 Summary: 
[2025-03-27 19:55:52,317] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037240557931363584
[2025-03-27 19:56:20,716] INFO: Iter 72600 Summary: 
[2025-03-27 19:56:20,716] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037414253838360306
[2025-03-27 19:56:49,173] INFO: Iter 72700 Summary: 
[2025-03-27 19:56:49,173] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03711410030722618
[2025-03-27 19:57:17,586] INFO: Iter 72800 Summary: 
[2025-03-27 19:57:17,586] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03740197390317917
[2025-03-27 19:57:45,951] INFO: Iter 72900 Summary: 
[2025-03-27 19:57:45,951] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03743005279451608
[2025-03-27 19:58:14,396] INFO: Iter 73000 Summary: 
[2025-03-27 19:58:14,397] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03727288875728846
[2025-03-27 19:58:42,759] INFO: Iter 73100 Summary: 
[2025-03-27 19:58:42,759] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03741100948303938
[2025-03-27 19:59:11,201] INFO: Iter 73200 Summary: 
[2025-03-27 19:59:11,201] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0376806852966547
[2025-03-27 19:59:39,589] INFO: Iter 73300 Summary: 
[2025-03-27 19:59:39,589] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037288266457617285
[2025-03-27 20:00:08,088] INFO: Iter 73400 Summary: 
[2025-03-27 20:00:08,089] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737616952508688
[2025-03-27 20:00:36,552] INFO: Iter 73500 Summary: 
[2025-03-27 20:00:36,553] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03729686789214611
[2025-03-27 20:01:05,357] INFO: Iter 73600 Summary: 
[2025-03-27 20:01:05,357] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037415533028542995
[2025-03-27 20:01:33,658] INFO: Iter 73700 Summary: 
[2025-03-27 20:01:33,658] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725362751632929
[2025-03-27 20:02:01,963] INFO: Iter 73800 Summary: 
[2025-03-27 20:02:01,963] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037266413420438765
[2025-03-27 20:02:30,259] INFO: Iter 73900 Summary: 
[2025-03-27 20:02:30,259] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03753297995775938
[2025-03-27 20:02:58,552] INFO: Iter 74000 Summary: 
[2025-03-27 20:02:58,552] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.036984335146844384
[2025-03-27 20:03:33,814] INFO: Iter 74100 Summary: 
[2025-03-27 20:03:33,814] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03722288094460964
[2025-03-27 20:04:02,253] INFO: Iter 74200 Summary: 
[2025-03-27 20:04:02,253] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728291690349579
[2025-03-27 20:04:30,553] INFO: Iter 74300 Summary: 
[2025-03-27 20:04:30,553] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037440710552036766
[2025-03-27 20:04:58,948] INFO: Iter 74400 Summary: 
[2025-03-27 20:04:58,948] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037366018667817115
[2025-03-27 20:05:27,174] INFO: Iter 74500 Summary: 
[2025-03-27 20:05:27,174] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733791057020426
[2025-03-27 20:05:55,324] INFO: Iter 74600 Summary: 
[2025-03-27 20:05:55,324] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0372224785387516
[2025-03-27 20:06:23,744] INFO: Iter 74700 Summary: 
[2025-03-27 20:06:23,745] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03708687711507082
[2025-03-27 20:06:52,044] INFO: Iter 74800 Summary: 
[2025-03-27 20:06:52,044] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037381802871823314
[2025-03-27 20:07:20,519] INFO: Iter 74900 Summary: 
[2025-03-27 20:07:20,519] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725524093955755
[2025-03-27 20:07:48,966] INFO: Iter 75000 Summary: 
[2025-03-27 20:07:48,966] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03708129048347473
[2025-03-27 20:08:17,435] INFO: Iter 75100 Summary: 
[2025-03-27 20:08:17,435] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733681961894035
[2025-03-27 20:08:46,217] INFO: Iter 75200 Summary: 
[2025-03-27 20:08:46,218] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03743749778717756
[2025-03-27 20:09:14,540] INFO: Iter 75300 Summary: 
[2025-03-27 20:09:14,540] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03732101995497942
[2025-03-27 20:09:42,904] INFO: Iter 75400 Summary: 
[2025-03-27 20:09:42,904] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037209018990397455
[2025-03-27 20:10:11,258] INFO: Iter 75500 Summary: 
[2025-03-27 20:10:11,258] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733858305960894
[2025-03-27 20:10:39,594] INFO: Iter 75600 Summary: 
[2025-03-27 20:10:39,594] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03689717222005129
[2025-03-27 20:11:08,046] INFO: Iter 75700 Summary: 
[2025-03-27 20:11:08,046] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0373239180073142
[2025-03-27 20:11:36,301] INFO: Iter 75800 Summary: 
[2025-03-27 20:11:36,301] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03726544179022312
[2025-03-27 20:12:04,744] INFO: Iter 75900 Summary: 
[2025-03-27 20:12:04,744] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03707107175141573
[2025-03-27 20:12:32,970] INFO: Iter 76000 Summary: 
[2025-03-27 20:12:32,970] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037213245667517186
[2025-03-27 20:13:08,279] INFO: Iter 76100 Summary: 
[2025-03-27 20:13:08,279] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03721562664955855
[2025-03-27 20:13:36,641] INFO: Iter 76200 Summary: 
[2025-03-27 20:13:36,641] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03763731103390455
[2025-03-27 20:14:04,925] INFO: Iter 76300 Summary: 
[2025-03-27 20:14:04,925] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037117534279823304
[2025-03-27 20:14:33,287] INFO: Iter 76400 Summary: 
[2025-03-27 20:14:33,287] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0373318400606513
[2025-03-27 20:15:01,699] INFO: Iter 76500 Summary: 
[2025-03-27 20:15:01,699] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03714231882244348
[2025-03-27 20:15:30,039] INFO: Iter 76600 Summary: 
[2025-03-27 20:15:30,040] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03715714920312166
[2025-03-27 20:15:58,612] INFO: Iter 76700 Summary: 
[2025-03-27 20:15:58,612] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725905690342188
[2025-03-27 20:16:26,991] INFO: Iter 76800 Summary: 
[2025-03-27 20:16:26,991] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037475337833166124
[2025-03-27 20:16:55,301] INFO: Iter 76900 Summary: 
[2025-03-27 20:16:55,301] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03741410654038191
[2025-03-27 20:17:23,714] INFO: Iter 77000 Summary: 
[2025-03-27 20:17:23,714] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03710686739534139
[2025-03-27 20:17:52,005] INFO: Iter 77100 Summary: 
[2025-03-27 20:17:52,005] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03727953590452671
[2025-03-27 20:18:20,368] INFO: Iter 77200 Summary: 
[2025-03-27 20:18:20,368] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03699910037219525
[2025-03-27 20:18:48,727] INFO: Iter 77300 Summary: 
[2025-03-27 20:18:48,727] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737020965665579
[2025-03-27 20:19:17,236] INFO: Iter 77400 Summary: 
[2025-03-27 20:19:17,236] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037127428352832795
[2025-03-27 20:19:45,567] INFO: Iter 77500 Summary: 
[2025-03-27 20:19:45,567] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037117961160838604
[2025-03-27 20:20:13,903] INFO: Iter 77600 Summary: 
[2025-03-27 20:20:13,903] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03739628959447146
[2025-03-27 20:20:42,265] INFO: Iter 77700 Summary: 
[2025-03-27 20:20:42,265] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03758234865963459
[2025-03-27 20:21:10,568] INFO: Iter 77800 Summary: 
[2025-03-27 20:21:10,569] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03705133598297834
[2025-03-27 20:21:38,916] INFO: Iter 77900 Summary: 
[2025-03-27 20:21:38,916] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.036910698562860486
[2025-03-27 20:22:07,333] INFO: Iter 78000 Summary: 
[2025-03-27 20:22:07,334] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03710767790675163
[2025-03-27 20:22:42,445] INFO: Iter 78100 Summary: 
[2025-03-27 20:22:42,445] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03747875951230526
[2025-03-27 20:23:10,718] INFO: Iter 78200 Summary: 
[2025-03-27 20:23:10,718] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03702045623213053
[2025-03-27 20:23:39,042] INFO: Iter 78300 Summary: 
[2025-03-27 20:23:39,042] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03729071598500013
[2025-03-27 20:24:07,326] INFO: Iter 78400 Summary: 
[2025-03-27 20:24:07,326] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037339805997908114
[2025-03-27 20:24:35,658] INFO: Iter 78500 Summary: 
[2025-03-27 20:24:35,658] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03720625203102827
[2025-03-27 20:25:04,098] INFO: Iter 78600 Summary: 
[2025-03-27 20:25:04,098] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03748078923672438
[2025-03-27 20:25:32,464] INFO: Iter 78700 Summary: 
[2025-03-27 20:25:32,464] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037408195063471796
[2025-03-27 20:26:01,021] INFO: Iter 78800 Summary: 
[2025-03-27 20:26:01,021] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03715146165341139
[2025-03-27 20:26:29,449] INFO: Iter 78900 Summary: 
[2025-03-27 20:26:29,449] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03709232900291681
[2025-03-27 20:26:57,854] INFO: Iter 79000 Summary: 
[2025-03-27 20:26:57,854] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03745373245328665
[2025-03-27 20:27:26,285] INFO: Iter 79100 Summary: 
[2025-03-27 20:27:26,285] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03709560099989176
[2025-03-27 20:27:54,584] INFO: Iter 79200 Summary: 
[2025-03-27 20:27:54,584] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037251828722655776
[2025-03-27 20:28:22,964] INFO: Iter 79300 Summary: 
[2025-03-27 20:28:22,965] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03696801967918873
[2025-03-27 20:28:51,255] INFO: Iter 79400 Summary: 
[2025-03-27 20:28:51,255] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03706540923565626
[2025-03-27 20:29:19,637] INFO: Iter 79500 Summary: 
[2025-03-27 20:29:19,638] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03715098835527897
[2025-03-27 20:29:48,135] INFO: Iter 79600 Summary: 
[2025-03-27 20:29:48,135] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03755393795669079
[2025-03-27 20:30:16,574] INFO: Iter 79700 Summary: 
[2025-03-27 20:30:16,574] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03717286489903927
[2025-03-27 20:30:44,876] INFO: Iter 79800 Summary: 
[2025-03-27 20:30:44,877] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03711655106395483
[2025-03-27 20:31:13,445] INFO: Iter 79900 Summary: 
[2025-03-27 20:31:13,445] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03705715928226709
[2025-03-27 20:31:41,851] INFO: Iter 80000 Summary: 
[2025-03-27 20:31:41,852] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03737976558506489
[2025-03-27 20:32:18,019] INFO: Iter 80100 Summary: 
[2025-03-27 20:32:18,019] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03731711570173502
[2025-03-27 20:32:46,479] INFO: Iter 80200 Summary: 
[2025-03-27 20:32:46,479] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03721770003437996
[2025-03-27 20:33:15,012] INFO: Iter 80300 Summary: 
[2025-03-27 20:33:15,012] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03687724743038416
[2025-03-27 20:33:43,439] INFO: Iter 80400 Summary: 
[2025-03-27 20:33:43,439] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037359877601265905
[2025-03-27 20:34:12,026] INFO: Iter 80500 Summary: 
[2025-03-27 20:34:12,026] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037125631272792815
[2025-03-27 20:34:40,415] INFO: Iter 80600 Summary: 
[2025-03-27 20:34:40,416] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037422140389680864
[2025-03-27 20:35:08,881] INFO: Iter 80700 Summary: 
[2025-03-27 20:35:08,881] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03730704706162214
[2025-03-27 20:35:37,334] INFO: Iter 80800 Summary: 
[2025-03-27 20:35:37,334] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03724798202514648
[2025-03-27 20:36:05,758] INFO: Iter 80900 Summary: 
[2025-03-27 20:36:05,758] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037019075490534306
[2025-03-27 20:36:34,190] INFO: Iter 81000 Summary: 
[2025-03-27 20:36:34,190] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03723434247076511
[2025-03-27 20:37:02,644] INFO: Iter 81100 Summary: 
[2025-03-27 20:37:02,644] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03703063078224659
[2025-03-27 20:37:31,024] INFO: Iter 81200 Summary: 
[2025-03-27 20:37:31,024] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037241107523441314
[2025-03-27 20:37:59,242] INFO: Iter 81300 Summary: 
[2025-03-27 20:37:59,242] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03717006385326385
[2025-03-27 20:38:27,610] INFO: Iter 81400 Summary: 
[2025-03-27 20:38:27,611] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728455279022455
[2025-03-27 20:38:55,956] INFO: Iter 81500 Summary: 
[2025-03-27 20:38:55,956] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03712865252047777
[2025-03-27 20:39:24,362] INFO: Iter 81600 Summary: 
[2025-03-27 20:39:24,362] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037261149100959304
[2025-03-27 20:39:52,783] INFO: Iter 81700 Summary: 
[2025-03-27 20:39:52,783] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03721024069935083
[2025-03-27 20:40:21,190] INFO: Iter 81800 Summary: 
[2025-03-27 20:40:21,190] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03702595613896847
[2025-03-27 20:40:49,524] INFO: Iter 81900 Summary: 
[2025-03-27 20:40:49,524] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03706470310688019
[2025-03-27 20:41:17,913] INFO: Iter 82000 Summary: 
[2025-03-27 20:41:17,913] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03715907011181116
[2025-03-27 20:41:53,510] INFO: Iter 82100 Summary: 
[2025-03-27 20:41:53,510] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03733268413692713
[2025-03-27 20:42:21,909] INFO: Iter 82200 Summary: 
[2025-03-27 20:42:21,909] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037219731286168095
[2025-03-27 20:42:50,124] INFO: Iter 82300 Summary: 
[2025-03-27 20:42:50,125] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037229418270289895
[2025-03-27 20:43:19,072] INFO: Iter 82400 Summary: 
[2025-03-27 20:43:19,072] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03699882123619318
[2025-03-27 20:43:47,648] INFO: Iter 82500 Summary: 
[2025-03-27 20:43:47,648] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03704685106873512
[2025-03-27 20:44:16,186] INFO: Iter 82600 Summary: 
[2025-03-27 20:44:16,186] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03746873155236244
[2025-03-27 20:44:44,507] INFO: Iter 82700 Summary: 
[2025-03-27 20:44:44,507] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03729724384844303
[2025-03-27 20:45:12,813] INFO: Iter 82800 Summary: 
[2025-03-27 20:45:12,814] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037129135243594645
[2025-03-27 20:45:41,248] INFO: Iter 82900 Summary: 
[2025-03-27 20:45:41,248] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037226512543857095
[2025-03-27 20:46:09,608] INFO: Iter 83000 Summary: 
[2025-03-27 20:46:09,608] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037191429995000365
[2025-03-27 20:46:37,924] INFO: Iter 83100 Summary: 
[2025-03-27 20:46:37,924] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03724065225571394
[2025-03-27 20:47:06,466] INFO: Iter 83200 Summary: 
[2025-03-27 20:47:06,466] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03711141124367714
[2025-03-27 20:47:34,864] INFO: Iter 83300 Summary: 
[2025-03-27 20:47:34,864] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03700477533042431
[2025-03-27 20:48:03,248] INFO: Iter 83400 Summary: 
[2025-03-27 20:48:03,248] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03732528742402792
[2025-03-27 20:48:31,548] INFO: Iter 83500 Summary: 
[2025-03-27 20:48:31,548] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037083968855440616
[2025-03-27 20:48:59,939] INFO: Iter 83600 Summary: 
[2025-03-27 20:48:59,939] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037216225489974025
[2025-03-27 20:49:28,407] INFO: Iter 83700 Summary: 
[2025-03-27 20:49:28,408] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03718572698533535
[2025-03-27 20:49:56,753] INFO: Iter 83800 Summary: 
[2025-03-27 20:49:56,753] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725251905620098
[2025-03-27 20:50:25,122] INFO: Iter 83900 Summary: 
[2025-03-27 20:50:25,123] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725283842533827
[2025-03-27 20:50:53,549] INFO: Iter 84000 Summary: 
[2025-03-27 20:50:53,549] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03723106794059276
[2025-03-27 20:51:28,677] INFO: Iter 84100 Summary: 
[2025-03-27 20:51:28,677] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037256868332624433
[2025-03-27 20:51:57,000] INFO: Iter 84200 Summary: 
[2025-03-27 20:51:57,000] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037331099696457384
[2025-03-27 20:52:25,330] INFO: Iter 84300 Summary: 
[2025-03-27 20:52:25,330] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03728710025548935
[2025-03-27 20:52:53,650] INFO: Iter 84400 Summary: 
[2025-03-27 20:52:53,651] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03686195351183415
[2025-03-27 20:53:22,007] INFO: Iter 84500 Summary: 
[2025-03-27 20:53:22,007] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03725693214684725
[2025-03-27 20:53:50,399] INFO: Iter 84600 Summary: 
[2025-03-27 20:53:50,399] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03710313379764557
[2025-03-27 20:54:18,707] INFO: Iter 84700 Summary: 
[2025-03-27 20:54:18,708] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037257933393120765
[2025-03-27 20:54:47,112] INFO: Iter 84800 Summary: 
[2025-03-27 20:54:47,112] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0370651264488697
[2025-03-27 20:55:15,545] INFO: Iter 84900 Summary: 
[2025-03-27 20:55:15,545] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037306667678058146
[2025-03-27 20:55:43,813] INFO: Iter 85000 Summary: 
[2025-03-27 20:55:43,813] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.037226330973207954
